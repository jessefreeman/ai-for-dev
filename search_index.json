{"index":{"version":"0.5.12","fields":[{"name":"title","boost":10},{"name":"keywords","boost":15},{"name":"body","boost":1}],"ref":"url","documentStore":{"store":{"./":["(llms)","additions,","ai","aifor.dev","anyon","be","best","build","code","code,","coding.","collect","commun","community.","community:","constantli","content","contribut","contribute.","convers","deeper","design","detail","dev","develop","discuss","discussions:","enhanc","evolv","experiments,","explor","figur","findings,","follow","gain","gitbook","github","github.","grow","guid","guide!","guide'","guide.","guidelin","help","https://jessefreeman.substack.com/.","improv","insight","insights,","instruct","interest,","introduct","invit","join","journey.","languag","larg","latest","learn","leverag","list","list:","llm","local","look","mail","make","markdown","me,","model","more","new","observ","onlin","optim","other","out","page.","part","pleas","power","practices,","productivity,","project","project,","refer","resourc","results,","set","share","stare","stay","still","support,","tabl","test","thank","thought","thrive.","tips,","together,","understand","up","updat","us","valuabl","version","want","we'r","welcom","whether","who'","work,","workflows.","worldwide.","you'r"],"01_introduction/01_01_how_to_use_this_guide.html":["(llms)","abil","add","advantag","allow","amaz","avoid","behind","benchmark","best","break","capabilities,","changed.","chatgpt","code","coding.","computer'","contribut","control","cover,","creat","crutch.","design","develop","down","enjoy","ensur","environment.","everyth","feel","few","figur","focu","free","full","gener","get","give","go","guid","hardware,","help","hope","however,","idea","iq","it’","i’d","i’ll","i’m","i’v","know","languag","larg","learn","leverag","llm","llms,","llm’","local","look","lose","lot","love","model","model,","multipl","my,","need","note","now.","once,","optim","order","other's,","over","part,","past","perform","person","pick","pleas","points.","possible.","product","projects.","reli","right","run","scale","scenes,","see","set","share","show","side.","sole","someon","someth","there’","time","tinker,","tip","try.","turn","understand","up","updat","us","well","work","year"],"01_introduction/01_02_why_run_models_locally.html":["(llms)","abil","access","adjust","advantag","allow","alway","another.","around","associ","assur","avoid","bandwidth.","base","be","benefici","benefit","better","build","capabilities,","capabilities.","capabl","chain","chatgpt,","choic","cloud","cobbl","code","code,","come","compani","compel","complet","comput","configur","connect","constant","continu","control","control,","correctly,","cost","creat","custom","data,","day","debug,","dedic","deeper","demystifi","depend","deploy","developers,","development,","document","documentation,","don’t","econom","efficiency,","efficiency.","enabl","ensur","especi","excit","exist","experi","extend","extern","fee","fine","flight","form","function","furthermore,","gener","growth","hand","hardwar","hardware,","hardware’","have","help","home.","ideal","improv","improvement.","inform","inner","internet","investment.","isn’t","i’m","i’v","keep","languag","larg","later","lead","learn","leverag","limit","llm","local","locally?","long","machin","machine,","maintain","make","model","models.","more","need","network","offer","offlin","offline,","older","on","one,","onlin","online.","open","openai’","opportun","opportunity.","optim","others.","out","output","over","part","partner!","perform","possible,","power,","primari","privaci","project","protect","provid","qualiti","real","recur","reli","remot","rest","results.","run","run.","scale","scenario","security.","see","sensit","servers.","servic","services.","sever","side","signific","significantli","somewher","specif","speed","stay","subscript","such","suggest","suit","tailor","teach","term","thing","time","togeth","train","tune","turn,","tutori","understand","unlik","up","us","util","valuabl","which,","without","work","work,","work.","you’ll"],"02_understanding_large_language_models/02_01_overview_of_llms.html":["(bidirect","(gener","(larg","(left","(llms)","(nlp)","(r&d),","(todo:","2019","abil","acceler","accessible,","accur","accuraci","add","advanc","agents,","ai","ai)","ai,","allow","amount","analysi","analysis,","analysis.","answer","applic","applications,","applications.","approach","appropri","architectur","artifici","ask","assist","assistance.","assistants,","auto","autom","base","benefici","bert","best","bidirect","bigger","both","boundari","build","built","can't","capabl","case","ceas","chatbots,","chatgpt","clean","code","code,","code.","coher","comments.","common","completion,","complex","comprehend","comput","constrain","consum","content","context","context,","contextu","convers","correctly,","correctly.","creat","creation","creation,","creation.","custom","cycl","data","data,","datasets,","debug","deep","degre","descript","design","detail","develop","direct","divers","don’t","dummi","effici","efficient,","effort,","enabl","encod","end","engines,","enhanc","environments.","error","especi","even","exist","expand","experi","explain","extens","extract","fast","fewer","fiction.","find","fine","first","fluenci","focu","focus","follow","forward","gener","generate,","generation,","give","given","google,","gpt","guid","guide.","guides.","handl","har","hate","help","here","high","human","idea","imagin","impress","improv","includ","incred","infer","information,","input","insight","instance,","instruct","integr","intelligence,","interact","interactions.","is).","it,","i’m","join","knowledge.","known","languag","language.","larg","later","learn","level","level,","leverag","llama","llm","llm.","llms:","llms?","locally.","look","lot","make","manag","manipul","manipulation.","me.","meaning,","meta","meta,","model","modeling,","more","more.","moreover,","natur","need","never","new","nich","now","numer","offer","on","one.","openai,","optim","out","overview","perform","perspect","pictur","power","practic","practices.","pre","primari","problems.","process","produc","product","prompts.","provid","push","question","rang","reduc","refactor","rel","rememb","repetit","reports.","repres","represent","requir","research","resourc","resources.","respons","right).","routin","run","samsung","save","scienc","search","see","sentenc","sentiment","signific","smarter,","someon","someth","sophist","specif","spent","still","such","suitabl","summar","support,","surpris","system","systems,","take","task","tasks,","tasks.","techniques,","technolog","test","tests?","text","time","today’","tool","train","transform","transformer)","transformers)","tri","try.","tune","type","understand","understand,","unit","up","us","user","utility.","valuabl","vast","verifi","versatil","version","virtual","want","way","wide","without","word","workflow,","workflows.","write","written,","you.","you’d","you’ll"],"02_understanding_large_language_models/02_02_model_variations.html":["abil","accuracy,","accurate,","actual","advanc","align","allud","aspect","assist","auto","availability.","back","balanc","base","basic","be","best","better","broad","candid","can’t","capabilities,","capabl","case","cases:","challeng","chart","circular","code","code.","codellama","codellama,","coding,","coding.","coher","come","command","complet","complex","computer’","consist","content","contexts,","control","correct","creat","creation","crucial","descriptions.","design","detail","develop","differ","divers","do","doesn’t","down,","downsid","dozen","each","easi","effici","enhanc","ensur","environ","environments.","essenti","evalu","expectations.","expecting.","explain","fast","faster","find","fit","flexibl","follow","function","gener","generation.","give","given","go","good","guid","handl","harder.","hardware’","help","high","hope","however,","id","ideal","import","improv","includ","instance,","instruct","instruct.","instructions,","intens","interact","involv","isn’t","it'","it’","i’v","languag","lastly,","later","learn","level","leverag","life","limitations,","llama,","llm","llm.","llms'","llm’","lm","local","locally.","look","lot","make","mani","meet","meta’","model","model'","model,","models,","model’","more","multi","multipl","need","needs,","needs.","newer","now,","offer","on","on.","optim","options,","out","particularli","perform","performance.","phind","pick","place","poor","practic","precis","previou","primari","problem","program","prompt","provid","purpos","put","r","randomli","rang","real","requir","requirements.","resourc","respons","result","results,","return","revert","review","right","run","scenarios.","search","select","significantli","slow","snippets,","sophist","special","specif","speed","spend","standard","starcod","start.","statement","step","strength","strengths,","studio,","suit","suitabl","superior","surpris","tailor","talk","task","task.","tasks,","tasks.","test","them.","there,","time","time,","to,","togeth","tool","turn","understand","up","us","usage.","variant.","variat","variou","versatile,","version","way","well","we’ll","wil","wizardcod","work","workflow,","works.","wrong","you’d","you’ll"],"02_understanding_large_language_models/02_03_model_naming_conventions.html":["(b):","(e.g.,","(k):","(medium)","(q):","13","13b","3","34b.","_","abbrevi","accur","accuraci","accuracy.","add","affect","against","approach","assist","attempt","avail","balanc","balance,","befor","better","billion","billions,","bit","capabilities.","capabl","captur","case,","choos","code","code,","codellama","compon","compromis","comput","configurations.","consist","constraint","constraint.","constraints.","convent","conversely,","data.","depend","design","detail","differ","discuss","each","efficiency.","environ","example:","famili","faster","feel","file","focus","follow","format","framework","gener","gguf","goal","good","hand,","hardwar","hardware.","help","here","here'","here.","higher","however,","identifi","impact","increas","indic","inform","instance,","instruct","instruct.q3_k_m.gguf","interpret","intric","involv","i’ll","k_","k_l","k_m","key","kind","larger","learn","level","level.","level:","limit","llm","load","local","lower","luckili","make","mean","medium","memori","minim","mix","mode","model","model:","model’","moder","more","name","name.","names:","need","new","number","offer","onc","one,","optim","over","overwhelming.","paramet","parameters.","pattern","perform","performance.","pick","power","precis","precision,","precision.","prioriti","properly.","properti","provid","q2)","q3","q4,","q5)","quantiz","quantization,","read","realli","refer","relationship","repres","requir","requirements,","requirements.","resourc","resources,","resources.","result","right","run","scheme","scheme:","select","sever","simplifi","size","size:","small,","smaller","specif","speed.","strict","such","suitabl","system","tasks.","test","that’","therefore,","thing","this,","time","times,","top","understand","us","usag","usage,","use.","variant","variat","variou","we’ll","within","you’v"],"02_understanding_large_language_models/02_04_parameter_size.html":["(approx.)","(ram)","100m","10b","1b",">","\\","abil","adjust","affect","amount","analysi","and,","applic","aside,","assess","assist","assistance,","availability:","balanc","base","basic","benchmark","benchmarking:","benefit","better","between","bias","boost","capabilities.","capac","capacity.","captur","case","check","choos","code","come","comparison","compatibility:","completions,","complex","complexity:","comput","conclus","configurations.","consid","consider","context","correl","cost","cpu","cpu,","crash","crucial.","data.","decision.","defin","delv","demands.","depend","deployment,","detail","develop","differ","directli","doesn't","down","dure","effect","effici","enabl","enhanc","enough","ensur","essenti","essential.","everyth","experi","extra","fast","flesh","gaug","gener","generation,","gpu","gpu,","greater","handl","handle.","hardwar","hardware,","hardware.","hardware:","here'","here’","high","higher","highli","however,","identifi","impact","import","includ","increas","infer","inference,","inform","inputs,","intric","it'","it:","joke","know","larg","larger","lead","learn","lightweight","limit","llm","llm:","llms,","load","local","locally.","longer","low","machine.","make","match","matter","measur","medium","memori","model","model'","model.","models,","models.","model’","moder","monitor","more","need","needs.","needs:","network.","neural","nuanc","number","off","offer","optimize:","out","output","overwhelm","paramet","parameters,","pattern","perform","performance,","performance.","performance:","practic","process","product","qualiti","ram","ram,","real","refactor","refer","repres","requir","requirements,","requirements.","research,","resources.","resources:","respons","result","right","run","run.","scale","section","section,","see","select","signific","simpl","simpler","size","size,","sizes:","slow","slowdowns.","slower","small","smaller","specif","speed","speed:","standard","start","store","suffice,","suggest","suggestions,","suitabl","system","system'","system,","take","task","tasks.","time","time:","times,","times.","todo:","trade","trainabl","training.","typic","understand","up","us","usag","usage,","usage:","veri","vram","vram,","vram.","want","we'll","weight","well","without","work","workflow.","yes,"],"02_understanding_large_language_models/02_05_quantization.html":["(llms)","accur","accuracy,","accuracy.","allow","along","ampl","applic","applications.","applied,","approach","appropri","aspect","assist","balanc","below","best","better","between","both","breakdown","capabilities.","capabl","choic","choos","code","codellama","combination,","comparison:","compromis","comput","concept","configur","critical.","crucial","crucial.","decreas","demand","descript","design","detail","determin","develop","differ","effective.","effici","efficiency.","enabl","end","ensur","environ","example.","factor","faster","further","gener","hardwar","hardware.","help","here’","high","higher","highest","however,","ideal","implications,","important.","improv","infer","involv","it'","it’","k_","k_l","k_l:","k_m","k_m:","k_s:","knowledg","languag","larg","large,","larger","largest","less","level","level,","limit","llm","load","local","look","lower","make","maxim","medium,","memori","minim","model","model'","moder","more","need","offer","optim","over","paramet","parameters.","perform","performance,","performance.","possibl","power.","precis","precision,","precision.","priorit","process","provid","purpos","q2","q2:","q3","q3:","q4","q4:","q5","q5:","quantiz","quick","quickly.","rang","reduc","refer","refin","requir","resourc","resources.","result","right","run","sacrific","scheme","schemes,","select","setup","setup.","size","size,","size.","small,","smaller","sophist","specif","speed.","such","suffix","suitabl","system","tabl","tailor","task","tasks,","trade","understand","us","usag","usage,","usage.","use.","variou","versatil","wider","workflow"],"02_understanding_large_language_models/02_06_quantization_schemes.html":["\"iq\"","(e.g.,","(intellig","(llms)","(ptq):","(qat):","32","8","accept","access","accur","accuraci","accuracy.","achiev","activ","addit","adjust","advanc","advantag","ai","align","allow","appli","applications.","approach","appropri","assist","asymmetr","awar","balanc","base","batteri","benchmarks.","benefici","benefit","best","better","between","bit","both","breakdown","broader","broadli","calcul","carefulli","cases.","categor","channel","channels.","choos","chosen","code","compar","complex","comput","concept","cons:","consid","constraint","consumption,","consumption:","convert","counterparts,","critical,","crucial","decreas","deploy","description:","design","despit","detail","determin","develop","devic","devices.","differ","dure","dynam","each","easier","edg","effect","effici","efficiency:","efficient,","element","enabl","encount","energi","enhanc","ensur","environ","environment.","especi","essenti","evalu","factor","faster","faster,","feasibl","fewer","float","follow","footprint","full","further","gener","good","greatest","handl","hardwar","hardware.","here’","high","higher","implement","implement,","implementations.","implications,","improv","incorpor","infer","inferenc","inference,","inference.","inference:","integer).","involv","iq","iq1","iq2.","it'","iter","label","languag","laptop","larg","larger","lead","leav","less","limit","llm","lm","load","load.","local","look","lower","maintain","make","maxim","meet","memori","method","minim","model","model'","model.","models:","more","need","neg","non","nuanc","off","offs:","opt","optim","origin","overal","overhead","overhead.","paramet","parameters,","particularli","per","perform","performance,","performance:","point)","posit","post","power","pre","precis","precision,","precision.","prefixes,","process","process.","pros:","provid","ptq.","qat","quantiz","quantization)","quantization.","quantization:","quantized,","rang","re","reduc","reduct","reduction.","refer","refine:","represent","requir","requirements.","requirements:","resourc","resources,","resources.","result","results,","retain","retraining.","right","rigor","run","same","scale","scenario","scheme","scheme:","schemes,","select","sever","significantli","similar","simpler","singl","size","size,","size:","slight","slightli","slower","smaller,","sophist","specif","speed","speed,","speed.","standard","static","steps:","still","store","straightforward","strategi","strategies,","strike","studio","studio,","studio.","studio:","such","symmetr","system","tailor","take","task","tasks.","techniqu","tensor","tensor.","term","test","time","times.","trade","train","translat","type","types,","typic","understand","understanding,","understanding.","up","us","usage.","usage:","use,","use.","valu","values,","values.","vari","weight","wide","within","without","you’r"],"02_understanding_large_language_models/02_07_scaling_models.html":["\"bigger\"","\"deeper.\"","\"instruct\"","\"wider\"","(8","(e.g.,","(units)","(where","13b","13b,","16","32","34b,","8","8b","abil","abstract","accur","accuracy,","achieve.","ad","adapt","add","addit","address","adjust","adjust!","advanc","advantag","ai","allow","anoth","anyth","approach","area","base","becom","benefici","better","better,","between","bias","bigger","bigger,","billion","bit","bit).","both","boundari","broader","capabl","capac","capacity.","captur","care","challeng","challenges:","code","come","complex","compon","comprehens","comput","conclus","consist","consum","convert","core","costly.","costs:","creativity.","crucial","data","data).","data,","data.","data:","dataset","datasets.","deal","deeper","deliv","deploy","detail","determin","develop","dimension","divers","domains,","each","effect","effectively.","efficiency:","embed","embeddings:","enhanc","ensur","entails.","especi","even","expand","expansion:","extrem","fail","feasibl","featur","fine","fit","follow","footprint,","full","gener","generation,","generation.","go","gpu","greater","grow","guid","handl","hardwar","hardware,","has,","help","here’","high","however,","hundr","imag","import","improv","includ","increas","infer","inferencing.","inform","input","inputs.","instance,","instruct","instructions.","intric","involv","is.","know","languag","larg","larger","larger,","layer","layer.","layer:","layers,","layers:","lead","learn","less","level","levels.","limit","llms,","longer","machin","make","manag","management:","mean","memori","method","model","model,","model?","models,","models:","model’","more","much","multi","natur","necessit","need","network","network,","network.","networks:","network—that","neural","neuron","new","next","nuanc","number","numer","on","onc","once,","optim","optimization:","output","outputs.","overfit","overfit,","overfitting:","overview","paramet","parameters)","parameters,","parameters.","parameters—th","particular","particularli","pass","pattern","per","perform","performance,","performance.","planning,","poorli","power","precis","predict","predictions.","primari","process","produc","provid","push","quantiz","ram","rang","recognition.","reduc","relationship","repres","representations,","representations:","requir","requirements,","requirements:","research","resourc","result","risk","run","sacrif","scale","section","signific","significantli","simpl","size,","size:","sophist","space.","special","specialization:","specif","speed","speed:","step","such","suffici","take","target","task","tasks,","tasks.","techniqu","ten","terms,","there’","this,","those","time","times,","times.","times:","token","tool","train","trained,","training:","tune","understand","understanding,","unseen","up","us","used,","util","variant","vast","vector","vram,","vram.","weight","well","widen","wider","within","without","word","words,","works:","you’d"],"03_selecting_models/03_01_selecting_the right model.html":["(llms)","13b","1:","2:","32gb","3:","4","4:","abil","accuracy,","achiev","add","adjust","advanc","ai","align","allow","approach","appropri","assess","attribut","balanc","base","befor","benefit","best","capabilities,","capabilities.","capabl","choic","choos","code","codellama","coding,","come","compat","completion,","complex,","comput","computer'","computer’","configur","configurations.","cpu,","crucial","decod","deliv","demand","denot","depend","deploy","detail","develop","differ","each","effect","effici","enhanc","ensur","equip","evalu","example,","expect","familiar","family,","faster,","feasibl","first","fit","follow","format.","four","full","fulli","gpu,","guid","handl","hardwar","hardware'","hardware,","heavili","help","identifi","inord","insight","instance,","instruct","instruct.q3_k_m.gguf,","integr","intens","issu","it.","iter","it’","key","languag","larg","less","level,","leverag","limit","llm","local","locally,","make","meet","memori","model","model'","model,","model.","models,","model’","more","name","necessari","need","needs.","needs—wheth","next","onc","optim","option.","paramet","part","perform","possibl","possible.","potenti","power","process","provid","quantiz","quick,","quickli","ram","ram,","regular","resourc","resources,","respons","right","run","select","setup.","size,","specif","specifications,","speed.","standard","step","steps—understand","storage.","straightforward.","strength","structur","structure,","such","suitabl","system","system’","tasks.","test","testing—y","thorough","time,","type,","understand","us","usage,","usage.","use.","valuabl","variant","variat","variou","well","workflow,","workflow.","yourself","you’ll"],"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":["(4","(llms),","(~16","(~32","13b","16","32","34b","34b.","40","64","70b","70b,","8","8b","addit","advanc","affect","align","appropri","around","avail","balanc","base","be","be,","best","cap.","capabilities.","capabl","capacity.","categories:","cater","challeng","characteristics,","choos","closer","code","come","common","commun","communities.","compatibility,","complex","comput","consum","creat","crucial","custom","decent","decid","decis","demand","demands,","design","desktop","detail","develop","development.","differ","distinct","dure","effectively.","effici","encount","end","ensur","expect","fast","faster","fit","follow","footprints,","fulli","gb","gb):","gb+","gb,","gb.","gener","good","gpu","grade","group","guid","handl","hardware,","has,","help","high","higher","highest","ideal","includ","inform","intens","iq2_m","it'","key","languag","laptop","less","level","lightweight","limit","limit,","limit.","local","low","make","mani","match","maximum","member","memori","memory,","memory.","mid","model","model.","moder","modest","modifi","more","more,","much","near","need","needs.","note","nuanc","number","offer","offici","offload","open","optim","optimizations,","option","origin","output","paramet","particularli","perform","performance.","pick","point","portion","power.","precis","precision,","precision.","process","project'","provid","push","q3_k_l","q4_k_","q4_k_m","q5_k_","q5_k_m","q6_k","q6_k_","q6_k_m","q8_0","quick","ram.","rang","refer","reliable,","requir","required,","requirements.","resourc","resources,","resources.","respons","right","run","sacrif","see,","select","serv","setup","setup.","signific","size","slower","smooth","sourc","span","specif","speed","speed.","speeds.","standard","such","suit","suitabl","support","system","system'","systems,","tabl","task","tasks.","term","those","three","tier","times.","top","typic","understand","upper","us","usag","usage,","util","validation,","variat","veri","version","well","within","without","work","you'll","you'r","~10","~14","~18","~22","~24","~28","~35","~4","~40","~6","~7","~8"],"03_selecting_models/03_03_model_size_and_memory.html":["(dure","(ram)","(the","16","8","access","account","actual","addit","alloc","amount","avail","between","biases)","bit","characterist","computations.","computer.","computer’","consist","contains,","crucial","data","depend","determin","differ","disk","distinct","doesn’t","drive","due","effici","entir","environ","environment,","example,","exce","facilit","factors,","file","fix","float","fulli","furthermore,","gener","give","hard","idea","implemented,","import","includ","inferencing),","inferencing,","influenc","input","intermedi","larg","larger","librari","linux.","llm","llms,","load","loaded,","loaded.","macos,","made","manag","mean","memori","memory,","model","model'","model,","model.","models,","model’","more","much","necessarili","need","note","number","occupi","oper","operation.","optim","paramet","parameters.","particularli","perform","point","precis","process","quantiti","quantization.","ram","ram.","refer","reflect","regardless","requir","requirements.","resourc","run","runtim","same","save","set","sever","size","size.","slightli","space","specif","storag","store","structur","such","suggest.","system","systems,","systems.","talk","temporari","that,","those","understand","us","usag","vari","variou","weight","well","whether","windows,"],"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":["(central","(graphic","(llms),","(such","(vram).","(weight","accelerated.","achiev","add","advantag","allow","amount","applic","avail","available,","balanc","base","becom","best","between","biases),","billion","bottleneck,","bulk","calcul","calculations.","capabl","carefulli","commun","compar","complex","comput","computation","computer'","computing,","concurrently,","consider","constantli","contain","context","core","cores.","cpu","cpu,","critic","crucial","data","data,","degradation,","demand","depend","design","differ","directly,","disposal.","down","drastic","dure","effect","effici","enhanc","ensur","entir","especi","experi","faster","feed","fewer","final","fit","full","fullest","gener","gpu","gpu,","gpu.","gpus.","gpu—i","gpu’","handl","hardwar","hardware—specif","highli","hit","however,","ideal","implic","import","improv","includ","inferenc","inferencing,","inferencing.","inferencing:","inferencing?","input","inputs.","intensive.","intermedi","involv","keep","kind","languag","larg","larger","lead","learn","less","leverag","limits.","linear","llm","llms,","load","longer","make","manag","mani","mathemat","matrix","mean","memori","mind","model","models.","model’","more","multipl","multiplications,","necessari","need","new","non","number","numer","offload","offloaded.","offloading,","oper","operations,","optim","otherwis","output","output.","overal","overhead","parallel","paralleliz","paramet","parameters.","partial","particularli","perfect","perform","performance,","performance.","phase.","potenti","potential.","power","pre","predict","process","process,","process.","processing,","produc","prompt)","purpos","rang","real","reduc","refer","repetit","representations,","requir","resourc","respons","response.","right","run","select","seri","significantli","simultaneously,","size","slow","slower","smaller","specif","speed","split","spread","strike","system","system’","take","task","tasks,","text","thousand","time","times.","tool","traditionally,","train","transfer","transformations,","understand","units)","us","user","versatil","vram","vram,","vs.","well","wide","without","work","workload"],"03_selecting_models/03_05_offloading_to_the_gpu.html":["\"out","(a","(graphic","(llms)","(ram)","(vram),","abil","acceler","accept","achiev","advantages,","alloc","allow","alon","amount","anyon","applic","architecture,","arise:","attempt","avail","be","becom","benefit","better","both","calcul","capabilities,","capabl","challeng","challenges.","clear.","compens","complex","complexity,","comput","consider","continu","core","cpu","cpu.","cpu/gpu","cpus,","cpus.","crash","crashes:","creat","crucial","datasets,","degrad","demand","depend","design","determin","directli","disk","divis","drastic","dure","easier","effect","effici","efficiency.","enhanc","enough","ensur","entirely,","equal.","errors.","especi","even","evolve.","exce","experi","extens","fail","far","faster","fit","focu","full","fulli","gain","gener","gpu","gpu,","gpu.","gpu’","grow","handl","hardwar","however,","immedi","import","important.","impract","improv","inconsist","increas","increasingli","inferenc","inferencing.","instability.","issu","issues,","iter","it’","keep","kind","labor","lack","languag","larg","larger","lead","less","leverag","limit","llm","load","make","mani","match","matrix","matter","maxim","memori","memory\"","memory)","mind,","mix","model","model,","model.","models,","models.","models:","more","moreover,","much","multipl","necess","need","number","offer","offload","oper","operations,","optim","optimally,","option—it’","outputs,","overal","parallel","part","particularli","perform","performance,","performance.","performance:","place","portion","possibl","possible,","potenti","power","process","process.","processing,","quick","ram,","ram.","rang","real","reduc","reduced,","regard","requir","required,","resourc","resources.","respons","result","revert","run","scalability:","scale","select","seriou","sever","sheer","signific","significantly.","simultaneously.","size","size,","slow","slowdown:","slower","smaller","sole","space","specif","speed","speed.","speed:","step","storag","strength","struggl","substanti","such","suit","swap","system","take","tasks,","therebi","therefore,","thousand","time","times.","understand","units).","unleash","unlik","unlock","up","us","used.","util","utilization:","vari","veri","virtual","volum","vram","vram,","way","well","wide","without","work","workload"],"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":["\"full","\"gpu","\"like","(16","(32","(64","(8","13b","16","28","3.1","32","34","34b","35","35.86","40","64","70b","7b","8","accur","accuraci","accuracy.","advanc","algorithm","all,","ampl","analysis.","approxim","around","attempt","avail","balanc","base","best","better","between","both","boundari","burden","capabilities,","capabilities.","capabl","capacity,","case,","categor","categori","cautiou","challenges.","choic","choice.","choos","close","closely.","code","codellama","combin","comfort","compat","complex","compromis","comput","confid","configur","consid","constraints,","cost","crash","critic","degrad","deliv","demand","depend","design","detail","detail,","detail.","determin","develop","development.","due","each","edg","effectively,","effectiveness.","effici","efficiency.","end","enhanc","ensur","entir","error.","especi","essenti","even","example,","exce","exceed","experi","fall","fast","find","full","fulli","further,","gain","gb","gb,","gb.","gener","gguf","good","gpu","gpu,","gpu.","gradual","guidelin","handl","handle,","hardwar","hardware.","help","here","high","higher","highest","highli","however,","i'v","ideal","improv","increas","indic","instruct","instruct.q2_k.gguf","insuffici","intens","involv","it'","iter","it’","larg","larger","less","level","level.","limit","limits,","llama","lm","machine.\"","machine:","make","manag","maxim","maximum","mayb","meant","memori","memory)","memory,","memory.","memoryd","message,","minim","model","model,","models,","models.","moder","monitor","more","more)","more,","need","notic","offer","offload","on","optim","outputs,","over","paramet","perform","performance,","performance.","phind","place","possible,","possible,\"","possible.\"","possible:","potenti","power.","practic","precis","precision,","precision.","problem","process","program","provi","provid","push","q5_k_m","q6_k","qualiti","quantiz","quick","ram,","rang","range.","rapid","reduc","reliev","rememb","repres","requir","resourc","resources.","respons","result","results.","right","run","see","series.","signific","significantli","smoothli","solv","some,","spare.","specif","speed","speed.","stabil","start","step","still","strike","strong","studio","studio,","such","suggest","suit","suppos","system","system'","system,","system.","system’","task","tasks.","test","that’","thebloke/codellama","therefore,","though","tier.","tri","trial","up","upper","us","usag","usage.","util","v1.q4_k_s.gguf","v1.q4_k_s.gguf:","v1.q5_k_m.gguf","v1.q6_k_s.gguf","v1.q6_k_s.gguf:","v1.q8_0.gguf","v1.q8_0.gguf,","valu","vari","variat","veri","versatil","version","want","warn","well","whether","wide","with.","within","without","workload","worth","you'll","~14","~23.84","~28","~5.73"],"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":["13b","13b,","2,","34b","7b","7b,","abil","accur","accuraci","addit","advantag","align","altern","ambigu","applic","applications,","assistance,","avail","available.","base","basic","best","better","between","both","bridg","broadli","built","capabilities.","capabl","cater","centric","choic","clear","code","code,","code.","codellama","come","complement","complet","complex","comput","contextu","convert","corpu","deeper","delv","depend","describ","descript","design","detail","develop","developer’","development.","differ","directli","discov","each","effect","enough","environment.","especi","essential.","excel","exclus","expand","experienc","explor","fast","fine","flexibl","focus","follow","following,","found","foundation,","function","gap","gener","generation,","generation.","give","good","handl","help","here’","high","higher","highest","highli","ideal","identifi","incomplet","influenc","input","input.","instruct","instruct,","instructions,","instruct’","intens","intent.","interact","interpret","intric","introduc","intuit","invalu","isn’t","iter","it’","i’v","key","languag","languages.","larger","learn","level","llama","logic,","logic.","look","make","maximum","mean","memori","meta’","model","models,","model’","moder","more","moreover,","multi","multipl","natur","need","needs.","note","nuanc","offer","open","optim","option","out","outlin","output","outputs.","overview","particularli","performs,","plain","power","precis","problem","process","produc","program","project","projects,","projects.","prompts,","prompts.","provid","publicli","purpos","purpose,","python","python,","quickli","rang","reduc","refin","relev","requir","requirements,","requirements.","resourc","resources.","respons","ropes.","scenarios,","scenarios.","script","section","serv","signific","simpl","size","slower","snippets.","solving.","sourc","special","specif","specifi","speed","speed,","stand","statement","still","streamlin","strengths,","strengths.","strong","structure.","suggest","suit","suitabl","summar","syntax","system","tabl","task","tasks,","tasks.","technic","there’","those","tool","tool.","tools,","train","translat","tune","understand","uniqu","upon.","us","usag","usage,","use.","vagu","valuabl","vari","variat","variations,","variou","vast","versatil","want","whether","wide","work","workflow","worri","you’r"],"04_models_for_ coding/04_02_other_open_source_coding_models.html":["abil","accuraci","adapt","adaptability.","addit","allow","analysis,","analyz","anoth","assistant.","balanc","base","best","between","bigcod","broad","capabl","centric","changes.","choic","choice.","code","code.","codellama","coding,","coding.","coher","come","command","compar","comparison","complet","complex,","content","context","continu","creativ","cross","crucial.","data","deeper","design","develop","developers,","differ","dig","discov","divers","documentation,","domains,","dynam","each","effect","effici","efficiently.","enhanc","ensur","environ","environments,","environments.","especi","even","excel","exist","experi","explor","fast","featur","findings,","flexibl","focus","forward","frequent","function","gener","generation,","generation.","good","guid","handl","help","here’","high","highli","ideal","impress","includ","initiative,","instruct","instructions,","integr","involv","it’","i’ll","key","languag","languages,","languages.","leverag","li","llama","llm","llms,","look","lose","made","make","mani","manipulation,","memori","model","model'","model,","models,","moder","more","multi","multipl","natur","need","new","non","notabl","note","offer","on","ones.","ongo","open","optim","part","particularli","perform","performance.","plan","polyglot","precis","precision,","process","program","project","projects,","projects.","provid","purpos","purpose,","python","r","r,","rang","recommend","refin","releas","relev","reliabl","remain","requir","requirements,","resourc","robust","robust,","r’","scenarios,","scenarios.","scope","seamlessli","set","signific","simpli","snippet","solution.","sourc","specif","speed","speed,","standards.","standout","starcod","starcoder’","stay","step","strength","strengths:","strong","such","suit","support,","switch","systems,","tabl","tailor","task","tasks,","tasks.","tool","train","transit","tune","type","understand","unlik","updat","us","usag","usage,","util","valuabl","variat","varieti","variou","versatil","weak","well","whether","wide","without","work","workflows.","writing.","you'r","you’r"],"05_installation/05_01_installing_lm_studio.html":["abil","access","add,","adjust","allow","alway","api","app.","app’","area","area,","back","base","capabilities.","clean","cloud","command","commands,","compat","complet","complete,","comput","configur","configurations.","configure,","connect","custom","data","default","depend","deploy","design","develop","directories.","download","dure","easili","endpoints.","environment,","experi","explor","extens","facilit","first","follow","free,","friendli","go","grant","guid","hardwar","here,","imag","import","includ","initi","input","instal","installer.","interfac","interface.","key","languag","later","launch","line,","llm","lm","local","machines.","main","manag","menu","menu,","mess","model","models,","more","necessari","need","offici","onc","open","optim","option","organ","outputs.","path","perform","performance.","permiss","plan","preferences,","process.","prompt","provid","readi","resourc","run","run.","scientist","screen","sections,","select","services,","set","settings.","setup","setup,","start","step.","studio","studio,","such","take","test","through","time","time,","tool","up","upon","user","variou","version","view","website.","without","workspac","workspace.","you’ll"],"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":["\"default","access","advantag","allow","appl","applic","balanc","callout:","capabilities.","chip,","choos","click","computer,","configur","default","design","effect","effici","ensur","essenti","execution.","experi","folder.","follow","full","fulli","hardware,","hardware’","here’","import","it:","languag","larg","launch","leverag","llm","lm","local","m3","mac,","macbook","maco","macos\"","macos.\"","mac’","make","max","maxim","menu","model","navig","open","oper","optim","options.","perform","performance:","power","preset","preset,","preset:","presets,","pro","pro'","process","resourc","run","seamless","seamlessly.","select","set","settings:","silicon","silicon.","smooth","specif","specifications,","steps:","studio","studio:","such","systems.","take","under","us","usag","variou","work"],"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":["(llms)","(macbook","(n_ctx)","(n_ctx):","(n_gpu_layers)","(n_gpu_layers):","(n_predict)","(n_predict):","(n_threads)","(n_threads):","(or","(word","100","1024","16","16gb","2048","24","32gb","4","50","512","8","8gb","abund","adequ","adjust","afforded,","air","allevi","alloc","allocation:","allow","analysis.","appl","approach","appropri","avail","avoid","balanc","base","best","between","both","capabilities,","capabilities.","capabl","cheat","comprehens","comput","configur","consid","constrain","consum","consumpt","context","context.","control","core","cores)","cores,","cost","cpu","cpus.","crucial","dataset","detail","determin","diminish","distribut","each","effici","efficiency.","efficiently.","end","end,","enhanc","ensur","especi","essenti","excess","find","follow","follows:","full","gener","gpu","gpu,","gpu.","gpu’","guid","handl","hardwar","hardware.","help","here","here’","high","however,","impact","import","importance,","instability.","it'","it’","key","languag","larg","larger","layer","length","length,","leverag","limit","lm","load","load,","load.","local","long","longer","low","lower","m1","m3","mac","mac,","macbook","macs.","macs:","maintain","make","mani","match","max","maxim","medium","memori","memory.","model","models.","model’","modern","more","much","multi","necessary.","number","offload","once.","optim","option","out","output","overal","overload","part","perform","performance,","performance:","physic","potenti","prevent","privacy,","pro","process","process.","processing,","processing.","prompt.","provid","ram","ram),","ram).","ram,","recommend","reduc","resourc","resources.","respons","responses,","results,","return","run","set","sheet","shift","shorten","shorter","signific","significantli","silicon","slowdown","slowdowns.","smoothli","specif","speed","speed,","speed.","still","studio","studio,","studio.","suffici","system","system.","system:","systems,","tabl","tailor","text","thread","three","time","time.","token","type","understand","understanding.","up","us","usag","usage.","util","vital","without","words)"],"05_installation/05_04_picking_models_in_lm_studio.html":["\"full","(llms).","(macos,","(vram).","acceler","addit","alloc","allow","amount","applic","architecture,","attempt","avail","balanc","becom","befor","best","better","between","bottleneck","capabilities.","capabl","case","cases,","check","choos","choose.","close","comfort","complex","comput","computations,","computations.","computer:","computing,","consid","consider:","consist","core","cores,","cpu","cpu.","crucial","data","decis","demand","depend","design","differ","disk","distinguish","drastic","due","dynam","effect","effectively.","effici","enhanc","ensur","entir","essenti","establish","exce","ey","factor","faster","few","fewer","file","fit","full","fulli","gaug","gener","gpu","gpu'","gpu,","gpu:","gpus,","gpu’","gradual","greatli","guid","handl","hardwar","hardware.","help","however,","ideal","identifi","impact","import","includ","increas","indic","inferenc","inferencing.","inferencing:","influenc","inform","inputs.","involv","issu","it'","it,","it’","keep","key","know","languag","larg","larger","lead","less","leverag","limit","limit.","limits,","limits:","linux).","llm","llms,","llms.","lm","load","longer","make","manag","maxim","mean","memori","memory,","messag","model","model'","model,","models,","models.","model’","monitor","more","move","much","necessari","need","new","nvidia","occupies,","offload","offload\"","offload,","offloading.","offloading:","offs:","onc","oper","operations,","opt","optim","output","overhead","parallel","paramet","part","particularli","perform","performance,","performance.","performance:","pick","point","possibl","possible,","possible:","potenti","practic","precis","predict","prepar","process","process,","processing,","purpos","ram","rang","real","refer","reli","relianc","remain","requir","respons","result","right","run","runtim","scale","scenario","select","significantli","size","size,","slower","small,","smaller","smi","space","space.","start","storag","strain.","structur","studio","studio,","studio.","such","suggest","suggest.","swap","system","system'","system—can","system’","tasks,","temporari","that,","those","thousand","time","times.","tip","tool","trade","train","understand","unsur","up:","us","usag","usage,","usage.","usage—th","variat","vary.","versatil","vram","vram,","vram.","vs.","whether","wide","windows,","within","work","you'll","you’r","you’v"],"06_evaluation/06_01_testing_each_model.html":["13b","34b","accept","accuraci","adequaci","against","allow","approach","attent","balanc","best","choos","code","codellama","compar","complex","consid","detail","detail.","determin","each","evalu","expectations.","experi","finally,","first","follow","given","handl","highest","identifi","increas","instruct.q3_k_s.gguf","instruct.q4_k_m.gguf","instruct.q5_k_m.gguf","m3","macbook","max.","meet","model","model.","models,","more","needs,","needs.","next,","note","offer","on","one.","optim","output","pay","perform","precis","precision,","precision.","pro","qualiti","quick","requir","resourc","respons","responses.","slower","solut","specif","speed","speed,","start","suit","switch","systemat","task","tasks.","test","time","typic","uniqu","usage,","well","whether","work","workflow"],"06_evaluation/06_02_evaluating_models.html":["(usest","1.","2.","3.","4.","5.","6.","abil","accur","accuraci","accuracy,","accuracy:","ad","add","adher","aim","allow","app","applic","application,","applications,","area","aspect","assess","assistance.","avail","balanc","best","capabl","challeng","check","choos","chosen","clarity:","clear","code","code,","code.","codellama","collabor","comment","compel","compet","complet","complex","complexities,","compon","components,","comprehens","consist","content.","conventions.","correct","correctness:","cpu,","creat","critic","crucial","current","date","delet","detail","detailed,","determin","develop","development,","development.","display","dure","each","eas","easier","effici","empti","ensur","environments.","error","essenti","evalu","expected.","factors,","features:","field","firstly,","follow","form","fronts:","function","functionality.","functionality:","fundament","furthermore,","gener","goal","gpu,","handl","hardwar","help","high","homepag","hook","hooks,","identifi","impact","implement","import","includ","inclus","industri","inputs,","instruct","involv","issu","it.","iter","javascript","key","lastly,","list","logic","m3","macbook","maintain","maintain,","manag","management.","max","max,","max.","measur","meet","metric","metrics.","model","model'","models'","modern","monitor","multipl","need","needs.","new","note","note,","notes,","notes.","offer","on","optim","option","outputs.","paradigms,","perform","practices,","practices.","primari","pro","process.","produc","prompt","prompt,","prompt.","prompt?","proper","provid","qualiti","quickli","ram","react","react.","readabl","real","reasons.","reliabl","requir","requirements,","resourc","resources,","respons","robust","run","sever","simpl","softwar","solut","specif","specifi","speed,","standard","standards.","state","state,","stay","submissions.","such","suit","syntax","systemat","take","taken","task","term","test","thoroughli","time","time:","titl","understand","uniqu","up","us","usabl","usag","usage.","usage:","useeffect)","user","variou","verifi","vital","well","without","work","world","write"],"07_best_practices/07_01_best_practices_for_running_local_llms.html":["best","llm","local","locally.","offer","practic","run","section"],"08_prompts/08_01_collection_of_prompts.html":["code","collect","differ","prompt","provid","section","tasks."],"08_prompts/08_02_example_create_shell_script.html":["!=","\"","\"\"","\"#","\"$(basenam","\"$chapter_name\"","\"$chapter_name\")","\"$dir_path\"","\"$dir_path\"/*;","\"$entry\"","\"$entry\")","\"$entry\")\"","\"$entry\")\")","\"$file_path\"","\"$h1_title\"","\"$input\"","\"${indent}","\"${indent}*","\".\"","\"can","\"remov","\"updat","\"you","#","#!/bin/bash","$(basenam","$1","$chapter_name\"","$indent\"","${{","&&","'","'#","'^#","'github","'main'","'s/^#","'s/^[0","'s/_/","'{for(i=1;i","*","*.md","+x","./generate_summary.sh",".github,",".github/workflows/update_summary.yml","//')","/g')","1","9]\\{2\\}_//')","9]{2}_","9]{2}_//;","==","=~",">","[","[$h1_title]($dir_path/$file_name)\"","[[","]","];","]];","^[0","_","action","actions/checkout","actions/checkout@v2","actions[bot]'","actions[bot]@users.noreply.github.com'","ad","adapt","add","alreadi","alway","approach","arguments:","around.","ask","authent","autom","automatically.","awk","back","basic","better.","block","branch","branch.","branches:","break","build","call","capit","case","case.","case:","chang","changes,","changes:","chapter","chapter_name=\"$1\"","chapter_name=$(clean_chapter_nam","chapter_name=$(echo","chapter_name=$(to_title_cas","chatgpt","check","checkout","chmod","ci","clean","clean_chapter_nam","clean_chapter_name()","cleaner","cleaning:","code","code,","code.","code.\"","code:","comment","commit","commit.","config","configur","contain","content","content:","contents\"","convert","copilot","correct","correct.md","correctli","correctly.","correctly.\"","creat","d","date","default","dir_path=\"$1\"","directori","directories:","directory.\"","document","done","doubl","dynam","e","each","echo","edit","effectively:","effectively?\"","effici","elif","email","end","ensur","entri","env:","even","everyth","evolves.","exampl","example,","excel","exclus","execut","executed,","explain","explan","extract","extract_h1_title()","f","few","fi","file","file'","file,","file:","file?\"","file_name=$(basenam","file_path=\"$1\"","files:","final","finally,","find","first","folder","folder'","follow","following:","forget","format","found","fulli","function","function:","further","g","gener","generate_summary.sh","git","git:","github","github_token","global","go","goal","grep","guide'","guide.","h1","h1_title=$(extract_h1_titl","h1_title=$(grep","handl","happi","hard","heavili","help","hood,","hour","i'll","ignor","ignore,","images,","import","includ","indent=\"$2\"","input","input=\"$1\"","instruct","integr","it'","it.","ithub_token:","jobs:","kind","latest","let'","letter","line","links.","list","list_structur","list_structure()","llm","local","look","loos","lowercas","m","main","make","manually.","markdown","match","minutes.","modifi","more","more,","move","name","name.\"","name:","names:","names?","need","next,","normal","now","now,","now.\"","number","on","on:","onc","optim","out","output","pain","part","path","point,","prefix","problem","process","project.","projects,","prompt","provid","pull","push","push.","push:","readm","realiz","rebuild","recurs","recursively:","recursively?\"","regex","regex.","remain","remov","repetit","repositori","repository,","repository.","repository:","respond","respons","response,","rest","result","results,","returns:","revis","root","run","run:","s/_/","saw","scan","scratch.","script","script:","secrets.github_token","section","sed","see","self","set","shell","simple,","smaller","snake","someth","start","step","steps:","still","string","studio","summari","summary.md","summary.md\"","summary:","sure","switch","system:","tabl","task","tasks.","termin","test","thing","this,","this:","through","time","titl","title:","titles:","to_title_case()","toc","toc.","travers","trigger","ubuntu","under","underscor","understand","up","updat","us","used.","user.email","user.nam","usernam","uses:","variou","via","visual","walk","want","wasn't","way","way,","we'r","word","worked,","workflow","works:","writ","write","yourself","{","|","}","}}"],"09_advanced_usage/09_01_advanced_model_tuning.html":["advanc","cover","model","models.","section","techniqu","tune"],"09_advanced_usage/09_02_integrating_llms_into_workflows.html":["develop","discuss","integr","llm","section","workflow","workflows."],"09_advanced_usage/09_03_custom_built_pcs.html":["(13b","10gb","16gb","1tb","2tb","32gb","4060","4070","4090","64gb","a100,","abil","access,","accommod","add","adequ","afford","aim","airflow","allow","amount","avoid","balanc","be","better","bottleneck","budget.","build","built","bulk","capac","capacity,","case","case.","cases,","caus","certain","choos","clock","complete.","compon","components.","conclusion:","concurrently,","consid","consider","considerations:","cool","cooler","cooling,","cooling:","core","count","cpu","cpu,","cpus,","critic","custom","data","data,","dataset","datasets,","datasets.","demand","depend","do","don’t","efficiently.","end","enough","ensur","especi","essenti","even","evolve,","expand","expandability:","expansions.","expensive.","extens","factor","fast","finally,","first,","flexibl","futur","gains,","gener","good","gpu","gpu,","gpu.","gpus,","great","greatest","grow.","handl","headroom","heavi","heavili","here","high","higher","hot","impact","import","imposs","infer","intens","interfac","invest","it'","it’","keep","key","lane","larg","larger","leav","less","light","limits,","llm","llms,","llms.","load","load.","loading:","local","look","maintain","major","make","manag","medium","memori","memory,","mid","mind,","mind:","minimum","model","models,","models.","modern","more","more.","motherboard","move","multipl","multitask","necessari","need","newer","nvme","offer","offload","ones,","operations.","optim","out","overal","parallel","particularli","pc","pcie","perform","performance.","plan","power","power:","preprocessing,","price.","priorit","process","processing.","proper","provid","psu","quick","ram","rang","recommend","reduc","relat","reli","reliabl","rtx","run","simultan","singl","size","size.","size:","slot","slowdown","small","smaller","solid","specif","speed","speed.","speed:","ssd","ssd:","storag","storage,","such","suffic","sufficient,","suppli","supply:","support","sure","sustain","system","task","tasks,","tasks.","tier","typic","under","unless","up),","upgrad","upgrades.","upgrades:","us","vram","vs.","want","wattag","with.","work","workload","workloads.","you'll","you'r"],"10_benchmarks/10_01_geekbench_ai_benchmark.html":["$100","(best","(core","(in","(ips)","(ips).","(onnx","(openvino","+","/","1","1,370","1.","100","2","2,170","2.","3","3.","3d","4.","4070","5.","53.4","5800x","6,110","6.","7","817.9","9.57","=","[backend]","[current","[framework]","[name","[name]","[provid","[score]","[size","[system","[task","abil","accord","ad","ai","ai.","allow","amount","analysis,","animation.","anoth","appl","apple'","applic","apps,","ar.","ar/vr.","art","artist","autonom","backend","backend,","backend.","base","below","below.","benchmark","benchmarks.","benchmarksdownload","benefit","best","between","block","both","bound","breakdown","brief","button","button.","calcul","capabl","categor","centric","chatbots,","choic","clarity.","classif","classifi","classification:","clear","click","clone","command:","communication.","compar","comparison","complet","comput","configurations,","consist","contents,","contribut","contributions!","contributor","copi","core","cost.","cpu","cpu.","cpu:","creat","creativ","critic","crucial","current","decid","deep","demonstr","depth","describ","descript","detail","detect","detection,","detection.","detection:","dev.git","differ","directli","directml","directml)","distanc","document","dollar,","driving.","each","easier","edit","effect","efficiently,","encourag","engin","enhanc","ensur","essenti","estim","estimation.","estimation:","evalu","execut","explain","face","facial","fast","field","file","fill","find","fit","flexibl","focus","follow","fork","format","formula","formula:","forward","framework","framework)","framework)**","geekbench","generation.","get","git","github","githubfirst,","give","gpu","gpu:","guid","half","handl","hardware,","help","here","here'","here,","highlight","https://github.com/jessefreeman/ai","i'v","identifi","identification.","imag","images,","import","important:","improv","includ","inclusion.","infer","inference,","inference.","insight","instruct","intel","invested.","ip","key","kips)","languag","languages,","lastly,","learn","leverag","local","look","m3","macbook","machin","maco","macos,","main","make","markdown.","max","max'","meaning","measur","medic","method","ml","ml.","model","models,","more","motherboard:","motion","much","multilingu","name]","navig","neural","new","nuc9v7qnx","object","offer","onc","onnx","openvino","optim","organization.","out","page.","parallel","particularli","peopl","per","perform","performance.","person","photo","photographi","pleas","pose","posit","precision,","precision:","previou","price","price)","price.","price:","price]","pro","process","processing,","project","project,","project.","project’","provid","pull","quantiz","quantized.","quantized:","ram:","rank","ratio","real","recognit","rel","relev","repositori","repository.","repositoryclon","request","resolut","resolution,","resolution:","result","results.","resultscollect","rtx","run","ryzen","scene,","score","scores,","section","secur","security,","segment","segmentation:","short","show","showcas","silicon","simul","singl","spam","spec","specs,","spend.","spent","spent,","split","standard","step","straightforward","strength","strong","style","submiss","submissions.","submit","summari","super","sure","system","system'","system,","systems,","systems.","system’","tabl","task","tasks,","tasks.","templat","template,","term","test","tested,","tests,","tests.","tests.]","text","they'r","three","tool","top","topic,","total","tracking,","transfer","transfer.","transfer:","translat","translation,","translation:","type","type]","types.","understand","us","user","valu","variou","vehicles,","video,","videos,","visual","vital","vram]","way","well","wide","window","within","workload","workloads.","world","you'r","you’v","|","~$1,169.51","~$2,999","~$814.96","~1.17","~168.1","~185.6","~203.7","~27.3","~4.57","×","“fork”"],"10_benchmarks/10_02_ryzen_7_5800_32.html":["$1,169.51","$1,396.96","$129.99","$179.92","$217.99","$262.36","$349.99","$653.24","$699","$73.99","(2","(wa","+","1.","1.38","116.6","125.1","12gb","12gb,","15,148","15.2","16gb)","17.1","175.9","180.6","19,608","2.","2.17","2022)","2022.","2024,","25,495","299.5","3,313","3,582","3.","32gb","3600","37,950","4.","4070","42.2","432.3","44.5","476.0","481.6","5,019","5.","53.4","5800x","5800x'","5800x,","6.","7","7,593","728.1","863.3","880.8","884","abil","acceler","acceleration.","ai","amd","asu","b550","backend","backend,","backend.","benchmark","classif","complex","compon","corsair","cpu","cpu:","current","ddr4","depth","detect","detection.","directml","edit","effici","estim","estimation,","evalu","even","excel","except","face","focus","founder","framework","framework,","framework.","gddr6x","geforc","gpu","gpu:","half","handl","hardwar","highlight","imag","ip","kip","lpx","make","measur","model","models,","motherboard:","nvidia","object","onnx","openvino","option","origin","overview","pair","parallel","particularli","perform","pose","power","precis","precision:","price","price:","process","provid","purchas","quantiz","quantized:","ram","ram:","reflect","reliabl","resolut","rog","rtx","ryzen","score","singl","specs:","strix","strong","style","super","super.","system","task","tasks,","tasks.","test","top","total","transfer","transfer,","us","vengeanc","without","workload","workloads,","x","–"],"10_benchmarks/10_03_m3_max.html":["$2,999","(14","+","1.","12,570","120.1","13,358","13,410","13,411","13,896","150.6","16.2","181.3","19.3","191.7","2,457","2.","2024.","203.2","205.0","261.7","3,841","3.","30","313.7","36gb","37.9","4,472","4.","460.6","5,356","5.","557.2","6.","6.11","712.5","75.5","755.7","81.0","817.9","ai","appl","apple’","backend.","balanc","benchmark","both","capabilities.","choic","classif","classification,","consist","core","cpu","cpu)","cpu:","current","depth","detect","detection,","develop","display:","ecosystem.","efficiency,","engin","equip","estim","estimation,","evalu","excel","face","framework","framework,","gpu","gpu'","gpu,","gpu:","half","hardwar","highlight","imag","impress","inch","ip","kip","liquid","m3","macbook","maco","max","max'","max,","measur","memori","ml","models.","neural","object","offer","overview","overview**","parallel","particularli","perform","pose","power","precis","precision:","price","price*:","pro","process","processing.","provid","quantized:","ram","ram:","reflect","resolut","resolution,","result","retina","score","show","singl","solid","specs:","speed","strong","style","super","system","task","test","text","top","total","transfer","transfer,","under","unifi","us","variou","well","within","work","workload","workloads.","xdr"],"10_benchmarks/10_04_nuc9v7qnx.html":["$119.99","$324.99","$369.98","$814.96","&","(2x32gb)","(nuc9i7qn","+","1.","1.37","10,609","10.6","109.1","114.8","13,112","13.4","196.6","2,283","2,353","2.","2024.","21,260","236.0","240.1","277.4","3,434","3,490","3.","32.6","3200mhz","34.4","36.0","380.2","4,812","4.","4060","4060'","491.9","5.","592","6.","64gb","75.0","83.6","874.2","8gb","8gb,","9.57","9750h","9850h","9850h'","9th","acceleration.","ai","alreadi","anoth","applications.","backend.","be","benchmark","cl22","compact","compon","computer.","core","cpu","crucial","ddr4","deliv","demonstr","depth","design","despit","detect","detection,","directml","estim","estimation,","extrem","face","framework","framework,","gddr6","geforc","gener","gpu","gpu:","great","half","hardwar","highlight","i7","imag","intel","ip","kip","kit","kit)","make","measur","medium","models,","models.","motherboard:","msi","nuc","nuc9v7qnx","nuc9v7qnx,","nvidia","object","only,","onnx","openvino","option","overview","pair","particularli","perform","pose","power","precis","precision:","price","price*:","process","purchas","quantiz","quantized:","ram","ram:","reflect","resolut","resolution,","rtx","scale","score","show","singl","small","solid","specs:","strong","style","super","system","task","tasks,","test","top","total","transfer","transfer,","us","variou","versatil","well","without","workloads.","–"],"11_additional_resources/11_01_tools.html":["4","access","ai","allow","api","assess","benchmark","both","building,","code","commun","complet","comprehens","continu","continue:","cpu","deploy","develop","devices,","download:","easi","easier","enabl","enhanc","environment.","evalu","extens","extension:","face","face'","features.","geekbench","gpt","gpt4all","gpt4all:","gpu","hug","integr","languag","learn","links.","list","llms.","lm","locally.","machin","make","manag","model","models,","nomic","offer","open","perform","platform","power","product","project","provid","reading,","recommend","run","scale","section","sourc","studio","studio:","suggest","tasks.","tool","tools,","training,","user","visual","vs","within"],"11_additional_resources/11_02_articles.html":["16gb","2","2,","50","`on","aaditya","advanc","allan","appl","apple'","applications.","articl","assist","benefit","benefits.","bhat:","both","bridg","cases.","code","code,","commun","comprehens","configur","configuration,","copilot","core","cost","cover","cpp","cpu","cuenca:","detail","developers.","discuss","drive","effect","efficiently.","enhanc","especi","explor","extens","faster","fine","fraser:","free","gener","gguf","gpu.","guid","hardwar","highlight","home","https://huggingface.co/:","includ","insight","instal","instruct","instruct—tailor","integr","introduct","iteration,","lam:","languag","laptop","larg","learn","leverag","links.","list","llama","llama,","llama:","llm","local","locally,","m1","mac","macs,","make","meta'","ml","ml,","model","models,","models.","na","nas,","nas:","natur","network","network.","offer","offic","open","optim","optimization.","option","pedro","perform","power","practic","privacy,","process","programming,","project","provid","python","python,","ram,","read","recommend","run","ryan","savings,","section","secur","select","selection,","server","set","setup","sharing.","shaw","show","silicon","silicon,","simon","simple,","small","sourc","step","stewart:","swift","talebi:","tasks.","techniqu","through","tip","tool","train","tune","up","us","usag","variants—base,","variou","vinc","vs","window","witt:"],"11_additional_resources/11_03_in_the_news.html":["15,","16,","19,","2024:","3","3,","3d","access","accuraci","advanc","ai","allow","amid","annot","announc","articl","audienc","august","avail","barrier","base","ben","between","broader","build","carl","changer","cloud","code","collaboration.","commun","consider","content","creat","custom","data","data,","dean","detail","develop","dickson:","discord","editing,","editor","elimin","enabl","encourag","entri","ethic","evalu","evaluation.","evaluator,","exist","expand","face","face’","fair","featur","feeds:","filter","franzen:","friendli","game","genai","gener","googl","guid","high","hug","human","imag","imagen","improv","industry.","inpainting,","instant","integr","interest","interface,","interface.","introduc","latest","launch","lerobot","list","llm","lower","made","make","manual","messag","meta","meta'","michael","midjourney","mirror","mix","model","model,","more","multiplay","need","new","novel","nuñez:","open","option","outpainting,","platform","platforms.","power","precis","prompt","provid","qualiti","quietli","reaction","reduc","releas","relianc","research","robot","robot:","robots,","rss","scalabl","seamless","section","self","significantli","simpl","skills,","system","takahashi:","taught","text","through","tool","train","tutori","u.s.","unifi","unveil","user","users,","web","wider","within","without"]},"length":37},"tokenStore":{"root":{"1":{"0":{"0":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"m":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}},"2":{"4":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},"docs":{}},"9":{"docs":{},".":{"1":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"docs":{},"b":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308}}},"g":{"docs":{},"b":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},",":{"6":{"0":{"9":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}},"docs":{}},".":{"6":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"1":{"4":{"docs":{},".":{"8":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"6":{"docs":{},".":{"6":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"docs":{}},"2":{"0":{"docs":{},".":{"1":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"5":{"docs":{},".":{"1":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"docs":{},"g":{"docs":{},"b":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}},",":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}}},",":{"5":{"7":{"0":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}},"docs":{}},"docs":{}}},"3":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}},"b":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.018867924528301886}},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}},",":{"1":{"1":{"2":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}},"3":{"5":{"8":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}},"docs":{}},"4":{"1":{"0":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"1":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}},"docs":{}},"8":{"9":{"6":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}},"docs":{}},"docs":{}},".":{"4":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"5":{"0":{"docs":{},".":{"6":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{},",":{"1":{"4":{"8":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}},"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}},".":{"2":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"6":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.011538461538461539},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}},"g":{"docs":{},"b":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.006389776357827476},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}},")":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}}},".":{"2":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}},",":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"7":{"5":{"docs":{},".":{"9":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"docs":{},".":{"1":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"8":{"0":{"docs":{},".":{"6":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"1":{"docs":{},".":{"3":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{}},"9":{"1":{"docs":{},".":{"7":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"6":{"docs":{},".":{"6":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"docs":{},",":{"6":{"0":{"8":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}},"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}},".":{"3":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}},"b":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308}}},":":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},".":{"3":{"7":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"8":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}},"t":{"docs":{},"b":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},",":{"3":{"7":{"0":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"docs":{}},"docs":{}},"docs":{}}},"2":{"0":{"1":{"9":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"docs":{}},"2":{"2":{"docs":{},")":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.011594202898550725}}},".":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}},"4":{"docs":{},",":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},".":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},":":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.023148148148148147}}}},"docs":{}},"3":{"docs":{},".":{"2":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"4":{"8":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},"docs":{}},"5":{"docs":{},".":{"0":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{}},"1":{"docs":{},",":{"2":{"6":{"0":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}},"docs":{}}},"3":{"6":{"docs":{},".":{"0":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"docs":{}},"4":{"0":{"docs":{},".":{"1":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},"5":{"docs":{},",":{"4":{"9":{"5":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}},"docs":{}}},"6":{"1":{"docs":{},".":{"7":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{}},"7":{"7":{"docs":{},".":{"4":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"docs":{}},"8":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"9":{"9":{"docs":{},".":{"5":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"docs":{}},"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},":":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},",":{"1":{"7":{"0":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"docs":{}},"docs":{}},"2":{"8":{"3":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}},"3":{"5":{"3":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}},"4":{"5":{"7":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}},"docs":{}},"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},".":{"1":{"7":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}},"t":{"docs":{},"b":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"3":{"0":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112}}},"1":{"3":{"docs":{},".":{"7":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{}},"2":{"0":{"0":{"docs":{},"m":{"docs":{},"h":{"docs":{},"z":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}},"docs":{}},"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.011538461538461539},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00583941605839416}},"g":{"docs":{},"b":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362}}}},".":{"6":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"4":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"b":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0076481835564053535},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}},".":{"4":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"5":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}},".":{"8":{"6":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"docs":{}},"docs":{}}},"6":{"0":{"0":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{},"g":{"docs":{},"b":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409}}}},".":{"0":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"7":{"docs":{},",":{"9":{"5":{"0":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}},"docs":{}},".":{"9":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"8":{"0":{"docs":{},".":{"2":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"docs":{}},"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},":":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},".":{"1":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":2.5014598540145987}}},"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}},"d":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}},",":{"3":{"1":{"3":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}},"4":{"3":{"4":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"9":{"0":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}},"5":{"8":{"2":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}},"8":{"4":{"1":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}},"docs":{}},"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"4":{"0":{"6":{"0":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.012084592145015106}},"'":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}},"docs":{}},"7":{"0":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":1.1256038647342996}}},"docs":{}},"9":{"0":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},"docs":{}},"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621}}},"2":{"docs":{},".":{"2":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"3":{"2":{"docs":{},".":{"3":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"docs":{}},"4":{"docs":{},".":{"5":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"6":{"0":{"docs":{},".":{"6":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{}},"7":{"6":{"docs":{},".":{"0":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"docs":{}},"8":{"1":{"docs":{},".":{"6":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"docs":{}},"9":{"1":{"docs":{},".":{"9":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"docs":{}},"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},":":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}},",":{"4":{"7":{"2":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}},"docs":{}},"8":{"1":{"2":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}},"docs":{}}},"5":{"0":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}}},"1":{"2":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},"docs":{}},"3":{"docs":{},".":{"4":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"5":{"7":{"docs":{},".":{"2":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{}},"8":{"0":{"0":{"docs":{},"x":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":1.1169082125603864}},"'":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362}}},",":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}}},"docs":{}},"docs":{}},"9":{"2":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}},",":{"0":{"1":{"9":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}},"3":{"5":{"6":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}},"docs":{}},"docs":{}}},"6":{"4":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621}},"g":{"docs":{},"b":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553}}}}},"docs":{},".":{"1":{"1":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}},"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}},",":{"1":{"1":{"0":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"docs":{}},"docs":{}},"docs":{}}},"7":{"0":{"docs":{},"b":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.0072992700729927005}},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464}}}}},"1":{"2":{"docs":{},".":{"5":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{}},"2":{"8":{"docs":{},".":{"1":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"docs":{}},"5":{"5":{"docs":{},".":{"7":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{},".":{"0":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"5":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":1.1256038647342996}},"b":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155}},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}},",":{"5":{"9":{"3":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}},"docs":{}}},"8":{"1":{"7":{"docs":{},".":{"9":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"docs":{},".":{"0":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}}},"3":{"docs":{},".":{"6":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"6":{"3":{"docs":{},".":{"3":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"docs":{}},"7":{"4":{"docs":{},".":{"2":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}}},"docs":{}},"8":{"0":{"docs":{},".":{"8":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}}},"4":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}},"b":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.011538461538461539}}},"g":{"docs":{},"b":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.006389776357827476},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},",":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}},"9":{"7":{"5":{"0":{"docs":{},"h":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}},"docs":{}},"docs":{}},"8":{"5":{"0":{"docs":{},"h":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},"'":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}},"docs":{}},"docs":{}},"docs":{},"]":{"docs":{},"\\":{"docs":{},"{":{"2":{"docs":{},"\\":{"docs":{},"}":{"docs":{},"_":{"docs":{},"/":{"docs":{},"/":{"docs":{},"'":{"docs":{},")":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}}},"docs":{}}},"{":{"2":{"docs":{},"}":{"docs":{},"_":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},"/":{"docs":{},"/":{"docs":{},";":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}},"docs":{}}},".":{"5":{"7":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}},"t":{"docs":{},"h":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}},"docs":{},"(":{"1":{"3":{"docs":{},"b":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"4":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"6":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"docs":{}},"2":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}},"x":{"3":{"2":{"docs":{},"g":{"docs":{},"b":{"docs":{},")":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}},"docs":{}},"docs":{}}},"3":{"2":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"docs":{}},"4":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"6":{"4":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"docs":{}},"8":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"docs":{},"l":{"docs":{},"l":{"docs":{},"m":{"docs":{},"s":{"docs":{},")":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":2.0021141649048624},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},".":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}},"a":{"docs":{},"r":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}},"e":{"docs":{},"f":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"b":{"docs":{},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}}}},")":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"r":{"docs":{},"a":{"docs":{},"p":{"docs":{},"h":{"docs":{},"i":{"docs":{},"c":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}}},"n":{"docs":{},"l":{"docs":{},"p":{"docs":{},")":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}},"_":{"docs":{},"c":{"docs":{},"t":{"docs":{},"x":{"docs":{},")":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}},":":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}},"g":{"docs":{},"p":{"docs":{},"u":{"docs":{},"_":{"docs":{},"l":{"docs":{},"a":{"docs":{},"y":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},")":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}},":":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}},":":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}}}}}},"t":{"docs":{},"h":{"docs":{},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"d":{"docs":{},"s":{"docs":{},")":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}},":":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}}}}}}},"u":{"docs":{},"c":{"9":{"docs":{},"i":{"7":{"docs":{},"q":{"docs":{},"n":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}},"docs":{}}},"docs":{}}}},"r":{"docs":{},"&":{"docs":{},"d":{"docs":{},")":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"a":{"docs":{},"m":{"docs":{},")":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}},"t":{"docs":{},"o":{"docs":{},"d":{"docs":{},"o":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"h":{"docs":{},"e":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}},"e":{"docs":{},".":{"docs":{},"g":{"docs":{},".":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}},"k":{"docs":{},")":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}},"m":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{},"u":{"docs":{},"m":{"docs":{},")":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}}}}},"a":{"docs":{},"c":{"docs":{},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}}}}}},"o":{"docs":{},"s":{"docs":{},",":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}},"q":{"docs":{},")":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}},"a":{"docs":{},"t":{"docs":{},")":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}},"a":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}},"p":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"x":{"docs":{},".":{"docs":{},")":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}}}}}},"i":{"docs":{},"n":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"t":{"docs":{},"e":{"docs":{},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}}}},"p":{"docs":{},"s":{"docs":{},")":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.010180995475113122}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"p":{"docs":{},"t":{"docs":{},"q":{"docs":{},")":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}},"u":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"s":{"docs":{},")":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}},"s":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}},"w":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}},"o":{"docs":{},"r":{"docs":{},"d":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}},"a":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.011594202898550725}}}},"~":{"1":{"6":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"docs":{}},"3":{"2":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"docs":{}},"docs":{}},"d":{"docs":{},"u":{"docs":{},"r":{"docs":{},"e":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}}},"c":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"l":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041}}}}}},"s":{"docs":{},"u":{"docs":{},"c":{"docs":{},"h":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}},"v":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{},")":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},",":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}},"o":{"docs":{},"r":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}}},"n":{"docs":{},"n":{"docs":{},"x":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}}},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"v":{"docs":{},"i":{"docs":{},"n":{"docs":{},"o":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}},"a":{"1":{"0":{"0":{"docs":{},",":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"docs":{}},"docs":{}},"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"d":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"i":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}},"v":{"docs":{},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{},"a":{"docs":{},"g":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}},"c":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"09_advanced_usage/09_01_advanced_model_tuning.html":{"ref":"09_advanced_usage/09_01_advanced_model_tuning.html","tf":3.5555555555555554},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}}},"j":{"docs":{},"u":{"docs":{},"s":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}},"!":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}},"a":{"docs":{},"p":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.009324009324009324},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"a":{"docs":{},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},".":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}}}}}}},"e":{"docs":{},"q":{"docs":{},"u":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},"a":{"docs":{},"c":{"docs":{},"i":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}}}}},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}}}}}},"i":{"docs":{"./":{"ref":"./","tf":0.034013605442176874},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.024886877828054297},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.04057971014492753},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.03380281690140845},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.03927492447129909},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.06060606060606061},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.032407407407407406}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},".":{"docs":{},"d":{"docs":{},"e":{"docs":{},"v":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}}}}},")":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"r":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}},"m":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"n":{"docs":{},"y":{"docs":{},"o":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}},"t":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},"e":{"docs":{},"r":{"docs":{},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}}}},"a":{"docs":{},"l":{"docs":{},"y":{"docs":{},"s":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738}}}}}},"z":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}},"s":{"docs":{},"w":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159}}}}}},"d":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}},"n":{"docs":{},"o":{"docs":{},"t":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}},"u":{"docs":{},"n":{"docs":{},"c":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}},"b":{"docs":{},"r":{"docs":{},"e":{"docs":{},"v":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}}}},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}},"l":{"docs":{},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.00859106529209622},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.007692307692307693},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}},"c":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.011182108626198083},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},":":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}}}}},"u":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}},",":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"e":{"docs":{},"v":{"docs":{},"i":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}},"a":{"docs":{},"n":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"w":{"docs":{},"a":{"docs":{},"y":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}},"i":{"docs":{},"g":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767}}}}},"o":{"docs":{},"n":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}},"g":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}}}},"g":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"m":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}}}}}}}}},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"d":{"docs":{},"i":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}}}},"m":{"docs":{},"a":{"docs":{},"z":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}},"o":{"docs":{},"u":{"docs":{},"n":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"p":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}}}},"b":{"docs":{},"i":{"docs":{},"g":{"docs":{},"u":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}},"d":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"i":{"docs":{},"d":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}},"v":{"docs":{},"o":{"docs":{},"i":{"docs":{},"d":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"a":{"docs":{},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}}}},"l":{"docs":{},"e":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}}}}}},"c":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.013888888888888888}},"i":{"docs":{},"b":{"docs":{},"l":{"docs":{},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}},",":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"l":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362}},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"d":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}}}}}}},"p":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}},"u":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"a":{"docs":{},"c":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.009230769230769232},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.008576329331046312},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.018867924528301886},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}},"y":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}}},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},":":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}},"t":{"docs":{},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}}}}},"o":{"docs":{},"u":{"docs":{},"n":{"docs":{},"t":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}},"m":{"docs":{},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}}}}}},"r":{"docs":{},"d":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"t":{"docs":{},"u":{"docs":{},"a":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}},"i":{"docs":{},"v":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525}}},"o":{"docs":{},"n":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},"s":{"docs":{},"/":{"docs":{},"c":{"docs":{},"h":{"docs":{},"e":{"docs":{},"c":{"docs":{},"k":{"docs":{},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"@":{"docs":{},"v":{"2":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"docs":{}}}}}}}}}}}},"[":{"docs":{},"b":{"docs":{},"o":{"docs":{},"t":{"docs":{},"]":{"docs":{},"'":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"@":{"docs":{},"u":{"docs":{},"s":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},".":{"docs":{},"n":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"p":{"docs":{},"l":{"docs":{},"y":{"docs":{},".":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"'":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"h":{"docs":{},"i":{"docs":{},"e":{"docs":{},"v":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446}},"e":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}},"r":{"docs":{},"o":{"docs":{},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},"c":{"docs":{},"h":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}},"e":{"docs":{},",":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}}}}}}},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"s":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"c":{"docs":{},"l":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":10.019762845849803},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"e":{"docs":{},"a":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419}}}}},"i":{"docs":{},"s":{"docs":{},"e":{"docs":{},":":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}},"g":{"docs":{},"u":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"/":{"docs":{},"v":{"docs":{},"r":{"docs":{},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"s":{"docs":{},"s":{"docs":{},"o":{"docs":{},"c":{"docs":{},"i":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}},"u":{"docs":{},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}},"t":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}},".":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}}},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}},"k":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.007399577167019027}}},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.007692307692307693}}}}}},"i":{"docs":{},"d":{"docs":{},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}},"y":{"docs":{},"m":{"docs":{},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}}},"u":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"s":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}}}}},"p":{"docs":{},"p":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"l":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":2.0393700787401574},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":2.011182108626198},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.011267605633802818},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.011857707509881422}},"i":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.012006861063464836}},"c":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.008456659619450317},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.009111617312072893},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}}}},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}}}}},"e":{"docs":{},"'":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}}},"’":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}}},"r":{"docs":{},"o":{"docs":{},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"p":{"docs":{},"r":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738}}}}},"x":{"docs":{},"i":{"docs":{},"m":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}},".":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}},"’":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}},"s":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"i":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}},"u":{"docs":{},"t":{"docs":{},"o":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629}},"m":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"y":{"docs":{},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}}}}},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}}}},"h":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},"d":{"docs":{},"i":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}},"g":{"docs":{},"u":{"docs":{},"s":{"docs":{},"t":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.023148148148148147}}}}}}},"f":{"docs":{},"f":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}},"o":{"docs":{},"r":{"docs":{},"d":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}}}}},"t":{"docs":{},"t":{"docs":{},"e":{"docs":{},"m":{"docs":{},"p":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},"n":{"docs":{},"t":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}},"r":{"docs":{},"i":{"docs":{},"b":{"docs":{},"u":{"docs":{},"t":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}}}}},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}},"k":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}}}},"a":{"docs":{},"d":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},"a":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}}},"b":{"5":{"5":{"0":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}},"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},"s":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.008670520231213872},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.018867924528301886},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.01282051282051282},"07_best_practices/07_01_best_practices_for_running_local_llms.html":{"ref":"07_best_practices/07_01_best_practices_for_running_local_llms.html","tf":2.1666666666666665},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.005656108597285068}}}},"h":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}}}},"n":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"c":{"docs":{},"h":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"k":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.02828054298642534},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.017391304347826087},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.016901408450704224},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.01812688821752266},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"d":{"docs":{},"o":{"docs":{},"w":{"docs":{},"n":{"docs":{},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}}}}}}}},"e":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"s":{"docs":{},".":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}}},"t":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.012307692307692308},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.014450867052023121},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"w":{"docs":{},"e":{"docs":{},"e":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.011560693641618497},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00583941605839416},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}},"r":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}},"u":{"docs":{},"i":{"docs":{},"l":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}}},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":3.3359511343804535}}}}},"l":{"docs":{},"k":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"r":{"docs":{},"d":{"docs":{},"e":{"docs":{},"n":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}},"d":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},".":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}},"t":{"docs":{},"t":{"docs":{},"o":{"docs":{},"n":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"k":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"d":{"docs":{},"o":{"docs":{},"w":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"l":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}},"i":{"docs":{},"d":{"docs":{},"g":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}},"e":{"docs":{},"f":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"h":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}},"e":{"docs":{},"s":{"docs":{},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}}},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{},"w":{"docs":{},"i":{"docs":{},"d":{"docs":{},"t":{"docs":{},"h":{"docs":{},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}}}}}},"s":{"docs":{},"e":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.013888888888888888},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0076481835564053535},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}},"i":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}},"c":{"docs":{},"k":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}}}},"l":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.009230769230769232},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.023121387283236993},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.008759124087591242},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.007987220447284345},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.018867924528301886},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}}}}},"t":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}},"r":{"docs":{},"r":{"docs":{},"i":{"docs":{},"e":{"docs":{},"r":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}}},"g":{"docs":{},"g":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}},"c":{"docs":{},"o":{"docs":{},"d":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}}}}}},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473}},")":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}},"a":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}},"e":{"docs":{},"s":{"docs":{},")":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}}},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},"t":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"c":{"docs":{},"k":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}}}}},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}}},"o":{"docs":{},"s":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}},"l":{"docs":{},"o":{"docs":{},"c":{"docs":{},"k":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"h":{"docs":{},"a":{"docs":{},"t":{"docs":{},":":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}},"c":{"docs":{},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.02127659574468085},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.01509433962264151},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.016913319238900635},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.04861111111111111},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.016826923076923076},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.010218978102189781},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":2.543977055449331},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":2.0396270396270397},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.018867924528301886},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.02564102564102564},"08_prompts/08_01_collection_of_prompts.html":{"ref":"08_prompts/08_01_collection_of_prompts.html","tf":0.1111111111111111},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.06060606060606061},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.03162055335968379},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.01282051282051282},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"l":{"docs":{},"l":{"docs":{},"a":{"docs":{},"m":{"docs":{},"a":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.018518518518518517},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.012307692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":2.513138686131387},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.03824091778202677},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.009324009324009324},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.02830188679245283},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}}}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.013888888888888888},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662}}},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993}}}}}}},"l":{"docs":{},"l":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"08_prompts/08_01_collection_of_prompts.html":{"ref":"08_prompts/08_01_collection_of_prompts.html","tf":5.222222222222222}}}}},"a":{"docs":{},"b":{"docs":{},"o":{"docs":{},"r":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}}}}}}}},"m":{"docs":{},"m":{"docs":{},"u":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}},":":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}},"i":{"docs":{},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}}},"c":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.010256410256410256},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}},"o":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.013986013986013986},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}},"s":{"docs":{},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"i":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448}},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.01509433962264151},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.008670520231213872},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.010291595197255575},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.011182108626198083},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":3.335595776772247}},"e":{"docs":{},"r":{"docs":{},"'":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},"’":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}},".":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},",":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}},":":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715}},"s":{"docs":{},".":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},",":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}},"a":{"docs":{},"n":{"docs":{},"i":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}},"i":{"docs":{},"s":{"docs":{},"o":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}},":":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248}}}}}}}},"t":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}},"i":{"docs":{},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}}}}}}},"c":{"docs":{},"t":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}},"e":{"docs":{},"l":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},"n":{"docs":{},"s":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}},"t":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}},"l":{"docs":{},"e":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.011574074074074073},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}}},"e":{"docs":{},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419}}},".":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"x":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.016826923076923076},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.00859106529209622},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0076481835564053535},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}},",":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}},"i":{"docs":{},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}},",":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}}},"r":{"docs":{},"e":{"docs":{},"h":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"s":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}},"o":{"docs":{},"m":{"docs":{},"i":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}},"o":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},".":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}}}},"e":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},"b":{"docs":{},"i":{"docs":{},"n":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}}}}}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}},"n":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}},"l":{"docs":{},"i":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}},",":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}}}}},"u":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}},"p":{"docs":{},"t":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}}}}},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}},"d":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},":":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},":":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.010471204188481676}}}}}}}}}}}}},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705}}}},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"s":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"x":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.022364217252396165}},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159}}},"u":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}},".":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738}}}}}},"r":{"docs":{},"i":{"docs":{},"b":{"docs":{},"u":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.034013605442176874},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041}},"e":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},"!":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"o":{"docs":{},"r":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"o":{"docs":{},"l":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.011320754716981131},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}},"i":{"docs":{},"n":{"docs":{},"u":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},"e":{"docs":{},":":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}}},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}},"e":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}}}},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.008456659619450317}}}},"n":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":3.336410256410256}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}}}}},"f":{"docs":{},"i":{"docs":{},"g":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},"u":{"docs":{},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":2.0393700787401574},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.01437699680511182},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},",":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}},"e":{"docs":{},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}},"d":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}},"n":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}},"c":{"docs":{},"l":{"docs":{},"u":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},":":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}}},"e":{"docs":{},"p":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}},"u":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{"09_advanced_usage/09_01_advanced_model_tuning.html":{"ref":"09_advanced_usage/09_01_advanced_model_tuning.html","tf":0.1111111111111111},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}}}},"b":{"docs":{},"b":{"docs":{},"l":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"i":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"n":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},":":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},"l":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}},"e":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.028169014084507043},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}},"s":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},")":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}}},",":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},"p":{"docs":{},"u":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}},"s":{"docs":{},"a":{"docs":{},"i":{"docs":{},"r":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362}}}}}}},"s":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"l":{"docs":{},"y":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}},"s":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.013986013986013986}}}}},"u":{"docs":{},"n":{"docs":{},"t":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},"e":{"docs":{},"r":{"docs":{},"p":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}}}}}}}},"p":{"docs":{},"i":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}},"o":{"docs":{},"l":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},"e":{"docs":{},"r":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},":":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}}},"a":{"docs":{},"p":{"docs":{},"a":{"docs":{},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.01090909090909091},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}}}}}}}},"l":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"c":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},",":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732}}}}},".":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"i":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}}}}},"n":{"docs":{},"'":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}},"d":{"docs":{},"i":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}},"’":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}},"s":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.010570824524312896},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"s":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},",":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}},"l":{"docs":{},"c":{"docs":{},"u":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}}}}}},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{},":":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}}}}}}},"r":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}},"f":{"docs":{},"u":{"docs":{},"l":{"docs":{},"l":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}},"l":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"t":{"docs":{},"e":{"docs":{},"g":{"docs":{},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"i":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"e":{"docs":{},"s":{"docs":{},":":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}}}}},"r":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}},"u":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"u":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}},"s":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"h":{"docs":{},"a":{"docs":{},"n":{"docs":{},"g":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448}},"e":{"docs":{},"d":{"docs":{},".":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}},"s":{"docs":{},".":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"r":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}},"n":{"docs":{},"e":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}}},"t":{"docs":{},"g":{"docs":{},"p":{"docs":{},"t":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}},"b":{"docs":{},"o":{"docs":{},"t":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"i":{"docs":{},"n":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}},"l":{"docs":{},"l":{"docs":{},"e":{"docs":{},"n":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"e":{"docs":{},"s":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}}}}},"r":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}},"i":{"docs":{},"c":{"docs":{},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}}}}}}}}}}},"p":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.011627906976744186}},"_":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"=":{"docs":{},"\"":{"docs":{},"$":{"1":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"docs":{}}},"$":{"docs":{},"(":{"docs":{},"c":{"docs":{},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"_":{"docs":{},"c":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"_":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}}}}}}}},"e":{"docs":{},"c":{"docs":{},"h":{"docs":{},"o":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}}}}}},"t":{"docs":{},"o":{"docs":{},"_":{"docs":{},"t":{"docs":{},"i":{"docs":{},"t":{"docs":{},"l":{"docs":{},"e":{"docs":{},"_":{"docs":{},"c":{"docs":{},"a":{"docs":{},"s":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}}}}}}}}}}}}}}}},"o":{"docs":{},"i":{"docs":{},"c":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}},"e":{"docs":{},".":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}},"o":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.009615384615384616},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256}},"e":{"docs":{},".":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}}}}}},"e":{"docs":{},"c":{"docs":{},"k":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}},"a":{"docs":{},"t":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}},"i":{"docs":{},"p":{"docs":{},",":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}}}},"m":{"docs":{},"o":{"docs":{},"d":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":3.3396758280479206},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.013888888888888888}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}},"v":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}}}},"u":{"docs":{},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{},".":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}}}},"c":{"docs":{},"i":{"docs":{},"a":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553}},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}},"a":{"docs":{},"s":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"e":{"docs":{},"s":{"docs":{},":":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"a":{"docs":{},"l":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}}}},"o":{"docs":{},"s":{"docs":{},"s":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662}}}}}},"l":{"2":{"2":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{},"o":{"docs":{},"u":{"docs":{},"d":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"s":{"docs":{},"e":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"r":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"l":{"docs":{},"y":{"docs":{},".":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}},"c":{"docs":{},"k":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"n":{"docs":{},"e":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041}}}}},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.008456659619450317}},"_":{"docs":{},"c":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"_":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},"e":{"docs":{},"(":{"docs":{},")":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}}}}}},"e":{"docs":{},"r":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},"r":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}},"i":{"docs":{},"c":{"docs":{},"k":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}}},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},":":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"f":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}},"i":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"c":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},",":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}}}}}}}}}}}}}},"u":{"docs":{},"s":{"docs":{},"t":{"docs":{},"o":{"docs":{},"m":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":3.338568935427574},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}}}}},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"a":{"docs":{},":":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}},"e":{"docs":{},"a":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{},"c":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"r":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}},"y":{"docs":{},"c":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}},"i":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"r":{"docs":{},"c":{"docs":{},"u":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}}}}}},"p":{"docs":{},"u":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":2.5296296296296297},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.01366742596810934},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.022364217252396165},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.013015184381778741},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.015706806282722512},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.013574660633484163},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.011594202898550725},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.015105740181268883},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"/":{"docs":{},"g":{"docs":{},"p":{"docs":{},"u":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},")":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}},"p":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"d":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},"e":{"docs":{},"e":{"docs":{},"p":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"e":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}},"c":{"docs":{},"r":{"docs":{},"i":{"docs":{},"p":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629}}}},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705}}}}}}}},"b":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"p":{"docs":{},"i":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}},"k":{"docs":{},"t":{"docs":{},"o":{"docs":{},"p":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}}},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00583941605839416},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.017208413001912046},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},",":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},".":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}},"e":{"docs":{},"r":{"docs":{},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.007692307692307693}}}}}},"c":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.017391304347826087},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.014084507042253521},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.01812688821752266}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}}}}}}}},"v":{"docs":{"./":{"ref":"./","tf":0.02040816326530612}},"e":{"docs":{},"l":{"docs":{},"o":{"docs":{},"p":{"docs":{"./":{"ref":"./","tf":0.02040816326530612},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.02536997885835095},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.018648018648018648},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.015384615384615385},"09_advanced_usage/09_02_integrating_llms_into_workflows.html":{"ref":"09_advanced_usage/09_02_integrating_llms_into_workflows.html","tf":0.1111111111111111},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},".":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}},"’":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},".":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}}}}}}}}}}},"i":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},",":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}}},".":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"b":{"docs":{},"u":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}},"m":{"docs":{},"y":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755}}}}}}}},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.0072992700729927005},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}}},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}}}},"p":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"l":{"docs":{},"o":{"docs":{},"y":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}}}}}},"t":{"docs":{},"h":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}}}},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"a":{"docs":{},"d":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}}}}}},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}}},"d":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}},"o":{"docs":{},"d":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273}}}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}},"f":{"docs":{},"i":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}},"a":{"docs":{},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}},"l":{"docs":{},"v":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},"i":{"docs":{},"v":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}},"e":{"docs":{},"t":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}}}}},"a":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488}}},"n":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"n":{"docs":{},"o":{"docs":{},"t":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}},"i":{"docs":{},"s":{"docs":{},"c":{"docs":{},"u":{"docs":{},"s":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"09_advanced_usage/09_02_integrating_llms_into_workflows.html":{"ref":"09_advanced_usage/09_02_integrating_llms_into_workflows.html","tf":0.1111111111111111},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},":":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}}}}}},"o":{"docs":{},"v":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},"r":{"docs":{},"d":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"c":{"docs":{},"t":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}},"g":{"docs":{},"u":{"docs":{},"i":{"docs":{},"s":{"docs":{},"h":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"b":{"docs":{},"u":{"docs":{},"t":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"k":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.014778325123152709},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},"p":{"docs":{},"o":{"docs":{},"s":{"docs":{},"a":{"docs":{},"l":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}},"l":{"docs":{},"a":{"docs":{},"y":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}},":":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}}}}}},"r":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159}},"l":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"y":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.009513742071881607}},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"y":{"docs":{},".":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},"m":{"docs":{},"l":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},")":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"_":{"docs":{},"p":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},"=":{"docs":{},"\"":{"docs":{},"$":{"1":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"docs":{}}}}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662}}}}},"i":{"docs":{},"s":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}},"f":{"docs":{},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.009259259259259259},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.014778325123152709},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.009324009324009324},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"08_prompts/08_01_collection_of_prompts.html":{"ref":"08_prompts/08_01_collection_of_prompts.html","tf":0.1111111111111111},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}}}},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"s":{"docs":{},"h":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}}},"g":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},"c":{"docs":{},"k":{"docs":{},"s":{"docs":{},"o":{"docs":{},"n":{"docs":{},":":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}}},"o":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"w":{"docs":{},"n":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},"s":{"docs":{},"i":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419}},":":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}}}}},"c":{"docs":{},"u":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}}}}}}}},"n":{"docs":{},"’":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"e":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"e":{"docs":{},"s":{"docs":{},"n":{"docs":{},"’":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}},"'":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}}},"z":{"docs":{},"e":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}},"m":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}},"u":{"docs":{},"b":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"l":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"a":{"docs":{},"t":{"docs":{},"a":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.013745704467353952},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.010471204188481676},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.006872852233676976},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}},"s":{"docs":{},"e":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256}}}}}}},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},")":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}},"e":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"y":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}},"u":{"docs":{},"m":{"docs":{},"m":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}},"r":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}},"e":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},"y":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"r":{"docs":{},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}},"d":{"docs":{},"r":{"4":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553}}},"docs":{}}}},"e":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"n":{"docs":{},"h":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}},"j":{"docs":{},"o":{"docs":{},"y":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}}},"s":{"docs":{},"u":{"docs":{},"r":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.011320754716981131},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.009259259259259259},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.01090909090909091},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.031496062992125984},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.009584664536741214},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.010256410256410256},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041}}}}},"v":{"docs":{},"i":{"docs":{},"r":{"docs":{},"o":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.009324009324009324}},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},".":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},",":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}},",":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}}}}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}},"c":{"docs":{},"o":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"u":{"docs":{},"n":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}},"r":{"docs":{},"a":{"docs":{},"g":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}},"d":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.020766773162939296},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},"p":{"docs":{},"o":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},".":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}}}},",":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738}}}},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.011267605633802818}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}}},"o":{"docs":{},"u":{"docs":{},"g":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}}}}}},"e":{"docs":{},"r":{"docs":{},"g":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}},"i":{"docs":{},"r":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"e":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}},"r":{"docs":{},"i":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"v":{"docs":{},"o":{"docs":{},"l":{"docs":{},"v":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}},"e":{"docs":{},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}},"s":{"docs":{},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},",":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}},"e":{"docs":{},"r":{"docs":{},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.018867924528301886},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":2.5153846153846153},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"o":{"docs":{},"r":{"docs":{},",":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}}}}},"x":{"docs":{},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},"i":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}}}},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}},"c":{"docs":{},"t":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}}},"e":{"docs":{},"d":{"docs":{},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}},"n":{"docs":{},"s":{"docs":{},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},".":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}}},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.013605442176870748},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}}}},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"n":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"a":{"docs":{},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},":":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}}}}},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},"s":{"docs":{},".":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}}}}},"c":{"docs":{},"i":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}},"e":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"e":{"docs":{},"d":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}}}},"l":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409}}},"s":{"docs":{},"s":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738}}}},"p":{"docs":{},"t":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}}},"l":{"docs":{},"u":{"docs":{},"s":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}},"s":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.020202020202020204},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},":":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}}}},"r":{"docs":{},"n":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}},"r":{"docs":{},"a":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448}},"_":{"docs":{},"h":{"1":{"docs":{},"_":{"docs":{},"t":{"docs":{},"i":{"docs":{},"t":{"docs":{},"l":{"docs":{},"e":{"docs":{},"(":{"docs":{},")":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}},"docs":{}}}}}},"e":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}},"a":{"docs":{},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"e":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}},".":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}},",":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}},"e":{"docs":{},"c":{"docs":{},"u":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}}}}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}}},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{},"o":{"docs":{},"m":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}},"s":{"docs":{},"y":{"docs":{},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"m":{"docs":{},".":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}}}}}}}},"h":{"docs":{},"o":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.014799154334038054}}}}},"f":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.008576329331046312},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00583941605839416},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.007987220447284345},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"y":{"docs":{},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}},"t":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},"l":{"docs":{},"y":{"docs":{},".":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}},"l":{"docs":{},"y":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},",":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"?":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"n":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},".":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}}}}}}}}}},"s":{"docs":{},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"a":{"docs":{},"l":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}}}}},"t":{"docs":{},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{},"i":{"docs":{},"s":{"docs":{},"h":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}},"i":{"docs":{},"m":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.020289855072463767},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.01971830985915493},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.027190332326283987}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},",":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}}}}}}}},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.010256410256410256}},"s":{"docs":{},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}},".":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":3.3616352201257858},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.015384615384615385},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}},"s":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"i":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}},"l":{"docs":{},"i":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}},"d":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}}},"i":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"o":{"docs":{},"r":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}}}},"l":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}},"i":{"docs":{},"f":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}},"m":{"docs":{},"b":{"docs":{},"e":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.006872852233676976}},"d":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"s":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}}}},"p":{"docs":{},"t":{"docs":{},"i":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"q":{"docs":{},"u":{"docs":{},"i":{"docs":{},"p":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}},"a":{"docs":{},"l":{"docs":{},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}},"y":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},"t":{"docs":{},"h":{"docs":{},"i":{"docs":{},"c":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}},"f":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},"i":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.006342494714587738}},"g":{"docs":{},"u":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}}},"n":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}},"e":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.006872852233676976},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}}},"a":{"docs":{},"l":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}}},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}},",":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"l":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.014799154334038054},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"'":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"?":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"_":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"=":{"docs":{},"$":{"docs":{},"(":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}}}}},"p":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},"=":{"docs":{},"\"":{"docs":{},"$":{"1":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"docs":{}}}}}}}}},"s":{"docs":{},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}},"l":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"x":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}},"e":{"docs":{},"l":{"docs":{},"d":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"o":{"docs":{},"l":{"docs":{},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"./":{"ref":"./","tf":0.013605442176870748},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.007692307692307693},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.005656108597285068}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}},"s":{"docs":{},":":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}},".":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}},"'":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},"c":{"docs":{},"u":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}},"s":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}}},"r":{"docs":{},"m":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"a":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.006787330316742082}},".":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}},"u":{"docs":{},"l":{"docs":{},"a":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"k":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055}}}},"o":{"docs":{},"t":{"docs":{},"p":{"docs":{},"r":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}}}}}}},"u":{"docs":{},"r":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},"n":{"docs":{},"d":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}}},"e":{"docs":{},"r":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}}}}}},"e":{"docs":{},"e":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}},"l":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}},"d":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}},"s":{"docs":{},":":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"w":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},"a":{"docs":{},"s":{"docs":{},"i":{"docs":{},"b":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"e":{"docs":{},"s":{"docs":{},":":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},".":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}},"q":{"docs":{},"u":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"w":{"docs":{},"o":{"docs":{},"r":{"docs":{},"k":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.006787330316742082},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},")":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055}},"*":{"docs":{},"*":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},",":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},".":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}}}}}}},"s":{"docs":{},"e":{"docs":{},"r":{"docs":{},":":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}},"n":{"docs":{},"z":{"docs":{},"e":{"docs":{},"n":{"docs":{},":":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}},"i":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"l":{"docs":{},"i":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}},"o":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},":":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}},"u":{"docs":{},"l":{"docs":{},"l":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}},"i":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}},"n":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.007692307692307693},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.010570824524312896}},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},":":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}},"d":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}}},"r":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"m":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}},",":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}}}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.010471204188481676}}}}}},"l":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}},"u":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}},"e":{"docs":{},"x":{"docs":{},"i":{"docs":{},"b":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"s":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}},"o":{"docs":{},"a":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}}},"a":{"docs":{},"s":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}}},"m":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}},"a":{"docs":{},"r":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}},"y":{"docs":{},",":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}}},"c":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}},"a":{"docs":{},"l":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"e":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.012084592145015106},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"'":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}},"’":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}},"i":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}},"r":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"r":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}},"l":{"docs":{},"l":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}},"g":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"s":{"docs":{},",":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}},"u":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},"p":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},"m":{"docs":{},"e":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.018518518518518517}}}}},"i":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.007399577167019027},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{"./":{"ref":"./","tf":0.013605442176870748},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041}},".":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}},"_":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}},"f":{"docs":{},"i":{"docs":{},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"v":{"docs":{},"e":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}}},"r":{"docs":{},"o":{"docs":{},"w":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446}},"t":{"docs":{},"h":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}},".":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"u":{"docs":{},"p":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}},"e":{"docs":{},"a":{"docs":{},"t":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732}}},"s":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}}}}},"l":{"docs":{},"i":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}},"p":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"a":{"docs":{},"d":{"docs":{},"e":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"u":{"docs":{},"a":{"docs":{},"l":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}},"n":{"docs":{},"t":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}},"u":{"docs":{},"i":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.02040816326530612},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":5.028368794326241},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.023715415019762844},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"e":{"docs":{},"!":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}},"'":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},".":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}}},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"r":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.012684989429175475},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.018518518518518517},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.010309278350515464},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.011472275334608031},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.016317016317016316},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.011182108626198083},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.023076923076923078},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.013888888888888888}},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"_":{"docs":{},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"y":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}}}}}}}}}}}}}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}},"a":{"docs":{},"i":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}},"t":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},"e":{"docs":{},"k":{"docs":{},"b":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"h":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.020202020202020204}}}}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"c":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}}}}}},"o":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}},"o":{"docs":{},"g":{"docs":{},"l":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"a":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"p":{"docs":{},"t":{"4":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},":":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}}},"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}},"u":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":2.534567901234568},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":5.050113895216401},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.008759124087591242},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.012779552715654952},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0368763557483731},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.020942408376963352},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.011312217194570135},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.011267605633802818},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},"s":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},",":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"—":{"docs":{},"i":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}},"’":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}}},"'":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},":":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}},"g":{"docs":{},"u":{"docs":{},"f":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"b":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.046153846153846156},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.029197080291970802}},")":{"docs":{},":":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696}}}},"+":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}}},".":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}}}},"l":{"docs":{},"o":{"docs":{},"b":{"docs":{},"a":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}},"d":{"docs":{},"d":{"docs":{},"r":{"6":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},"x":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}},"docs":{}}}}},"h":{"1":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448}},"_":{"docs":{},"t":{"docs":{},"i":{"docs":{},"t":{"docs":{},"l":{"docs":{},"e":{"docs":{},"=":{"docs":{},"$":{"docs":{},"(":{"docs":{},"e":{"docs":{},"x":{"docs":{},"t":{"docs":{},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"_":{"docs":{},"h":{"1":{"docs":{},"_":{"docs":{},"t":{"docs":{},"i":{"docs":{},"t":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}},"docs":{}}}}}}}}}},"g":{"docs":{},"r":{"docs":{},"e":{"docs":{},"p":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}}}}},"docs":{},"e":{"docs":{},"l":{"docs":{},"p":{"docs":{"./":{"ref":"./","tf":0.04081632653061224},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.02127659574468085},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.012684989429175475},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.01090909090909091},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.006389776357827476},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.007692307692307693},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041}}}},"r":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"'":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154}}},"’":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"a":{"docs":{},"v":{"docs":{},"i":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"l":{"docs":{},"i":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}},"d":{"docs":{},"r":{"docs":{},"o":{"docs":{},"o":{"docs":{},"m":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}}},"t":{"docs":{},"t":{"docs":{},"p":{"docs":{},"s":{"docs":{},":":{"docs":{},"/":{"docs":{},"/":{"docs":{},"j":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"e":{"docs":{},"f":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"m":{"docs":{},"a":{"docs":{},"n":{"docs":{},".":{"docs":{},"s":{"docs":{},"u":{"docs":{},"b":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"c":{"docs":{},"k":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{},"/":{"docs":{},"j":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"e":{"docs":{},"f":{"docs":{},"r":{"docs":{},"e":{"docs":{},"e":{"docs":{},"m":{"docs":{},"a":{"docs":{},"n":{"docs":{},"/":{"docs":{},"a":{"docs":{},"i":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}}}}}}}}}}}}}}}}}}},"h":{"docs":{},"u":{"docs":{},"g":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"f":{"docs":{},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{},".":{"docs":{},"c":{"docs":{},"o":{"docs":{},"/":{"docs":{},":":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}}}}}}}}}}}}}}}}}}},"a":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}},"d":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.01090909090909091},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"e":{"docs":{},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"’":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}},"'":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},"—":{"docs":{},"s":{"docs":{},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"i":{"docs":{},"f":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}}}}}}},"e":{"docs":{},"r":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}}}},"n":{"docs":{},"d":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}},"l":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.012345679012345678},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.009111617312072893},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.015184381778741865},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.01282051282051282},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}},"e":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}},",":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621}}}}},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}},"v":{"docs":{},"e":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}},"t":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}},"p":{"docs":{},"p":{"docs":{},"i":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"l":{"docs":{},"f":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.014492753623188406},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.030985915492957747},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.01812688821752266}}}}},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}},"w":{"docs":{},"e":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371}}}}}}}},"m":{"docs":{},"e":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}},"p":{"docs":{},"a":{"docs":{},"g":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}},"o":{"docs":{},"k":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"s":{"docs":{},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}},"d":{"docs":{},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"u":{"docs":{},"r":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"t":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.008670520231213872},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.015384615384615385},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.020437956204379562},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0248565965583174},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.009584664536741214},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.008670520231213872},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.008576329331046312},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},"s":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}},"l":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.011857707509881422}}}}}}}}},"t":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}},"u":{"docs":{},"m":{"docs":{},"a":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}},"n":{"docs":{},"d":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}},"g":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.020202020202020204},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}},"i":{"7":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}},"docs":{},"m":{"docs":{},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"v":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}}}}}},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}}},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"c":{"docs":{},"e":{"docs":{},",":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}}}},"s":{"docs":{},"s":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.006389776357827476},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}}}}},"l":{"docs":{},"i":{"docs":{},"c":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}}}}}},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}}}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}}}}}}}}},"a":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0223463687150838},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.01809954751131222},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.011594202898550725},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.014084507042253521},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.012084592145015106},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.013888888888888888}},"i":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}},"n":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}}},"m":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}},"n":{"docs":{},"s":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}}}},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.018518518518518517},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.009230769230769232},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":2.5058394160583943},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.021032504780114723},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.007692307692307693},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}},"q":{"2":{"docs":{},"_":{"docs":{},"k":{"docs":{},".":{"docs":{},"g":{"docs":{},"g":{"docs":{},"u":{"docs":{},"f":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}}}}},"3":{"docs":{},"_":{"docs":{},"k":{"docs":{},"_":{"docs":{},"m":{"docs":{},".":{"docs":{},"g":{"docs":{},"g":{"docs":{},"u":{"docs":{},"f":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}},",":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}}}}},"s":{"docs":{},".":{"docs":{},"g":{"docs":{},"g":{"docs":{},"u":{"docs":{},"f":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}}}}}}}}},"4":{"docs":{},"_":{"docs":{},"k":{"docs":{},"_":{"docs":{},"m":{"docs":{},".":{"docs":{},"g":{"docs":{},"g":{"docs":{},"u":{"docs":{},"f":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}}}}}}}}},"5":{"docs":{},"_":{"docs":{},"k":{"docs":{},"_":{"docs":{},"m":{"docs":{},".":{"docs":{},"g":{"docs":{},"g":{"docs":{},"u":{"docs":{},"f":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}}}}}}}}},"docs":{}}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},"’":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},"—":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}}}}}}},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}},"t":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}}}},"l":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":3.344506517690875},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"l":{"docs":{},"e":{"docs":{},"r":{"docs":{},".":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}}}},"u":{"docs":{},"f":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{},"i":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}}}},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}},"n":{"docs":{},"e":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}}}}},"p":{"docs":{},"r":{"docs":{},"e":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767}}}}}},"m":{"docs":{},"e":{"docs":{},"d":{"docs":{},"i":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}},"f":{"docs":{},"a":{"docs":{},"c":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"e":{"docs":{},".":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}},",":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}},"g":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"09_advanced_usage/09_02_integrating_llms_into_workflows.html":{"ref":"09_advanced_usage/09_02_integrating_llms_into_workflows.html","tf":3.5555555555555554},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.020202020202020204},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}},"e":{"docs":{},"r":{"docs":{},")":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}},"l":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":2.515105740181269}},"l":{"docs":{},"i":{"docs":{},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}}}}}},"n":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}},"t":{"docs":{},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767}}}}}},"r":{"docs":{},"o":{"docs":{},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}},"t":{"docs":{"./":{"ref":"./","tf":10},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}},"i":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767}}}}},"u":{"docs":{},"i":{"docs":{},"t":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}},"v":{"docs":{},"i":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}}},"e":{"docs":{},"d":{"docs":{},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"o":{"docs":{},"l":{"docs":{},"v":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}}}}},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}}}}}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.01201923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.008576329331046312},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":2.5271604938271603},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.009111617312072893},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.013015184381778741}},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},")":{"docs":{},",":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}},",":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},":":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}}},"?":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}}}}},"l":{"docs":{},"u":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}},"n":{"docs":{},"e":{"docs":{},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}},"c":{"docs":{},"l":{"docs":{},"u":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},"s":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"a":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.018900343642611683},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"l":{"docs":{},"i":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}}}}},"o":{"docs":{},"r":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}},"n":{"docs":{},"s":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{},"e":{"docs":{},"t":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}}},"h":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},"=":{"docs":{},"\"":{"docs":{},"$":{"1":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"docs":{}}}}}},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}}}}},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.012307692307692308},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},"u":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"i":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},"y":{"docs":{},".":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"=":{"docs":{},"\"":{"docs":{},"$":{"2":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"docs":{}}}}}}}},"o":{"docs":{},"r":{"docs":{},"d":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},",":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}}}}}},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}},"e":{"docs":{},"a":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}},"l":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.0072992700729927005},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041}},"c":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}}}}}}},"q":{"1":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},"2":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},"_":{"docs":{},"m":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}},"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.008576329331046312}}},"t":{"docs":{},"’":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.009259259259259259},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},"'":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00583941605839416},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}}}},".":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{},"_":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}}}}}},"’":{"docs":{},"d":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}},"l":{"docs":{},"l":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}},"m":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"v":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155}}}},"s":{"docs":{},"n":{"docs":{},"’":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}},")":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},"s":{"docs":{},"u":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}},"'":{"docs":{},"v":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"l":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"g":{"docs":{},"n":{"docs":{},"o":{"docs":{},"r":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"e":{"docs":{},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}},"p":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.014705882352941176},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.0463768115942029},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.04788732394366197},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0513595166163142}}}},"j":{"docs":{},"o":{"docs":{},"i":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.02040816326530612},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{},"e":{"docs":{},"y":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}}}},"k":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}},"b":{"docs":{},"s":{"docs":{},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"a":{"docs":{},"v":{"docs":{},"a":{"docs":{},"s":{"docs":{},"c":{"docs":{},"r":{"docs":{},"i":{"docs":{},"p":{"docs":{},"t":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}}}}}},"l":{"docs":{},"a":{"docs":{},"n":{"docs":{},"g":{"docs":{},"u":{"docs":{},"a":{"docs":{},"g":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":2.0075471698113208},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":2.016913319238901},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.015296367112810707},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.023310023310023312},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.02766798418972332}},"e":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159}}},"s":{"docs":{},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},",":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}},"e":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"r":{"docs":{},"g":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":2.0075471698113208},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":2.0063424947145876},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.019753086419753086},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.02050113895216401},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.01735357917570499},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.010471204188481676},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.023715415019762844}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.012307692307692308},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.016826923076923076},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.015463917525773196},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488}}}},",":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}},"s":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248}}}}}}},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}},"s":{"docs":{},"t":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"b":{"docs":{},"e":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}},"o":{"docs":{},"r":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}},"p":{"docs":{},"t":{"docs":{},"o":{"docs":{},"p":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}}}}}},"y":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.010309278350515464},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738}},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488}}},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488}}},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488}}}}}}},"c":{"docs":{},"k":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}},"u":{"docs":{},"n":{"docs":{},"c":{"docs":{},"h":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"m":{"docs":{},":":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.009230769230769232},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.00859106529209622},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}},"d":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}}},"v":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"a":{"docs":{},"g":{"docs":{"./":{"ref":"./","tf":0.013605442176870748},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.011320754716981131},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.008456659619450317},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"l":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.012307692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.028901734104046242},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155}},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},":":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}},"s":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"n":{"docs":{},"g":{"docs":{},"t":{"docs":{},"h":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.01757188498402556}},",":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}}}}}}},"t":{"docs":{},"'":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"r":{"docs":{},"o":{"docs":{},"b":{"docs":{},"o":{"docs":{},"t":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}},"i":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662}},"s":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},":":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}},"_":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.008456659619450317}},"e":{"docs":{},"(":{"docs":{},")":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}}}}},"m":{"docs":{},"i":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.009584664536741214},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}}}}}},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464}}},".":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},"s":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},",":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},":":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}},"f":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"w":{"docs":{},"e":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464}}}}}}}}}}},"b":{"docs":{},"r":{"docs":{},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}}}},"n":{"docs":{},"u":{"docs":{},"x":{"docs":{},".":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}},")":{"docs":{},".":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}},"e":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"a":{"docs":{},"r":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}},"k":{"docs":{},"s":{"docs":{},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}},"q":{"docs":{},"u":{"docs":{},"i":{"docs":{},"d":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}}}}},"l":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.022641509433962263},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.03805496828752643},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.015945330296127564},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.01675977653631285},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"07_best_practices/07_01_best_practices_for_running_local_llms.html":{"ref":"07_best_practices/07_01_best_practices_for_running_local_llms.html","tf":2.1666666666666665},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.013742071881606765},"09_advanced_usage/09_02_integrating_llms_into_workflows.html":{"ref":"09_advanced_usage/09_02_integrating_llms_into_workflows.html","tf":3.5555555555555554},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.03162055335968379},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}},"s":{"docs":{},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}}},":":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"?":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"'":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},".":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}},"’":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}},"a":{"docs":{},"m":{"docs":{},"a":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":2.505736137667304},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.011857707509881422}},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},":":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}},"o":{"docs":{},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.013605442176870748},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.026415094339622643},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.01090909090909091},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"07_best_practices/07_01_best_practices_for_running_local_llms.html":{"ref":"07_best_practices/07_01_best_practices_for_running_local_llms.html","tf":2.0833333333333335},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.019027484143763214},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.03557312252964427}},"l":{"docs":{},"y":{"docs":{},"?":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":2.0037735849056606}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"07_best_practices/07_01_best_practices_for_running_local_llms.html":{"ref":"07_best_practices/07_01_best_practices_for_running_local_llms.html","tf":0.08333333333333333},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}},",":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}},"o":{"docs":{},"k":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"s":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"s":{"docs":{},"e":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662}}}},"t":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},"v":{"docs":{},"e":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}},"n":{"docs":{},"g":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.011320754716981131},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}},"a":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256}},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}},".":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}},",":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},":":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}},"w":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.006389776357827476}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.011560693641618497},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"c":{"docs":{},"a":{"docs":{},"s":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}},"g":{"docs":{},"i":{"docs":{},"c":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}},"m":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":3.3780260707635006},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":2.0708661417322833},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":2.009584664536741},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":2.513015184381779},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.020202020202020204}}},"u":{"docs":{},"c":{"docs":{},"k":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}}}}},"p":{"docs":{},"x":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362}}}}},"m":{"1":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.009584664536741214},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}}},"3":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.010256410256410256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":1.6863849765258214}}},"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.013605442176870748}}},"n":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}}},"k":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.014450867052023121},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.00859106529209622},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.01090909090909091},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.009111617312072893},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.01338432122370937},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.016317016317016316},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"r":{"docs":{},"k":{"docs":{},"d":{"docs":{},"o":{"docs":{},"w":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.011627906976744186}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}},"c":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.011857707509881422}},"h":{"docs":{},"i":{"docs":{},"n":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},"e":{"docs":{},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}},"\"":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}},":":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"s":{"docs":{},".":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}}},"o":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"\"":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}},".":{"docs":{},"\"":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}}}}},",":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},"b":{"docs":{},"o":{"docs":{},"o":{"docs":{},"k":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.015748031496062992},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.009584664536741214},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.010256410256410256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":1.6779342723004693}}}}}},"’":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}},"s":{"docs":{},".":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},":":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},",":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"n":{"docs":{},"a":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.014778325123152709},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.01675977653631285},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}}}},"i":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}},"p":{"docs":{},"u":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},",":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}}}}}},"u":{"docs":{},"a":{"docs":{},"l":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"l":{"docs":{},"y":{"docs":{},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}},"h":{"docs":{},"e":{"docs":{},"m":{"docs":{},"a":{"docs":{},"t":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715}}}}}}},"r":{"docs":{},"i":{"docs":{},"x":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}},"x":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":1.68075117370892}},"i":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"u":{"docs":{},"m":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.0072992700729927005},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}},".":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}}},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"'":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}},"d":{"docs":{},"e":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"y":{"docs":{},"b":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}}}},"j":{"docs":{},"o":{"docs":{},"r":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}},"e":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"a":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.010309278350515464},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"t":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}},"s":{"docs":{},"u":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}},"t":{"docs":{},"a":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"’":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},"'":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},"h":{"docs":{},"o":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"r":{"docs":{},"i":{"docs":{},"c":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}},"s":{"docs":{},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}},"e":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}},"d":{"docs":{},"i":{"docs":{},"u":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.006389776357827476},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},",":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}}}},"c":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}}},"m":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.009230769230769232},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.019230769230769232},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":2.519230769230769},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":3.392446633825944},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.030656934306569343},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.022364217252396165},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.01735357917570499},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.010471204188481676},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"y":{"docs":{},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.0072992700729927005},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},".":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},"\"":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}},")":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621}}},"d":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}},"s":{"docs":{},"s":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}},"a":{"docs":{},"g":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"e":{"docs":{},",":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}}},"n":{"docs":{},"u":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}},"l":{"docs":{"./":{"ref":"./","tf":0.02040816326530612},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.02127659574468085},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":2.018867924528302},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":2.0211416490486256},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":5.034722222222222},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":3.37025641025641},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.040865384615384616},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.03757225433526012},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.04288164665523156},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":5.058419243986254},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":3.395151515151515},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":2.5596153846153844},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":3.3825944170771756},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0345679012345679},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.02733485193621868},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.040875912408759124},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.017208413001912046},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":2.027972027972028},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.027932960893854747},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.015748031496062992},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.011182108626198083},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":2.552060737527115},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":3.389937106918239},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":2.528205128205128},"09_advanced_usage/09_01_advanced_model_tuning.html":{"ref":"09_advanced_usage/09_01_advanced_model_tuning.html","tf":3.444444444444444},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.015706806282722512},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.030303030303030304},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.03557312252964427},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00583941605839416},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}},"s":{"docs":{},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.010845986984815618},"09_advanced_usage/09_01_advanced_model_tuning.html":{"ref":"09_advanced_usage/09_01_advanced_model_tuning.html","tf":0.1111111111111111},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.011389521640091117},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}},"'":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"'":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.010256410256410256}}},"’":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.006872852233676976},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.014778325123152709},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}}},":":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}},"?":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}},"r":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.011560693641618497},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.025},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.0072992700729927005},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.028680688336520075},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.027972027972027972}},"n":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}}}},"s":{"docs":{},"t":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"r":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.012684989429175475},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.021538461538461538},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.016826923076923076},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.014450867052023121},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.010291595197255575},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.041237113402061855},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.01090909090909091},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.023076923076923078},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.009111617312072893},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.019120458891013385},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.009584664536741214},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.013089005235602094},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},"o":{"docs":{},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},")":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}},"v":{"docs":{},"e":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{},"b":{"docs":{},"o":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}}}}}}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.013986013986013986},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}},"p":{"docs":{},"l":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"i":{"docs":{},"c":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}}}}},"a":{"docs":{},"y":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"t":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"u":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}},"c":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"y":{"docs":{},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"u":{"docs":{},"m":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}},"d":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}},",":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},":":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"u":{"docs":{},"t":{"docs":{},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}},"x":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}},"d":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256}},"j":{"docs":{},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{},"e":{"docs":{},"y":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}}}}}}},"c":{"docs":{},"h":{"docs":{},"a":{"docs":{},"e":{"docs":{},"l":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}}}},"r":{"docs":{},"r":{"docs":{},"o":{"docs":{},"r":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}},"l":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.005656108597285068},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.016901408450704224},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},",":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}},"s":{"docs":{},"i":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553}}}}},"n":{"docs":{},"e":{"docs":{},"w":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":10.01388888888889}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"e":{"docs":{},"d":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.02127659574468085},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.011574074074074073},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.012307692307692308},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.014423076923076924},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.014778325123152709},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.011472275334608031},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.011655011655011656},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.013015184381778741},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}},"—":{"docs":{},"w":{"docs":{},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}}}}}}},"t":{"docs":{},"w":{"docs":{},"o":{"docs":{},"r":{"docs":{},"k":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},"s":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488}}}},"—":{"docs":{},"t":{"docs":{},"h":{"docs":{},"a":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}},"u":{"docs":{},"r":{"docs":{},"a":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.011267605633802818}}}},"o":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732}}}}}},"g":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525}}},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}},"i":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},"l":{"docs":{},"i":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}},"y":{"docs":{},".":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}}}}},"x":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273}},",":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}},"a":{"docs":{},"r":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696}}}}},"o":{"docs":{},"t":{"docs":{},"e":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.015384615384615385}},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},"s":{"docs":{},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}}},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}}}}},"i":{"docs":{},"c":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":2.0023310023310024}}}}}},"w":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},".":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"n":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},"r":{"docs":{},"m":{"docs":{},"a":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"m":{"docs":{},"i":{"docs":{},"c":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}},"v":{"docs":{},"e":{"docs":{},"l":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"a":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.011472275334608031},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"m":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":3.339487179487179},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.01818181818181818},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.009513742071881607}},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"s":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"?":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448}}},"]":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.007918552036199095}}}}},"v":{"docs":{},"i":{"docs":{},"g":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"s":{"docs":{},",":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},":":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"i":{"docs":{},"c":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}},"u":{"docs":{},"m":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}},"b":{"docs":{},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.00859106529209622},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.01437699680511182},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}},"c":{"9":{"docs":{},"v":{"7":{"docs":{},"q":{"docs":{},"n":{"docs":{},"x":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":2.503021148036254}},",":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}}},"docs":{}}},"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"ñ":{"docs":{},"e":{"docs":{},"z":{"docs":{},":":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}}}},"v":{"docs":{},"i":{"docs":{},"d":{"docs":{},"i":{"docs":{},"a":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}},"m":{"docs":{},"e":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}},"o":{"docs":{},"b":{"docs":{},"s":{"docs":{},"e":{"docs":{},"r":{"docs":{},"v":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}},"j":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.006787330316742082},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.011594202898550725},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.011267605633802818},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}}}}}},"n":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.009259259259259259},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}},"e":{"docs":{},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}},"y":{"docs":{},",":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}},"c":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"e":{"docs":{},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},".":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}},"e":{"docs":{},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}},"s":{"docs":{},".":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},",":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},"g":{"docs":{},"o":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}},"n":{"docs":{},"x":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.011594202898550725},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}}}},"p":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"i":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.009259259259259259},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.012307692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.02023121387283237},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.012006861063464836},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0076481835564053535},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.031496062992125984},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":2.009584664536741},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.011857707509881422}},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}},".":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}}}},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}},"o":{"docs":{},"n":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},".":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}}},".":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},"—":{"docs":{},"i":{"docs":{},"t":{"docs":{},"’":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}}}},"e":{"docs":{},"n":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":2.006993006993007},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"a":{"docs":{},"i":{"docs":{},"’":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}},"v":{"docs":{},"i":{"docs":{},"n":{"docs":{},"o":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553}}}}}}},"r":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.014778325123152709},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.014814814814814815},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},".":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}}}}},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"u":{"docs":{},"n":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755}},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}}}}}}}}},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}},"'":{"docs":{},"s":{"docs":{},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}}},"s":{"docs":{},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}},"w":{"docs":{},"i":{"docs":{},"s":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}}},"u":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.018867924528301886},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},",":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}}}},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767}}}}}}},"r":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}}},"i":{"docs":{},"g":{"docs":{},"i":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}}}},"g":{"docs":{},"a":{"docs":{},"n":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}},"i":{"docs":{},"z":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"v":{"docs":{},"i":{"docs":{},"e":{"docs":{},"w":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":2.0021141649048624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":2.505736137667304},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":1.114009661835749},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":1.6666666666666665},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":2.503021148036254}},"*":{"docs":{},"*":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409}}}}}}}},"w":{"docs":{},"h":{"docs":{},"e":{"docs":{},"l":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"h":{"docs":{},"e":{"docs":{},"a":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}},"f":{"docs":{},"i":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}}},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.007987220447284345}}}}}}}}},"f":{"docs":{},"f":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}},"e":{"docs":{},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.018867924528301886},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.012307692307692308},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.011560693641618497},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.013461538461538462},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.008759124087591242},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"07_best_practices/07_01_best_practices_for_running_local_llms.html":{"ref":"07_best_practices/07_01_best_practices_for_running_local_llms.html","tf":0.08333333333333333},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}}}},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}},"e":{"docs":{},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":5.031890660592255},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.013138686131386862},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.01437699680511182},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.01735357917570499},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"e":{"docs":{},"d":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},".":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},":":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}},"\"":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},",":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}},"s":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},"i":{"docs":{},"c":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"i":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}},"l":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}},"c":{"docs":{},"c":{"docs":{},"u":{"docs":{},"p":{"docs":{},"i":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}}}}},"p":{"docs":{},"a":{"docs":{},"g":{"docs":{},"e":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"r":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}},"n":{"docs":{},"e":{"docs":{},"r":{"docs":{},"!":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}},"i":{"docs":{},"c":{"docs":{},"u":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}},"l":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.009324009324009324},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553}}}}}}}}},"a":{"docs":{},"l":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.015384615384615385},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":5.0360576923076925},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.006872852233676976},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00583941605839416},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715}}},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},")":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},"—":{"docs":{},"t":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}}}},"l":{"docs":{},"l":{"docs":{},"e":{"docs":{},"l":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.009111617312072893},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}},"i":{"docs":{},"z":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}},"d":{"docs":{},"i":{"docs":{},"g":{"docs":{},"m":{"docs":{},"s":{"docs":{},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}}}},"s":{"docs":{},"t":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}},"s":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}},"t":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488}}}}}},"h":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"y":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}},"i":{"docs":{},"n":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"r":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553}}}}},"l":{"docs":{},"e":{"docs":{},"a":{"docs":{},"s":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041}}}}},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}},"n":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"n":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}},"i":{"docs":{},"n":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}},"t":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}},"s":{"docs":{},".":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}}}}},"o":{"docs":{},"w":{"docs":{},"e":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.008456659619450317},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.010471204188481676},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.013888888888888888}},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}},".":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},":":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}},"i":{"docs":{},"n":{"docs":{},"t":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"s":{"docs":{},".":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}},")":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"b":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"e":{"docs":{},".":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}},"\"":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}},"\"":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}}}},":":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}},"i":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},"e":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.011594202898550725},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.014084507042253521},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.01812688821752266}}}},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}},"l":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}},"a":{"docs":{},"l":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}}}},"r":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}},"l":{"docs":{},"y":{"docs":{},"g":{"docs":{},"l":{"docs":{},"o":{"docs":{},"t":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}}},"r":{"docs":{},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"i":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"07_best_practices/07_01_best_practices_for_running_local_llms.html":{"ref":"07_best_practices/07_01_best_practices_for_running_local_llms.html","tf":2.1666666666666665},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}}}}}}}}}},"o":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.01437699680511182},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.010256410256410256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":1.6779342723004693}},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}},"t":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},"i":{"docs":{},"v":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}}}}}}}},"j":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},"s":{"docs":{},".":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"'":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"’":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"t":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}},"v":{"docs":{},"i":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"d":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00583941605839416},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.007987220447284345},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_01_collection_of_prompts.html":{"ref":"08_prompts/08_01_collection_of_prompts.html","tf":0.1111111111111111},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.006787330316742082},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}},"b":{"docs":{},"l":{"docs":{},"e":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}}},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.012027491408934709},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.022222222222222223},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.02050113895216401},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.009584664536741214},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},".":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}}}}}}}},"m":{"docs":{},"p":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.015384615384615385},"08_prompts/08_01_collection_of_prompts.html":{"ref":"08_prompts/08_01_collection_of_prompts.html","tf":5.222222222222222},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}},")":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},".":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},"?":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}},"g":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993}},"m":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"l":{"docs":{},"y":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}},"t":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}}}},"s":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705}}}},"'":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}}},"i":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}},"v":{"docs":{},"a":{"docs":{},"c":{"docs":{},"i":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}},"y":{"docs":{},",":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256}},"i":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}}}},"c":{"docs":{},"e":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},".":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},")":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"]":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"*":{"docs":{},":":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}}},"e":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}},"c":{"docs":{},"i":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.014450867052023121},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.012006861063464836},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.01730769230769231},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.020437956204379562},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.022535211267605635},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055}}},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00583941605839416},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.017391304347826087},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.016901408450704224},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.01812688821752266}}}}}}}}},"v":{"docs":{},"i":{"docs":{},"o":{"docs":{},"u":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}}}}}},"f":{"docs":{},"i":{"docs":{},"x":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}},"e":{"docs":{},"r":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}}}}}},"d":{"docs":{},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}}}},"s":{"docs":{},"e":{"docs":{},"t":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.015748031496062992}},",":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}},":":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}},"s":{"docs":{},",":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}}}}}},"p":{"docs":{},"a":{"docs":{},"r":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},"r":{"docs":{},"o":{"docs":{},"c":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}}}}}}}}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.016968325791855202}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.018461538461538463},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.009615384615384616},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.03468208092485549},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.010291595197255575},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.006872852233676976},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.02181818181818182},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.01730769230769231},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.024691358024691357},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.01366742596810934},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.014598540145985401},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.015748031496062992},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.013015184381778741},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.010256410256410256},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.015706806282722512},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.030542986425339366},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":1.1285024154589371},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":1.6863849765258214},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":2.5181268882175227},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.020202020202020204},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{},"e":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.011560693641618497},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}}},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}},"s":{"docs":{},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}},"s":{"docs":{},"o":{"docs":{},"n":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":3.335595776772247}}}},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}},"m":{"docs":{},"i":{"docs":{},"s":{"docs":{},"s":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}},"o":{"docs":{},"p":{"docs":{},"l":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"d":{"docs":{},"r":{"docs":{},"o":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}},"i":{"docs":{},"c":{"docs":{},"k":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":2.501923076923077},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":2.502169197396963}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}},"u":{"docs":{},"s":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.0072992700729927005},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.007399577167019027}},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}},"r":{"docs":{},"p":{"docs":{},"o":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"e":{"docs":{},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}},"c":{"docs":{},"h":{"docs":{},"a":{"docs":{},"s":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}}},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},"b":{"docs":{},"l":{"docs":{},"i":{"docs":{},"c":{"docs":{},"l":{"docs":{},"i":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}}},"l":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055}}}}},"h":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.01167883211678832}}}}},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}},"y":{"docs":{},"s":{"docs":{},"i":{"docs":{},"c":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.006389776357827476}}}}}},"o":{"docs":{},"t":{"docs":{},"o":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}},"g":{"docs":{},"r":{"docs":{},"a":{"docs":{},"p":{"docs":{},"h":{"docs":{},"i":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}}},"t":{"docs":{},"q":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}},"y":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"n":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.015296367112810707},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}},"c":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":3.338568935427574}},"i":{"docs":{},"e":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"s":{"docs":{},"u":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"r":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993}},"e":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}},"f":{"docs":{},"e":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},"a":{"docs":{},"c":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}}},"i":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}},"e":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}},"l":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}}},"s":{"docs":{},"o":{"docs":{},"u":{"docs":{},"r":{"docs":{},"c":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.021538461538461538},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.046242774566473986},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.012006861063464836},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.014598540145985401},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.009560229445506692},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.007692307692307693}},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.009230769230769232},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}}}},"l":{"docs":{},"u":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.011267605633802818},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":3.341251885369532},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409}},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"c":{"docs":{},"o":{"docs":{},"l":{"docs":{},"l":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}}}},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}}}},"p":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.011182108626198083},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"e":{"docs":{},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},"s":{"docs":{},",":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},".":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"d":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},"l":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"i":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}},"e":{"docs":{},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}},"n":{"docs":{},"c":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}},"e":{"docs":{},"v":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}},"a":{"docs":{},"t":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},"h":{"docs":{},"i":{"docs":{},"p":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488}}}}}}}}}}},"e":{"docs":{},"v":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"a":{"docs":{},"s":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}}},"a":{"docs":{},"l":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"l":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}},"i":{"docs":{},"z":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"d":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"i":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}},"m":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"c":{"docs":{},"t":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.015384615384615385}},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}},"s":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}},"c":{"docs":{},"u":{"docs":{},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}},"s":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},"l":{"docs":{},"y":{"docs":{},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"?":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}}}}},"o":{"docs":{},"g":{"docs":{},"n":{"docs":{},"i":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}}},"m":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}}},"m":{"docs":{},"o":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}},"v":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.006342494714587738}}}},"e":{"docs":{},"m":{"docs":{},"b":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"d":{"docs":{},"u":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0137221269296741},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.012779552715654952},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}},"p":{"docs":{},"e":{"docs":{},"t":{"docs":{},"i":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},"y":{"docs":{},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"c":{"docs":{},"l":{"docs":{},"o":{"docs":{},"n":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}}}},"r":{"docs":{},"e":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}}}}}}}}},"q":{"docs":{},"u":{"docs":{},"i":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.012307692307692308},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.014450867052023121},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.010309278350515464},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.009615384615384616},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.014778325123152709},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.01167883211678832},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.008676789587852495},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.018867924528301886},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}},"d":{"docs":{},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055}}}}}}},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}},"s":{"docs":{},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525}}}}},"r":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"a":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446}}}}},"i":{"docs":{},"e":{"docs":{},"w":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}},"s":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"g":{"docs":{},"u":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}}}}}},"e":{"docs":{},"x":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.006342494714587738}},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"b":{"docs":{},"u":{"docs":{},"i":{"docs":{},"l":{"docs":{},"d":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.009259259259259259},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.012307692307692308},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":3.3478787878787877},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":2.503846153846154},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},")":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}},"s":{"docs":{},"k":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488}}}}},"u":{"docs":{},"n":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":2.026415094339623},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.009615384615384616},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.01090909090909091},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.009111617312072893},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"07_best_practices/07_01_best_practices_for_running_local_llms.html":{"ref":"07_best_practices/07_01_best_practices_for_running_local_llms.html","tf":2.1666666666666665},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.020942408376963352},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.023715415019762844}},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419}}},"t":{"docs":{},"i":{"docs":{},"m":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.014778325123152709},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}}}}},"a":{"docs":{},"n":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0076481835564053535},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},"e":{"docs":{},".":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}},"d":{"docs":{},"o":{"docs":{},"m":{"docs":{},"l":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}}}},"k":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}},"m":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.006389776357827476},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.013015184381778741},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.015706806282722512},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553}},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}},".":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}},")":{"docs":{},",":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738}}},".":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}},"p":{"docs":{},"i":{"docs":{},"d":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"p":{"docs":{},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}},"b":{"docs":{},"u":{"docs":{},"s":{"docs":{},"t":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},",":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}},"o":{"docs":{},"t":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},":":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}},"s":{"docs":{},",":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}},"o":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"g":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}},",":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},"’":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662}}},"t":{"docs":{},"x":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":1.1256038647342996},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.015105740181268883}}}},"y":{"docs":{},"z":{"docs":{},"e":{"docs":{},"n":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":1.1256038647342996}}}}},"a":{"docs":{},"n":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"s":{"docs":{},"s":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}},"s":{"docs":{},"e":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0335195530726257},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.015748031496062992},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.01757188498402556},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.011857707509881422}},"u":{"docs":{},"p":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}},".":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}},"t":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"s":{"docs":{},".":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}},":":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}}}}}}}},"e":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}},"c":{"docs":{},"u":{"docs":{},"r":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"07_best_practices/07_01_best_practices_for_running_local_llms.html":{"ref":"07_best_practices/07_01_best_practices_for_running_local_llms.html","tf":0.08333333333333333},"08_prompts/08_01_collection_of_prompts.html":{"ref":"08_prompts/08_01_collection_of_prompts.html","tf":0.1111111111111111},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_01_advanced_model_tuning.html":{"ref":"09_advanced_usage/09_01_advanced_model_tuning.html","tf":0.1111111111111111},"09_advanced_usage/09_02_integrating_llms_into_workflows.html":{"ref":"09_advanced_usage/09_02_integrating_llms_into_workflows.html","tf":0.1111111111111111},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}},"s":{"docs":{},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}}},"r":{"docs":{},"e":{"docs":{},"t":{"docs":{},"s":{"docs":{},".":{"docs":{},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{},"_":{"docs":{},"t":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}}}}}}}}}}}}},"n":{"docs":{},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}},"i":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159}}}}}}}}},"r":{"docs":{},"v":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}},"e":{"docs":{},"r":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"s":{"docs":{},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}},"i":{"docs":{},"c":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}},",":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}}},"i":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715}},"o":{"docs":{},"u":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}},"a":{"docs":{},"r":{"docs":{},"c":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}},"m":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},"s":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"l":{"docs":{},"i":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},"y":{"docs":{},".":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}}}}}}}}}},"l":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":3.3587878787878784},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.015748031496062992},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.013015184381778741},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}}},"f":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}},"d":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448}}},"g":{"docs":{},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}}}}},"h":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}},"w":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}},"o":{"docs":{},"w":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"c":{"docs":{},"a":{"docs":{},"s":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"r":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"e":{"docs":{},"n":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}},"r":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}},"e":{"docs":{},"e":{"docs":{},"r":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}},"t":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}},"l":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":3.3375616631430582}}}}},"i":{"docs":{},"f":{"docs":{},"t":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738}}}}}},"t":{"docs":{},"a":{"docs":{},"r":{"docs":{},"e":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}},"c":{"docs":{},"o":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.016317016317016316}},"e":{"docs":{},"r":{"docs":{},"’":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}},"t":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}},"y":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},"n":{"docs":{},"d":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":2.5025641025641026},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"s":{"docs":{},".":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}},"t":{"docs":{},"e":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"m":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}},",":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}},"i":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}},"i":{"docs":{},"l":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"e":{"docs":{},"p":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.03272727272727273},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.011857707509881422}},"s":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"—":{"docs":{},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}}}}}}}}}},".":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},":":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}},"r":{"docs":{},"e":{"docs":{},"n":{"docs":{},"g":{"docs":{},"t":{"docs":{},"h":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767}}},":":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}},"a":{"docs":{},"m":{"docs":{},"l":{"docs":{},"i":{"docs":{},"n":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767}}}}}}}},"i":{"docs":{},"c":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}},"k":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}},"n":{"docs":{},"g":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}}}},"x":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}},"a":{"docs":{},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},".":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}}}}}}}}},"n":{"docs":{},".":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},"t":{"docs":{},"e":{"docs":{},"g":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}}}}},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"u":{"docs":{},"r":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.014545454545454545},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"e":{"docs":{},",":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}}},"g":{"docs":{},"g":{"docs":{},"l":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}},"o":{"docs":{},"n":{"docs":{},"g":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}}}}},"u":{"docs":{},"d":{"docs":{},"i":{"docs":{},"o":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":3.3668528864059586},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":2.062992125984252},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":2.0063897763578273},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":2.5086767895878523},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.020202020202020204}},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}}},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473}}},"a":{"docs":{},"g":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.024630541871921183},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.010471204188481676}},"e":{"docs":{},".":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},",":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}},"y":{"docs":{},"l":{"docs":{},"e":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.005656108597285068},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.01971830985915493},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.01812688821752266}}}}}},"u":{"docs":{},"p":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{},"t":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}},"s":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}},"l":{"docs":{},"i":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},"y":{"docs":{},":":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}}}}}},"e":{"docs":{},"r":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.007918552036199095},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":1.1314009661835749},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.014084507042253521},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.012084592145015106}},"i":{"docs":{},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}},".":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}}}},"b":{"docs":{},"s":{"docs":{},"c":{"docs":{},"r":{"docs":{},"i":{"docs":{},"p":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}}},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}},"m":{"docs":{},"i":{"docs":{},"s":{"docs":{},"s":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},".":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.005656108597285068}}}}}},"c":{"docs":{},"h":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00583941605839416},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"g":{"docs":{},"g":{"docs":{},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}}},".":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}},"i":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.008670520231213872},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.009615384615384616},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155}}}}}}},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}},"i":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"y":{"docs":{},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.009513742071881607}},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}},"r":{"docs":{},"p":{"docs":{},"r":{"docs":{},"i":{"docs":{},"s":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}}},"e":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}},"f":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}},"i":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},",":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}},"x":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}}}}},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}},"c":{"docs":{},"a":{"docs":{},"l":{"docs":{},"e":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":5.020618556701031},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}},"a":{"docs":{},"b":{"docs":{},"i":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},":":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}},"l":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"n":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{},"o":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}}}},"i":{"docs":{},"e":{"docs":{},"n":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"t":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}}}},"h":{"docs":{},"e":{"docs":{},"m":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.018461538461538463},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.02023121387283237},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":5.015437392795883}},":":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525}}}}}}}},"r":{"docs":{},"i":{"docs":{},"p":{"docs":{},"t":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":3.348132487667371}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"e":{"docs":{},"e":{"docs":{},"n":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}},"a":{"docs":{},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}},"o":{"docs":{},"p":{"docs":{},"e":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}},"r":{"docs":{},"e":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.02608695652173913},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.02535211267605634},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.027190332326283987}},"s":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"i":{"docs":{},"d":{"docs":{},"e":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}},".":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}}},"g":{"docs":{},"n":{"docs":{},"i":{"docs":{},"f":{"docs":{},"i":{"docs":{},"c":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{},"l":{"docs":{},"i":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.004792332268370607},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}},"y":{"docs":{},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}}}}}}}},"m":{"docs":{},"p":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"i":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}},"f":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525}}},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}},"i":{"docs":{},"l":{"docs":{},"a":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}},"u":{"docs":{},"l":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"e":{"docs":{},"o":{"docs":{},"u":{"docs":{},"s":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446}}}}}}}}}}}}}},"o":{"docs":{},"n":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"z":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.021538461538461538},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":5.028846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.008670520231213872},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.00686106346483705},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":3.3776683087027912},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.013015184381778741},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},":":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}}},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},"s":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}},".":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"n":{"docs":{},"g":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.012084592145015106}}}}},"l":{"docs":{},"i":{"docs":{},"c":{"docs":{},"o":{"docs":{},"n":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":2.031496062992126},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":2.011182108626198},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},".":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}},",":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}}}}}}}}},"o":{"docs":{},"l":{"docs":{},"e":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}},"v":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}},"u":{"docs":{},"t":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},".":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}},"i":{"docs":{},"d":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}},"m":{"docs":{},"e":{"docs":{},"o":{"docs":{},"n":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}},"t":{"docs":{},"h":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.02127659574468085},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"w":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}},",":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}},"p":{"docs":{},"h":{"docs":{},"i":{"docs":{},"s":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}},"u":{"docs":{},"r":{"docs":{},"c":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":2.006993006993007},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}}}}},"f":{"docs":{},"t":{"docs":{},"w":{"docs":{},"a":{"docs":{},"r":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}},"p":{"docs":{},"e":{"docs":{},"c":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"i":{"docs":{},"f":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.013888888888888888},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.009230769230769232},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0076481835564053535},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"i":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}},"c":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}}}}}}}}}}}},"a":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.009259259259259259},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767}},"i":{"docs":{},"z":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}}}}}},"s":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},":":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}},"e":{"docs":{},"d":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.008759124087591242},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.01597444089456869},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.018867924528301886},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}}},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128}}},"s":{"docs":{},".":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}},"n":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.007918552036199095}},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"a":{"docs":{},"c":{"docs":{},"e":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},".":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},"n":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"r":{"docs":{},"e":{"docs":{},".":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}},"m":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"r":{"docs":{},"e":{"docs":{},"a":{"docs":{},"d":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}},"a":{"docs":{},"m":{"docs":{},"s":{"docs":{},"u":{"docs":{},"n":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"e":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}},"v":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"s":{"docs":{},",":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}},"c":{"docs":{},"r":{"docs":{},"i":{"docs":{},"f":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}},"i":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}}}}}}},"w":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}},"l":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.008676789587852495},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}}}}},"o":{"docs":{},"o":{"docs":{},"t":{"docs":{},"h":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496}},"l":{"docs":{},"i":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}}},"i":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},"y":{"docs":{},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.009615384615384616},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.01730769230769231},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.015945330296127564},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.01897810218978102},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.01757188498402556},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.019522776572668113},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.01583710407239819},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.011267605633802818},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},".":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}},"'":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.01167883211678832},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"’":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},".":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.006389776357827476}}},":":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.006389776357827476},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"—":{"docs":{},"c":{"docs":{},"a":{"docs":{},"n":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}},"a":{"docs":{},"t":{"docs":{"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}},"m":{"docs":{},"m":{"docs":{},"e":{"docs":{},"t":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525}}}}}}},"n":{"docs":{},"t":{"docs":{},"a":{"docs":{},"x":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}},"d":{"docs":{},"o":{"docs":{},"w":{"docs":{},"n":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}},":":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}},"t":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}},"l":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}}}}}},"n":{"docs":{},"i":{"docs":{},"p":{"docs":{},"p":{"docs":{},"e":{"docs":{},"t":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629}}},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}}}}},"a":{"docs":{},"k":{"docs":{},"e":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}}}}}},"w":{"docs":{},"a":{"docs":{},"p":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},"i":{"docs":{},"t":{"docs":{},"c":{"docs":{},"h":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"f":{"docs":{},"t":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}},"/":{"docs":{},"_":{"docs":{},"/":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"s":{"docs":{},"d":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},":":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"k":{"docs":{},"i":{"docs":{},"l":{"docs":{},"l":{"docs":{},"s":{"docs":{},",":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}}},"t":{"docs":{},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"i":{"docs":{},"l":{"docs":{},"o":{"docs":{},"r":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738}}}}}},"k":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.007692307692307693}},"n":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}},"a":{"docs":{},"h":{"docs":{},"a":{"docs":{},"s":{"docs":{},"h":{"docs":{},"i":{"docs":{},":":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}}}},"s":{"docs":{},"k":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.008456659619450317},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.016826923076923076},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.006872852233676976},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.009615384615384616},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.008759124087591242},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.011655011655011656},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.02830188679245283},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.016968325791855202},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.03188405797101449},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.030985915492957747},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.027190332326283987}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.009259259259259259},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.009615384615384616},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.009324009324009324},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"08_prompts/08_01_collection_of_prompts.html":{"ref":"08_prompts/08_01_collection_of_prompts.html","tf":0.1111111111111111},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.005656108597285068},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}},"l":{"docs":{},"k":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}},"e":{"docs":{},"b":{"docs":{},"i":{"docs":{},":":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}},"r":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}},"u":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.014545454545454545},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":3.3616352201257858},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":2.5205128205128204},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.019230769230769232},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553}},"s":{"docs":{},"?":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}},"]":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},"—":{"docs":{},"y":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"a":{"docs":{},"c":{"docs":{},"h":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}},"r":{"docs":{},"m":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}},"i":{"docs":{},"n":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"c":{"docs":{},"h":{"docs":{},"n":{"docs":{},"i":{"docs":{},"q":{"docs":{},"u":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"09_advanced_usage/09_01_advanced_model_tuning.html":{"ref":"09_advanced_usage/09_01_advanced_model_tuning.html","tf":0.1111111111111111},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}},"c":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}},"o":{"docs":{},"l":{"docs":{},"o":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}}}},"x":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.010570824524312896},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.004524886877828055},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.013888888888888888}}}},"n":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}},"s":{"docs":{},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788}},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525}}}}}}},"m":{"docs":{},"p":{"docs":{},"o":{"docs":{},"r":{"docs":{},"a":{"docs":{},"r":{"docs":{},"i":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}},"l":{"docs":{},"a":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041}},"e":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}},"h":{"docs":{},"a":{"docs":{},"n":{"docs":{},"k":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}},"t":{"docs":{},"’":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},",":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},"o":{"docs":{},"u":{"docs":{},"g":{"docs":{},"h":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}},"s":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}},"s":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},"r":{"docs":{},"o":{"docs":{},"u":{"docs":{},"g":{"docs":{},"h":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}},"l":{"docs":{},"i":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"v":{"docs":{},"e":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}},"e":{"docs":{},"e":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"a":{"docs":{},"d":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.011182108626198083}}}}},"o":{"docs":{},"u":{"docs":{},"g":{"docs":{},"h":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}},"e":{"docs":{},"r":{"docs":{},"e":{"docs":{},"’":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}},"b":{"docs":{},"i":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}},"m":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}},"b":{"docs":{},"l":{"docs":{},"o":{"docs":{},"k":{"docs":{},"e":{"docs":{},"/":{"docs":{},"c":{"docs":{},"o":{"docs":{},"d":{"docs":{},"e":{"docs":{},"l":{"docs":{},"l":{"docs":{},"a":{"docs":{},"m":{"docs":{},"a":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621}}}}}}}}}}}}}}}}},"y":{"docs":{},"'":{"docs":{},"r":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}}}}}},"i":{"docs":{},"p":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"s":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}},"m":{"docs":{},"e":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.008456659619450317},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.011389521640091117},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}}},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},".":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738}}}}},"n":{"docs":{},"k":{"docs":{},"e":{"docs":{},"r":{"docs":{},",":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835}}}}}}},"e":{"docs":{},"r":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}},".":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}},"t":{"docs":{},"l":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.019027484143763214}},"e":{"docs":{},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"s":{"docs":{},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}},"o":{"docs":{},"g":{"docs":{},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}},"e":{"docs":{},"r":{"docs":{},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}}}}},"d":{"docs":{},"a":{"docs":{},"y":{"docs":{},"’":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}},"o":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}},"o":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.011655011655011656},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":10.030303030303031},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},"s":{"docs":{},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.020202020202020204}}}}}},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},"p":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}},"i":{"docs":{},"c":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.011182108626198083}}}}},"_":{"docs":{},"t":{"docs":{},"i":{"docs":{},"t":{"docs":{},"l":{"docs":{},"e":{"docs":{},"_":{"docs":{},"c":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"(":{"docs":{},")":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}}}},"c":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448}},".":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}},"r":{"docs":{},"y":{"docs":{},".":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.006342494714587738},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.008576329331046312},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.022336769759450172},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.013888888888888888}},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},",":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}},"n":{"docs":{},"s":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159}},"e":{"docs":{},"r":{"docs":{},")":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"s":{"docs":{},")":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}}}}}},"e":{"docs":{},"r":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.011267605633802818},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.012084592145015106}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},",":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553}}}}}},"l":{"docs":{},"a":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}},"i":{"docs":{},"t":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}},"d":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"a":{"docs":{},"l":{"docs":{},"l":{"docs":{},"y":{"docs":{},",":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358}}}}}}}}}}}}},"v":{"docs":{},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"c":{"docs":{},"k":{"docs":{},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}},"i":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}},"a":{"docs":{},"l":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}},"g":{"docs":{},"g":{"docs":{},"e":{"docs":{},"r":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}},"u":{"docs":{},"r":{"docs":{},"n":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}},"n":{"docs":{},"e":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.006872852233676976},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"09_advanced_usage/09_01_advanced_model_tuning.html":{"ref":"09_advanced_usage/09_01_advanced_model_tuning.html","tf":3.5555555555555554},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}}}},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{},"i":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}}}},"y":{"docs":{},"p":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},",":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}},"]":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"i":{"docs":{},"c":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}},"u":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409}},"s":{"docs":{},"t":{"docs":{},"a":{"docs":{},"n":{"docs":{},"d":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.010570824524312896},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.016203703703703703},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.009230769230769232},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.01818181818181818},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.015184381778741865},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.010256410256410256},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}}}}}}},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}},"l":{"docs":{},"i":{"docs":{},"k":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}},"e":{"docs":{},"a":{"docs":{},"s":{"docs":{},"h":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}},"s":{"docs":{},"s":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"o":{"docs":{},"c":{"docs":{},"k":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}},"i":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}},"s":{"docs":{},")":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0049382716049382715}},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}},"q":{"docs":{},"u":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}},"f":{"docs":{},"i":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}},"s":{"docs":{},"e":{"docs":{},"e":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}},"u":{"docs":{},"r":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},"v":{"docs":{},"e":{"docs":{},"i":{"docs":{},"l":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}}}},"p":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.004629629629629629},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.00859106529209622},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.01597444089456869},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.013742071881606765},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.011857707509881422}},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.013605442176870748},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.009513742071881607}}}}},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}},"o":{"docs":{},"n":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}},".":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}},":":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},")":{"docs":{},",":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"g":{"docs":{},"r":{"docs":{},"a":{"docs":{},"d":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},"e":{"docs":{},"s":{"docs":{},".":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}},":":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}}}}},"s":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":5.035460992907802},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.011320754716981131},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.014799154334038054},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.011574074074074073},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.01201923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.010291595197255575},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.019704433497536946},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.014814814814814815},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.011389521640091117},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.008759124087591242},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.01282051282051282},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.013742071881606765},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.01583710407239819},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.011594202898550725},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.006042296072507553},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.019762845849802372}},"e":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.018518518518518517}},".":{"docs":{},"e":{"docs":{},"m":{"docs":{},"a":{"docs":{},"i":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"s":{"docs":{},",":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}},".":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},"d":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},".":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"e":{"docs":{},"f":{"docs":{},"f":{"docs":{},"e":{"docs":{},"c":{"docs":{},"t":{"docs":{},")":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}}}}},"s":{"docs":{},":":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"a":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.017341040462427744},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.024630541871921183},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.016058394160583942},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.009584664536741214},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}},"e":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.011560693641618497},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}},":":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}},"—":{"docs":{},"t":{"docs":{},"h":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}},"b":{"docs":{},"l":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}},"t":{"docs":{},"i":{"docs":{},"l":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.006389776357827476}},"i":{"docs":{},"t":{"docs":{},"y":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}},"z":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},":":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}}}}}}}},"b":{"docs":{},"u":{"docs":{},"n":{"docs":{},"t":{"docs":{},"u":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},".":{"docs":{},"s":{"docs":{},".":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.009259259259259259}}}}}},"v":{"1":{"docs":{},".":{"docs":{},"q":{"4":{"docs":{},"_":{"docs":{},"k":{"docs":{},"_":{"docs":{},"s":{"docs":{},".":{"docs":{},"g":{"docs":{},"g":{"docs":{},"u":{"docs":{},"f":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},":":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}}}}}}}},"5":{"docs":{},"_":{"docs":{},"k":{"docs":{},"_":{"docs":{},"m":{"docs":{},".":{"docs":{},"g":{"docs":{},"g":{"docs":{},"u":{"docs":{},"f":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}}}}}}},"6":{"docs":{},"_":{"docs":{},"k":{"docs":{},"_":{"docs":{},"s":{"docs":{},".":{"docs":{},"g":{"docs":{},"g":{"docs":{},"u":{"docs":{},"f":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}},":":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}}}}}}}},"8":{"docs":{},"_":{"0":{"docs":{},".":{"docs":{},"g":{"docs":{},"g":{"docs":{},"u":{"docs":{},"f":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}},",":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}}}}},"docs":{}}},"docs":{}}}},"docs":{},"a":{"docs":{},"l":{"docs":{},"u":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00583941605839416},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"a":{"docs":{},"b":{"docs":{},"l":{"docs":{"./":{"ref":"./","tf":0.013605442176870748},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788}}}}}},"i":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}}}}}}},"s":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}},"r":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767}},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},"s":{"docs":{},"—":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},",":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}}}}}}},"t":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":5.0092592592592595},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.007692307692307693},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":2.502919708029197},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":2.5095602294455066},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},"s":{"docs":{},",":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767}}}}}}}}},"o":{"docs":{},"u":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.011655011655011656},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}},"e":{"docs":{},"t":{"docs":{},"i":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}},"y":{"docs":{},".":{"docs":{"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}},"g":{"docs":{},"u":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}},"e":{"docs":{},"r":{"docs":{},"s":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.009259259259259259},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0057692307692307696},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0076481835564053535},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},"e":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}}}}}},"i":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.004807692307692308},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621}},"f":{"docs":{},"i":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}}}},"c":{"docs":{},"t":{"docs":{},"o":{"docs":{},"r":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}},"h":{"docs":{},"i":{"docs":{},"c":{"docs":{},"l":{"docs":{},"e":{"docs":{},"s":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}},"n":{"docs":{},"g":{"docs":{},"e":{"docs":{},"a":{"docs":{},"n":{"docs":{},"c":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362}}}}}}}}},"i":{"docs":{},"r":{"docs":{},"t":{"docs":{},"u":{"docs":{},"a":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}},"e":{"docs":{},"w":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}},"t":{"docs":{},"a":{"docs":{},"l":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"a":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"s":{"docs":{},"u":{"docs":{},"a":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102}}}}}},"d":{"docs":{},"e":{"docs":{},"o":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"s":{"docs":{},",":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}},"n":{"docs":{},"c":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.00683371298405467},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.013089005235602094}},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.004555808656036446},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247}}},"]":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}},"s":{"docs":{"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.020202020202020204},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}},".":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":2.504938271604938},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"o":{"docs":{},"l":{"docs":{},"u":{"docs":{},"m":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}}}},"w":{"docs":{},"a":{"docs":{},"n":{"docs":{},"t":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"y":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"r":{"docs":{},"n":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}},"l":{"docs":{},"k":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"s":{"docs":{},"n":{"docs":{},"'":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}},"t":{"docs":{},"t":{"docs":{},"a":{"docs":{},"g":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}}}},"e":{"docs":{},"'":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"l":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}},"l":{"docs":{},"c":{"docs":{},"o":{"docs":{},"m":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}},"l":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.006872852233676976},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.007692307692307693},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}},"’":{"docs":{},"l":{"docs":{},"l":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.006944444444444444},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}}}},"i":{"docs":{},"g":{"docs":{},"h":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.009852216748768473}}}}}},"a":{"docs":{},"k":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}},"b":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.013888888888888888}},"s":{"docs":{},"i":{"docs":{},"t":{"docs":{},"e":{"docs":{},".":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}}}},"h":{"docs":{},"e":{"docs":{},"t":{"docs":{},"h":{"docs":{},"e":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.013605442176870748},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943}}}}}}},"o":{"docs":{},"'":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}},"i":{"docs":{},"c":{"docs":{},"h":{"docs":{},",":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}}}}}},"o":{"docs":{},"r":{"docs":{},"k":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.009876543209876543},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.009111617312072893},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.016317016317016316},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.007874015748031496},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}},",":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774}}},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794},"09_advanced_usage/09_02_integrating_llms_into_workflows.html":{"ref":"09_advanced_usage/09_02_integrating_llms_into_workflows.html","tf":3.444444444444444}},"s":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"09_advanced_usage/09_02_integrating_llms_into_workflows.html":{"ref":"09_advanced_usage/09_02_integrating_llms_into_workflows.html","tf":0.1111111111111111}}}},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.007272727272727273}}},".":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}}}},".":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.007547169811320755}}},"s":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},":":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"p":{"docs":{},"a":{"docs":{},"c":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.0111731843575419}},"e":{"docs":{},".":{"docs":{"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}}}}}},"l":{"docs":{},"o":{"docs":{},"a":{"docs":{},"d":{"docs":{"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.007853403141361256},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}},"s":{"docs":{},".":{"docs":{"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.005633802816901409},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},",":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}}}}}}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"l":{"docs":{},"d":{"docs":{"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"w":{"docs":{},"i":{"docs":{},"d":{"docs":{},"e":{"docs":{},".":{"docs":{"./":{"ref":"./","tf":0.006802721088435374}}}}}}}}},"d":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},")":{"docs":{"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.001597444089456869}}}}},"t":{"docs":{},"h":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}},"r":{"docs":{},"i":{"docs":{"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0019120458891013384}}}}}},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.011320754716981131},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.007407407407407408},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.004662004662004662},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.011182108626198083},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}},"i":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"11_additional_resources/11_01_tools.html":{"ref":"11_additional_resources/11_01_tools.html","tf":0.010101010101010102},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}},".":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"t":{"docs":{},":":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}},"d":{"docs":{},"e":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.006993006993006993},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"r":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}},"n":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.003436426116838488}}}}},"l":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}},"z":{"docs":{},"a":{"docs":{},"r":{"docs":{},"d":{"docs":{},"c":{"docs":{},"o":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}}}}}},"n":{"docs":{},"d":{"docs":{},"o":{"docs":{},"w":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.006787330316742082},"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.007905138339920948}},"s":{"docs":{},",":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}}}}},"r":{"docs":{},"i":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}},"e":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}}},"t":{"docs":{},"e":{"docs":{},"n":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"i":{"docs":{},"n":{"docs":{},"g":{"docs":{},".":{"docs":{"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331}}}}}}}},"o":{"docs":{},"n":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147}}}}}}},"y":{"docs":{},"o":{"docs":{},"u":{"docs":{},"'":{"docs":{},"r":{"docs":{"./":{"ref":"./","tf":0.006802721088435374},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},"l":{"docs":{},"l":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}}},"’":{"docs":{},"l":{"docs":{},"l":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095}}}},"d":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794},"02_understanding_large_language_models/02_02_model_variations.html":{"ref":"02_understanding_large_language_models/02_02_model_variations.html","tf":0.0023148148148148147},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}},"v":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"r":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}},"r":{"docs":{},"s":{"docs":{},"e":{"docs":{},"l":{"docs":{},"f":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}},"e":{"docs":{},"a":{"docs":{},"r":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.014184397163120567}}}},"s":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}}}}},"k":{"docs":{},"n":{"docs":{},"o":{"docs":{},"w":{"docs":{"01_introduction/01_01_how_to_use_this_guide.html":{"ref":"01_introduction/01_01_how_to_use_this_guide.html","tf":0.0070921985815602835},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.007211538461538462},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.006507592190889371}},"l":{"docs":{},"e":{"docs":{},"d":{"docs":{},"g":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}},"e":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}}}},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.0021141649048625794}}}}}},"e":{"docs":{},"e":{"docs":{},"p":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419}}}},"y":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0038240917782026767},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"ref":"04_models_for_ coding/04_02_other_open_source_coding_models.html","tf":0.002331002331002331},"05_installation/05_01_installing_lm_studio.html":{"ref":"05_installation/05_01_installing_lm_studio.html","tf":0.00558659217877095},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"ref":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","tf":0.003194888178913738},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.004338394793926247},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.002617801047120419},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}},"_":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}},"l":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}},":":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}}},"m":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.009230769230769232},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}},":":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}}},"s":{"docs":{},":":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}}}},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"ref":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","tf":0.0024691358024691358},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"p":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.005797101449275362},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},"s":{"docs":{},")":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"t":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},")":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}}}}},"q":{"2":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}},")":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}},":":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}}},"3":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.006153846153846154},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}},":":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}},"_":{"docs":{},"k":{"docs":{},"_":{"docs":{},"l":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}}},"4":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}},":":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}},"_":{"docs":{},"k":{"docs":{},"_":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}},"m":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}}},"5":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}},")":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}},":":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}},"_":{"docs":{},"k":{"docs":{},"_":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}},"m":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621}}}}}}},"6":{"docs":{},"_":{"docs":{},"k":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}},"_":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0038461538461538464}},"m":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}}}}}},"8":{"docs":{},"_":{"0":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"docs":{}}},"docs":{},"u":{"docs":{},"a":{"docs":{},"l":{"docs":{},"i":{"docs":{},"t":{"docs":{},"i":{"docs":{"01_introduction/01_02_why_run_models_locally.html":{"ref":"01_introduction/01_02_why_run_models_locally.html","tf":0.0037735849056603774},"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.005128205128205128},"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}},"n":{"docs":{},"t":{"docs":{},"i":{"docs":{},"z":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.03076923076923077},"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":10.031791907514451},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":5.051457975986278},"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.005154639175257732},"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.004379562043795621},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.017391304347826087},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.01812688821752266}},"a":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077}}},")":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},".":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.005145797598627788},"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}},":":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.010291595197255575}}}}}}}},"e":{"docs":{},"d":{"docs":{},",":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}},".":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},":":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.008695652173913044},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.008450704225352112},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}}}}},"t":{"docs":{},"i":{"docs":{"03_selecting_models/03_03_model_size_and_memory.html":{"ref":"03_selecting_models/03_03_model_size_and_memory.html","tf":0.0049261083743842365}}}}}}}},"e":{"docs":{},"s":{"docs":{},"t":{"docs":{},"i":{"docs":{},"o":{"docs":{},"n":{"docs":{"02_understanding_large_language_models/02_01_overview_of_llms.html":{"ref":"02_understanding_large_language_models/02_01_overview_of_llms.html","tf":0.004228329809725159}}}}}}}},"i":{"docs":{},"c":{"docs":{},"k":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.005780346820809248},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"06_evaluation/06_01_testing_each_model.html":{"ref":"06_evaluation/06_01_testing_each_model.html","tf":0.009433962264150943},"09_advanced_usage/09_03_custom_built_pcs.html":{"ref":"09_advanced_usage/09_03_custom_built_pcs.html","tf":0.005235602094240838}},"l":{"docs":{},"y":{"docs":{},".":{"docs":{"02_understanding_large_language_models/02_05_quantization.html":{"ref":"02_understanding_large_language_models/02_05_quantization.html","tf":0.002890173410404624}}}},"i":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"ref":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","tf":0.0057361376673040155},"06_evaluation/06_02_evaluating_models.html":{"ref":"06_evaluation/06_02_evaluating_models.html","tf":0.002564102564102564}}}},",":{"docs":{"03_selecting_models/03_01_selecting_the right model.html":{"ref":"03_selecting_models/03_01_selecting_the right model.html","tf":0.0036363636363636364}}}}},"e":{"docs":{},"t":{"docs":{},"l":{"docs":{},"i":{"docs":{"11_additional_resources/11_03_in_the_news.html":{"ref":"11_additional_resources/11_03_in_the_news.html","tf":0.004629629629629629}}}}}}}},"a":{"docs":{},"t":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.0017152658662092624}}}}},"_":{"docs":{"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"ref":"02_understanding_large_language_models/02_03_model_naming_conventions.html","tf":0.003076923076923077},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},">":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154},"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"\\":{"docs":{"02_understanding_large_language_models/02_04_parameter_size.html":{"ref":"02_understanding_large_language_models/02_04_parameter_size.html","tf":0.002403846153846154}}},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},"i":{"docs":{},"q":{"docs":{},"\"":{"docs":{"02_understanding_large_language_models/02_06_quantization_schemes.html":{"ref":"02_understanding_large_language_models/02_06_quantization_schemes.html","tf":0.003430531732418525}}}},"n":{"docs":{},"s":{"docs":{},"t":{"docs":{},"r":{"docs":{},"u":{"docs":{},"c":{"docs":{},"t":{"docs":{},"\"":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}}}},"b":{"docs":{},"i":{"docs":{},"g":{"docs":{},"g":{"docs":{},"e":{"docs":{},"r":{"docs":{},"\"":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}}},"d":{"docs":{},"e":{"docs":{},"e":{"docs":{},"p":{"docs":{},"e":{"docs":{},"r":{"docs":{},".":{"docs":{},"\"":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}},"f":{"docs":{},"a":{"docs":{},"u":{"docs":{},"l":{"docs":{},"t":{"docs":{"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"ref":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","tf":0.015748031496062992}}}}}}}}},"w":{"docs":{},"i":{"docs":{},"d":{"docs":{},"e":{"docs":{},"r":{"docs":{},"\"":{"docs":{"02_understanding_large_language_models/02_07_scaling_models.html":{"ref":"02_understanding_large_language_models/02_07_scaling_models.html","tf":0.001718213058419244}}}}}}}},"o":{"docs":{},"u":{"docs":{},"t":{"docs":{"03_selecting_models/03_05_offloading_to_the_gpu.html":{"ref":"03_selecting_models/03_05_offloading_to_the_gpu.html","tf":0.002277904328018223}}}}},"f":{"docs":{},"u":{"docs":{},"l":{"docs":{},"l":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854},"05_installation/05_04_picking_models_in_lm_studio.html":{"ref":"05_installation/05_04_picking_models_in_lm_studio.html","tf":0.0021691973969631237}}}}}},"g":{"docs":{},"p":{"docs":{},"u":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00291970802919708}}}}},"l":{"docs":{},"i":{"docs":{},"k":{"docs":{},"e":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}}}}},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}}},"#":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"$":{"docs":{},"(":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}}}}}}}}}},"c":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"_":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.005285412262156448}},")":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}}}}},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"_":{"docs":{},"p":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},"/":{"docs":{},"*":{"docs":{},";":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}}},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"r":{"docs":{},"y":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.009513742071881607}},")":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}},")":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}},"f":{"docs":{},"i":{"docs":{},"l":{"docs":{},"e":{"docs":{},"_":{"docs":{},"p":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}},"h":{"1":{"docs":{},"_":{"docs":{},"t":{"docs":{},"i":{"docs":{},"t":{"docs":{},"l":{"docs":{},"e":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}},"docs":{}},"i":{"docs":{},"n":{"docs":{},"p":{"docs":{},"u":{"docs":{},"t":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}},"{":{"docs":{},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"}":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},"*":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}}}}}}}}}}}},".":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}}}},"c":{"docs":{},"a":{"docs":{},"n":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.006342494714587738}}}}},"r":{"docs":{},"e":{"docs":{},"m":{"docs":{},"o":{"docs":{},"v":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},"u":{"docs":{},"p":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},"y":{"docs":{},"o":{"docs":{},"u":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"~":{"1":{"0":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"4":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"6":{"8":{"docs":{},".":{"1":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"docs":{}}},"docs":{}},"8":{"5":{"docs":{},".":{"6":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"docs":{}}},"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"docs":{},".":{"1":{"7":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"docs":{}},"docs":{}}},"2":{"0":{"3":{"docs":{},".":{"7":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"docs":{}}},"docs":{}},"2":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"3":{"docs":{},".":{"8":{"4":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"docs":{}},"docs":{}}},"4":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"7":{"docs":{},".":{"3":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"docs":{}}},"8":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"docs":{}},"3":{"5":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"docs":{}},"4":{"0":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}},".":{"5":{"7":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"docs":{}},"docs":{}}},"5":{"docs":{},".":{"7":{"3":{"docs":{"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"ref":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","tf":0.00145985401459854}}},"docs":{}},"docs":{}}},"6":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"7":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"8":{"docs":{"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"ref":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","tf":0.0019230769230769232}}},"docs":{},"$":{"1":{"docs":{},",":{"1":{"6":{"9":{"docs":{},".":{"5":{"1":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}}},"2":{"docs":{},",":{"9":{"9":{"9":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},"docs":{}},"docs":{}},"docs":{}}},"8":{"1":{"4":{"docs":{},".":{"9":{"6":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}}},"!":{"docs":{},"=":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"#":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.015856236786469344}},"!":{"docs":{},"/":{"docs":{},"b":{"docs":{},"i":{"docs":{},"n":{"docs":{},"/":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"h":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}},"$":{"1":{"0":{"0":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.01809954751131222}}},"docs":{}},"1":{"9":{"docs":{},".":{"9":{"9":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}}},"docs":{}},"2":{"9":{"docs":{},".":{"9":{"9":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}}},"docs":{}},"7":{"9":{"docs":{},".":{"9":{"2":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},",":{"1":{"6":{"9":{"docs":{},".":{"5":{"1":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"3":{"9":{"6":{"docs":{},".":{"9":{"6":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{}}},"2":{"1":{"7":{"docs":{},".":{"9":{"9":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}}},"docs":{}},"6":{"2":{"docs":{},".":{"3":{"6":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{},",":{"9":{"9":{"9":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}},"docs":{}},"docs":{}},"docs":{}}},"3":{"2":{"4":{"docs":{},".":{"9":{"9":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}}},"docs":{}},"4":{"9":{"docs":{},".":{"9":{"9":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}}},"docs":{}},"6":{"9":{"docs":{},".":{"9":{"8":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"6":{"5":{"3":{"docs":{},".":{"2":{"4":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}}},"docs":{}},"9":{"9":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}},"7":{"3":{"docs":{},".":{"9":{"9":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}}},"docs":{}},"docs":{}}},"docs":{}},"8":{"1":{"4":{"docs":{},".":{"9":{"6":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}}},"docs":{}},"docs":{}}},"docs":{}},"docs":{}},"docs":{},"(":{"docs":{},"b":{"docs":{},"a":{"docs":{},"s":{"docs":{},"e":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}},"c":{"docs":{},"h":{"docs":{},"a":{"docs":{},"p":{"docs":{},"t":{"docs":{},"e":{"docs":{},"r":{"docs":{},"_":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}}}},"i":{"docs":{},"n":{"docs":{},"d":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}}}}}}}}},"{":{"docs":{},"{":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}},"&":{"docs":{"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},"&":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"'":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}},"#":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}},"^":{"docs":{},"#":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}},"m":{"docs":{},"a":{"docs":{},"i":{"docs":{},"n":{"docs":{},"'":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}},"s":{"docs":{},"/":{"docs":{},"^":{"docs":{},"#":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}},"[":{"0":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}},"docs":{}}},"_":{"docs":{},"/":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}},"{":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"(":{"docs":{},"i":{"docs":{},"=":{"1":{"docs":{},";":{"docs":{},"i":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}},"docs":{}}}}}}}}},"*":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.003171247357293869}},".":{"docs":{},"m":{"docs":{},"d":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}},"+":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.003393665158371041},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":1.114009661835749},"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.0030211480362537764}},"x":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},".":{"docs":{},"/":{"docs":{},"g":{"docs":{},"e":{"docs":{},"n":{"docs":{},"e":{"docs":{},"r":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"_":{"docs":{},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"y":{"docs":{},".":{"docs":{},"s":{"docs":{},"h":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}}}}}}}}}}}}}}},"g":{"docs":{},"i":{"docs":{},"t":{"docs":{},"h":{"docs":{},"u":{"docs":{},"b":{"docs":{},",":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}},"/":{"docs":{},"w":{"docs":{},"o":{"docs":{},"r":{"docs":{},"k":{"docs":{},"f":{"docs":{},"l":{"docs":{},"o":{"docs":{},"w":{"docs":{},"s":{"docs":{},"/":{"docs":{},"u":{"docs":{},"p":{"docs":{},"d":{"docs":{},"a":{"docs":{},"t":{"docs":{},"e":{"docs":{},"_":{"docs":{},"s":{"docs":{},"u":{"docs":{},"m":{"docs":{},"m":{"docs":{},"a":{"docs":{},"r":{"docs":{},"y":{"docs":{},".":{"docs":{},"y":{"docs":{},"m":{"docs":{},"l":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"/":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"/":{"docs":{},"'":{"docs":{},")":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}},"g":{"docs":{},"'":{"docs":{},")":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}},"=":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"=":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}},"~":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}},"[":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.006342494714587738}},"$":{"docs":{},"h":{"1":{"docs":{},"_":{"docs":{},"t":{"docs":{},"i":{"docs":{},"t":{"docs":{},"l":{"docs":{},"e":{"docs":{},"]":{"docs":{},"(":{"docs":{},"$":{"docs":{},"d":{"docs":{},"i":{"docs":{},"r":{"docs":{},"_":{"docs":{},"p":{"docs":{},"a":{"docs":{},"t":{"docs":{},"h":{"docs":{},"/":{"docs":{},"$":{"docs":{},"f":{"docs":{},"i":{"docs":{},"l":{"docs":{},"e":{"docs":{},"_":{"docs":{},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},")":{"docs":{},"\"":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}},"docs":{}}},"[":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}}},"b":{"docs":{},"a":{"docs":{},"c":{"docs":{},"k":{"docs":{},"e":{"docs":{},"n":{"docs":{},"d":{"docs":{},"]":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}},"c":{"docs":{},"u":{"docs":{},"r":{"docs":{},"r":{"docs":{},"e":{"docs":{},"n":{"docs":{},"t":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}},"f":{"docs":{},"r":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{},"w":{"docs":{},"o":{"docs":{},"r":{"docs":{},"k":{"docs":{},"]":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}}}}}},"n":{"docs":{},"a":{"docs":{},"m":{"docs":{},"e":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}},"]":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}}}}}},"p":{"docs":{},"r":{"docs":{},"o":{"docs":{},"v":{"docs":{},"i":{"docs":{},"d":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"s":{"docs":{},"c":{"docs":{},"o":{"docs":{},"r":{"docs":{},"e":{"docs":{},"]":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.010180995475113122}}}}}}},"i":{"docs":{},"z":{"docs":{},"e":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}},"y":{"docs":{},"s":{"docs":{},"t":{"docs":{},"e":{"docs":{},"m":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"t":{"docs":{},"a":{"docs":{},"s":{"docs":{},"k":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.006787330316742082}}}}}}},"]":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}},";":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}}},"]":{"docs":{},";":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.004228329809725159}}}}},"^":{"docs":{},"[":{"0":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0021141649048625794}}},"docs":{}}},"{":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.009513742071881607}}},"|":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.010570824524312896},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0022624434389140274}}},"}":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.007399577167019027}},"}":{"docs":{"08_prompts/08_02_example_create_shell_script.html":{"ref":"08_prompts/08_02_example_create_shell_script.html","tf":0.0010570824524312897}}}},"×":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}},"“":{"docs":{},"f":{"docs":{},"o":{"docs":{},"r":{"docs":{},"k":{"docs":{},"”":{"docs":{"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"ref":"10_benchmarks/10_01_geekbench_ai_benchmark.html","tf":0.0011312217194570137}}}}}}}},"x":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.002898550724637681}},"d":{"docs":{},"r":{"docs":{"10_benchmarks/10_03_m3_max.html":{"ref":"10_benchmarks/10_03_m3_max.html","tf":0.0028169014084507044}}}}},"–":{"docs":{"10_benchmarks/10_02_ryzen_7_5800_32.html":{"ref":"10_benchmarks/10_02_ryzen_7_5800_32.html","tf":0.011594202898550725},"10_benchmarks/10_04_nuc9v7qnx.html":{"ref":"10_benchmarks/10_04_nuc9v7qnx.html","tf":0.00906344410876133}}},"`":{"docs":{},"o":{"docs":{},"n":{"docs":{"11_additional_resources/11_02_articles.html":{"ref":"11_additional_resources/11_02_articles.html","tf":0.003952569169960474}}}}}},"length":6910},"corpusTokens":["!=","\"","\"\"","\"#","\"$(basenam","\"$chapter_name\"","\"$chapter_name\")","\"$dir_path\"","\"$dir_path\"/*;","\"$entry\"","\"$entry\")","\"$entry\")\"","\"$entry\")\")","\"$file_path\"","\"$h1_title\"","\"$input\"","\"${indent}","\"${indent}*","\".\"","\"bigger\"","\"can","\"deeper.\"","\"default","\"full","\"gpu","\"instruct\"","\"iq\"","\"like","\"out","\"remov","\"updat","\"wider\"","\"you","#","#!/bin/bash","$(basenam","$1","$1,169.51","$1,396.96","$100","$119.99","$129.99","$179.92","$2,999","$217.99","$262.36","$324.99","$349.99","$369.98","$653.24","$699","$73.99","$814.96","$chapter_name\"","$indent\"","${{","&","&&","'","'#","'^#","'github","'main'","'s/^#","'s/^[0","'s/_/","'{for(i=1;i","(13b","(14","(16","(2","(2x32gb)","(32","(4","(64","(8","(a","(approx.)","(b):","(best","(bidirect","(central","(core","(dure","(e.g.,","(gener","(graphic","(in","(intellig","(ips)","(ips).","(k):","(larg","(left","(llms)","(llms),","(llms).","(macbook","(macos,","(medium)","(n_ctx)","(n_ctx):","(n_gpu_layers)","(n_gpu_layers):","(n_predict)","(n_predict):","(n_threads)","(n_threads):","(nlp)","(nuc9i7qn","(onnx","(openvino","(or","(ptq):","(q):","(qat):","(r&d),","(ram)","(such","(the","(todo:","(units)","(usest","(vram),","(vram).","(wa","(weight","(where","(word","(~16","(~32","*","*.md","+","+x","./generate_summary.sh",".github,",".github/workflows/update_summary.yml","/","//')","/g')","1","1,370","1.","1.37","1.38","10,609","10.6","100","100m","1024","109.1","10b","10gb","114.8","116.6","12,570","120.1","125.1","12gb","12gb,","13","13,112","13,358","13,410","13,411","13,896","13.4","13b","13b,","15,","15,148","15.2","150.6","16","16,","16.2","16gb","16gb)","17.1","175.9","180.6","181.3","19,","19,608","19.3","191.7","196.6","1:","1b","1tb","2","2,","2,170","2,283","2,353","2,457","2.","2.17","2019","2022)","2022.","2024,","2024.","2024:","203.2","2048","205.0","21,260","236.0","24","240.1","25,495","261.7","277.4","28","299.5","2:","2tb","3","3,","3,313","3,434","3,490","3,582","3,841","3.","3.1","30","313.7","32","32.6","3200mhz","32gb","34","34.4","34b","34b,","34b.","35","35.86","36.0","3600","36gb","37,950","37.9","380.2","3:","3d","4","4,472","4,812","4.","40","4060","4060'","4070","4090","42.2","432.3","44.5","460.6","476.0","481.6","491.9","4:","5,019","5,356","5.","50","512","53.4","557.2","5800x","5800x'","5800x,","592","6,110","6.","6.11","64","64gb","7","7,593","70b","70b,","712.5","728.1","75.0","75.5","755.7","7b","7b,","8","81.0","817.9","83.6","863.3","874.2","880.8","884","8b","8gb","8gb,","9.57","9750h","9850h","9850h'","9]\\{2\\}_//')","9]{2}_","9]{2}_//;","9th","=","==","=~",">","[","[$h1_title]($dir_path/$file_name)\"","[[","[backend]","[current","[framework]","[name","[name]","[provid","[score]","[size","[system","[task","\\","]","];","]];","^[0","_","`on","a100,","aaditya","abbrevi","abil","abstract","abund","acceler","accelerated.","acceleration.","accept","access","access,","accessible,","accommod","accord","account","accur","accuraci","accuracy,","accuracy.","accuracy:","accurate,","achiev","achieve.","action","actions/checkout","actions/checkout@v2","actions[bot]'","actions[bot]@users.noreply.github.com'","activ","actual","ad","adapt","adaptability.","add","add,","addit","additions,","address","adequ","adequaci","adher","adjust","adjust!","advanc","advantag","advantages,","affect","afford","afforded,","against","agents,","ai","ai)","ai,","ai.","aifor.dev","aim","air","airflow","algorithm","align","all,","allan","allevi","alloc","allocation:","allow","allud","alon","along","alreadi","altern","alway","amaz","ambigu","amd","amid","amount","ampl","analysi","analysis,","analysis.","analyz","and,","animation.","annot","announc","anoth","another.","answer","anyon","anyth","api","app","app.","appl","apple'","apple’","appli","applic","application,","applications,","applications.","applied,","approach","appropri","approxim","apps,","app’","ar.","ar/vr.","architectur","architecture,","area","area,","arguments:","arise:","around","around.","art","articl","artifici","artist","aside,","ask","aspect","assess","assist","assistance,","assistance.","assistant.","assistants,","associ","assur","asu","asymmetr","attempt","attent","attribut","audienc","august","authent","auto","autom","automatically.","autonom","avail","availability.","availability:","available,","available.","avoid","awar","awk","b550","back","backend","backend,","backend.","balanc","balance,","bandwidth.","barrier","base","basic","batteri","be","be,","becom","befor","behind","below","below.","ben","benchmark","benchmarking:","benchmarks.","benchmarksdownload","benefici","benefit","benefits.","bert","best","better","better,","better.","between","bhat:","bias","biases)","biases),","bidirect","bigcod","bigger","bigger,","billion","billions,","bit","bit).","block","boost","both","bottleneck","bottleneck,","bound","boundari","branch","branch.","branches:","break","breakdown","bridg","brief","broad","broader","broadli","budget.","build","building,","built","bulk","burden","button","button.","calcul","calculations.","call","callout:","can't","candid","can’t","cap.","capabilities,","capabilities.","capabl","capac","capacity,","capacity.","capit","captur","care","carefulli","carl","case","case,","case.","case:","cases,","cases.","cases:","categor","categori","categories:","cater","caus","cautiou","ceas","centric","certain","chain","challeng","challenges.","challenges:","chang","changed.","changer","changes,","changes.","changes:","channel","channels.","chapter","chapter_name=\"$1\"","chapter_name=$(clean_chapter_nam","chapter_name=$(echo","chapter_name=$(to_title_cas","characterist","characteristics,","chart","chatbots,","chatgpt","chatgpt,","cheat","check","checkout","chip,","chmod","choic","choice.","choos","choose.","chosen","ci","circular","cl22","clarity.","clarity:","classif","classifi","classification,","classification:","clean","clean_chapter_nam","clean_chapter_name()","cleaner","cleaning:","clear","clear.","click","clock","clone","close","closely.","closer","cloud","cobbl","code","code,","code.","code.\"","code:","codellama","codellama,","coding,","coding.","coher","collabor","collaboration.","collect","combin","combination,","come","comfort","command","command:","commands,","comment","comments.","commit","commit.","common","commun","communication.","communities.","community.","community:","compact","compani","compar","comparison","comparison:","compat","compatibility,","compatibility:","compel","compens","compet","complement","complet","complete,","complete.","completion,","completions,","complex","complex,","complexities,","complexity,","complexity:","compon","components,","components.","comprehend","comprehens","compromis","comput","computation","computations,","computations.","computer'","computer,","computer.","computer:","computer’","computing,","concept","conclus","conclusion:","concurrently,","confid","config","configur","configuration,","configurations,","configurations.","configure,","connect","cons:","consid","consider","consider:","considerations:","consist","constant","constantli","constrain","constraint","constraint.","constraints,","constraints.","consum","consumpt","consumption,","consumption:","contain","contains,","content","content.","content:","contents\"","contents,","context","context,","context.","contexts,","contextu","continu","continue:","contribut","contribute.","contributions!","contributor","control","control,","convent","conventions.","convers","conversely,","convert","cool","cooler","cooling,","cooling:","copi","copilot","core","cores)","cores,","cores.","corpu","correct","correct.md","correctli","correctly,","correctly.","correctly.\"","correctness:","correl","corsair","cost","cost.","costly.","costs:","count","counterparts,","cover","cover,","cpp","cpu","cpu)","cpu,","cpu.","cpu/gpu","cpu:","cpus,","cpus.","crash","crashes:","creat","creation","creation,","creation.","creativ","creativity.","critic","critical,","critical.","cross","crucial","crucial.","crutch.","cuenca:","current","custom","cycl","d","data","data).","data,","data.","data:","dataset","datasets,","datasets.","date","day","ddr4","deal","dean","debug","debug,","decent","decid","decis","decision.","decod","decreas","dedic","deep","deeper","default","defin","degrad","degradation,","degre","delet","deliv","delv","demand","demands,","demands.","demonstr","demystifi","denot","depend","deploy","deployment,","depth","describ","descript","description:","descriptions.","design","desktop","despit","detail","detail,","detail.","detailed,","detect","detection,","detection.","detection:","determin","dev","dev.git","develop","developers,","developers.","developer’","development,","development.","devic","devices,","devices.","dickson:","differ","dig","dimension","diminish","dir_path=\"$1\"","direct","directli","directly,","directml","directml)","directori","directories.","directories:","directory.\"","discord","discov","discuss","discussions:","disk","display","display:","disposal.","distanc","distinct","distinguish","distribut","divers","divis","do","document","documentation,","doesn't","doesn’t","dollar,","domains,","done","don’t","doubl","down","down,","download","download:","downsid","dozen","drastic","drive","driving.","due","dummi","dure","dynam","e","each","eas","easi","easier","easili","echo","econom","ecosystem.","edg","edit","editing,","editor","effect","effective.","effectively,","effectively.","effectively:","effectively?\"","effectiveness.","effici","efficiency,","efficiency.","efficiency:","efficient,","efficiently,","efficiently.","effort,","element","elif","elimin","email","embed","embeddings:","empti","enabl","encod","encount","encourag","end","end,","endpoints.","energi","engin","engines,","enhanc","enjoy","enough","ensur","entails.","entir","entirely,","entri","env:","environ","environment,","environment.","environments,","environments.","equal.","equip","error","error.","errors.","especi","essenti","essential.","establish","estim","estimation,","estimation.","estimation:","ethic","evalu","evaluation.","evaluator,","even","everyth","evolv","evolve,","evolve.","evolves.","exampl","example,","example.","example:","exce","exceed","excel","except","excess","excit","exclus","execut","executed,","execution.","exist","expand","expandability:","expansion:","expansions.","expect","expectations.","expected.","expecting.","expensive.","experi","experienc","experiments,","explain","explan","explor","extend","extens","extension:","extern","extra","extract","extract_h1_title()","extrem","ey","f","face","face'","face’","facial","facilit","factor","factors,","fail","fair","fall","famili","familiar","family,","far","fast","faster","faster,","feasibl","featur","features.","features:","fee","feed","feeds:","feel","few","fewer","fi","fiction.","field","figur","file","file'","file,","file:","file?\"","file_name=$(basenam","file_path=\"$1\"","files:","fill","filter","final","finally,","find","findings,","fine","first","first,","firstly,","fit","fix","flesh","flexibl","flight","float","fluenci","focu","focus","folder","folder'","folder.","follow","following,","following:","follows:","footprint","footprint,","footprints,","forget","fork","form","format","format.","formula","formula:","forward","found","foundation,","founder","four","framework","framework)","framework)**","framework,","framework.","franzen:","fraser:","free","free,","frequent","friendli","fronts:","full","fullest","fulli","function","function:","functionality.","functionality:","fundament","further","further,","furthermore,","futur","g","gain","gains,","game","gap","gaug","gb","gb):","gb+","gb,","gb.","gddr6","gddr6x","geekbench","geforc","genai","gener","generate,","generate_summary.sh","generation,","generation.","get","gguf","git","git:","gitbook","github","github.","github_token","githubfirst,","give","given","global","go","goal","good","googl","google,","gpt","gpt4all","gpt4all:","gpu","gpu'","gpu,","gpu.","gpu:","gpus,","gpus.","gpu—i","gpu’","grade","gradual","grant","great","greater","greatest","greatli","grep","group","grow","grow.","growth","guid","guide!","guide'","guide.","guidelin","guides.","h1","h1_title=$(extract_h1_titl","h1_title=$(grep","half","hand","hand,","handl","handle,","handle.","happi","har","hard","harder.","hardwar","hardware'","hardware,","hardware.","hardware:","hardware—specif","hardware’","has,","hate","have","headroom","heavi","heavili","help","here","here'","here,","here.","here’","high","higher","highest","highli","highlight","hit","home","home.","homepag","hood,","hook","hooks,","hope","hot","hour","however,","https://github.com/jessefreeman/ai","https://huggingface.co/:","https://jessefreeman.substack.com/.","hug","human","hundr","i'll","i'v","i7","id","idea","ideal","identifi","identification.","ignor","ignore,","imag","imagen","images,","imagin","immedi","impact","implement","implement,","implementations.","implemented,","implic","implications,","import","importance,","important.","important:","imposs","impract","impress","improv","improvement.","inch","includ","inclus","inclusion.","incomplet","inconsist","incorpor","increas","increasingli","incred","indent=\"$2\"","indic","industri","industry.","infer","inferenc","inference,","inference.","inference:","inferencing),","inferencing,","inferencing.","inferencing:","inferencing?","influenc","inform","information,","initi","initiative,","inner","inord","inpainting,","input","input.","input=\"$1\"","inputs,","inputs.","insight","insights,","instability.","instal","installer.","instance,","instant","instruct","instruct,","instruct.","instruct.q2_k.gguf","instruct.q3_k_m.gguf","instruct.q3_k_m.gguf,","instruct.q3_k_s.gguf","instruct.q4_k_m.gguf","instruct.q5_k_m.gguf","instructions,","instructions.","instruct—tailor","instruct’","insuffici","integer).","integr","intel","intelligence,","intens","intensive.","intent.","interact","interactions.","interest","interest,","interfac","interface,","interface.","intermedi","internet","interpret","intric","introduc","introduct","intuit","invalu","invest","invested.","investment.","invit","involv","ip","iq","iq1","iq2.","iq2_m","is).","is.","isn’t","issu","issues,","it'","it,","it.","it:","iter","iteration,","ithub_token:","it’","i’d","i’ll","i’m","i’v","javascript","jobs:","join","joke","journey.","k_","k_l","k_l:","k_m","k_m:","k_s:","keep","key","kind","kip","kips)","kit","kit)","know","knowledg","knowledge.","known","label","labor","lack","lam:","lane","languag","language.","languages,","languages.","laptop","larg","large,","larger","larger,","largest","lastly,","later","latest","launch","layer","layer.","layer:","layers,","layers:","lead","learn","leav","length","length,","lerobot","less","let'","letter","level","level,","level.","level:","levels.","leverag","li","librari","life","light","lightweight","limit","limit,","limit.","limitations,","limits,","limits.","limits:","line","line,","linear","links.","linux).","linux.","liquid","list","list:","list_structur","list_structure()","llama","llama,","llama:","llm","llm.","llm:","llms'","llms,","llms.","llms:","llms?","llm’","lm","load","load,","load.","loaded,","loaded.","loading:","local","locally,","locally.","locally?","logic","logic,","logic.","long","longer","look","loos","lose","lot","love","low","lower","lowercas","lpx","luckili","m","m1","m3","mac","mac,","macbook","machin","machine,","machine.","machine.\"","machine:","machines.","maco","macos\"","macos,","macos.\"","macs,","macs.","macs:","mac’","made","mail","main","maintain","maintain,","major","make","manag","management.","management:","mani","manipul","manipulation,","manipulation.","manual","manually.","markdown","markdown.","match","mathemat","matrix","matter","max","max'","max,","max.","maxim","maximum","mayb","me,","me.","mean","meaning","meaning,","meant","measur","medic","medium","medium,","meet","member","memori","memory\"","memory)","memory,","memory.","memoryd","menu","menu,","mess","messag","message,","meta","meta'","meta,","meta’","method","metric","metrics.","michael","mid","midjourney","mind","mind,","mind:","minim","minimum","minutes.","mirror","mix","ml","ml,","ml.","mode","model","model'","model,","model.","model:","model?","modeling,","models'","models,","models.","models:","model’","moder","modern","modest","modifi","monitor","more","more)","more,","more.","moreover,","motherboard","motherboard:","motion","move","msi","much","multi","multilingu","multipl","multiplay","multiplications,","multitask","my,","na","name","name.","name.\"","name:","name]","names:","names?","nas,","nas:","natur","navig","near","necess","necessari","necessarili","necessary.","necessit","need","needs,","needs.","needs:","needs—wheth","neg","network","network,","network.","networks:","network—that","neural","neuron","never","new","newer","next","next,","nich","nomic","non","normal","notabl","note","note,","notes,","notes.","notic","novel","now","now,","now.","now.\"","nuanc","nuc","nuc9v7qnx","nuc9v7qnx,","number","numer","nuñez:","nvidia","nvme","object","observ","occupi","occupies,","off","offer","offic","offici","offlin","offline,","offload","offload\"","offload,","offloaded.","offloading,","offloading.","offloading:","offs:","older","on","on.","on:","onc","once,","once.","one,","one.","ones,","ones.","ongo","onlin","online.","only,","onnx","open","openai,","openai’","openvino","oper","operation.","operations,","operations.","opportun","opportunity.","opt","optim","optimally,","optimization.","optimization:","optimizations,","optimize:","option","option.","options,","options.","option—it’","order","organ","organization.","origin","other","other's,","others.","otherwis","out","outlin","outpainting,","output","output.","outputs,","outputs.","over","overal","overfit","overfit,","overfitting:","overhead","overhead.","overload","overview","overview**","overwhelm","overwhelming.","page.","pain","pair","paradigms,","parallel","paralleliz","paramet","parameters)","parameters,","parameters.","parameters—th","part","part,","partial","particular","particularli","partner!","pass","past","path","pattern","pay","pc","pcie","pedro","peopl","per","perfect","perform","performance,","performance.","performance:","performs,","permiss","person","perspect","phase.","phind","photo","photographi","physic","pick","pictur","place","plain","plan","planning,","platform","platforms.","pleas","point","point)","point,","points.","polyglot","poor","poorli","portion","pose","posit","possibl","possible,","possible,\"","possible.","possible.\"","possible:","post","potenti","potential.","power","power,","power.","power:","practic","practices,","practices.","pre","precis","precision,","precision.","precision:","predict","predictions.","preferences,","prefix","prefixes,","prepar","preprocessing,","preset","preset,","preset:","presets,","prevent","previou","price","price)","price*:","price.","price:","price]","primari","priorit","prioriti","privaci","privacy,","pro","pro'","problem","problems.","process","process,","process.","processing,","processing.","produc","product","productivity,","program","programming,","project","project'","project,","project.","projects,","projects.","project’","prompt","prompt)","prompt,","prompt.","prompt?","prompts,","prompts.","proper","properly.","properti","pros:","protect","provi","provid","psu","ptq.","publicli","pull","purchas","purpos","purpose,","push","push.","push:","put","python","python,","q2","q2)","q2:","q3","q3:","q3_k_l","q4","q4,","q4:","q4_k_","q4_k_m","q5","q5)","q5:","q5_k_","q5_k_m","q6_k","q6_k_","q6_k_m","q8_0","qat","qualiti","quantiti","quantiz","quantization)","quantization,","quantization.","quantization:","quantized,","quantized.","quantized:","question","quick","quick,","quickli","quickly.","quietli","r","r,","ram","ram),","ram).","ram,","ram.","ram:","randomli","rang","range.","rank","rapid","ratio","re","react","react.","reaction","read","readabl","readi","reading,","readm","real","realiz","realli","reasons.","rebuild","recognit","recognition.","recommend","recur","recurs","recursively:","recursively?\"","reduc","reduced,","reduct","reduction.","refactor","refer","refin","refine:","reflect","regard","regardless","regex","regex.","regular","rel","relat","relationship","releas","relev","reli","reliabl","reliable,","relianc","reliev","remain","rememb","remot","remov","repetit","reports.","repositori","repository,","repository.","repository:","repositoryclon","repres","represent","representations,","representations:","request","requir","required,","requirements,","requirements.","requirements:","research","research,","resolut","resolution,","resolution:","resourc","resources,","resources.","resources:","respond","respons","response,","response.","responses,","responses.","rest","result","results,","results.","resultscollect","retain","retina","retraining.","return","returns:","revert","review","revis","right","right).","rigor","risk","robot","robot:","robots,","robust","robust,","rog","root","ropes.","routin","rss","rtx","run","run.","run:","runtim","ryan","ryzen","r’","s/_/","sacrif","sacrific","same","samsung","save","savings,","saw","scalability:","scalabl","scale","scan","scenario","scenarios,","scenarios.","scene,","scenes,","scheme","scheme:","schemes,","scienc","scientist","scope","score","scores,","scratch.","screen","script","script:","seamless","seamlessli","seamlessly.","search","secrets.github_token","section","section,","sections,","secur","security,","security.","sed","see","see,","segment","segmentation:","select","selection,","self","sensit","sentenc","sentiment","seri","series.","seriou","serv","server","servers.","servic","services,","services.","set","settings.","settings:","setup","setup,","setup.","sever","share","sharing.","shaw","sheer","sheet","shell","shift","short","shorten","shorter","show","showcas","side","side.","signific","significantli","significantly.","silicon","silicon,","silicon.","similar","simon","simpl","simple,","simpler","simpli","simplifi","simul","simultan","simultaneously,","simultaneously.","singl","size","size,","size.","size:","sizes:","skills,","slight","slightli","slot","slow","slowdown","slowdown:","slowdowns.","slower","small","small,","smaller","smaller,","smarter,","smi","smooth","smoothli","snake","snippet","snippets,","snippets.","softwar","sole","solid","solut","solution.","solv","solving.","some,","someon","someth","somewher","sophist","sourc","space","space.","spam","span","spare.","spec","special","specialization:","specif","specifi","specifications,","specs,","specs:","speed","speed,","speed.","speed:","speeds.","spend","spend.","spent","spent,","split","spread","ssd","ssd:","stabil","stand","standard","standards.","standout","starcod","starcoder’","stare","start","start.","state","state,","statement","static","stay","step","step.","steps:","steps—understand","stewart:","still","storag","storage,","storage.","store","straightforward","straightforward.","strain.","strategi","strategies,","streamlin","strength","strengths,","strengths.","strengths:","strict","strike","string","strix","strong","structur","structure,","structure.","struggl","studio","studio,","studio.","studio:","style","submiss","submissions.","submit","subscript","substanti","such","suffic","suffice,","suffici","sufficient,","suffix","suggest","suggest.","suggestions,","suit","suitabl","summar","summari","summary.md","summary.md\"","summary:","super","super.","superior","suppli","supply:","support","support,","suppos","sure","surpris","sustain","swap","swift","switch","symmetr","syntax","system","system'","system,","system.","system:","systemat","systems,","systems.","system—can","system’","tabl","tailor","takahashi:","take","taken","talebi:","talk","target","task","task.","tasks,","tasks.","taught","teach","technic","techniqu","techniques,","technolog","templat","template,","temporari","ten","tensor","tensor.","term","termin","terms,","test","tested,","testing—y","tests,","tests.","tests.]","tests?","text","thank","that,","that’","thebloke/codellama","them.","there,","therebi","therefore,","there’","they'r","thing","this,","this:","thorough","thoroughli","those","though","thought","thousand","thread","three","thrive.","through","tier","tier.","time","time,","time.","time:","times,","times.","times:","tinker,","tip","tips,","titl","title:","titles:","to,","to_title_case()","toc","toc.","today’","todo:","togeth","together,","token","tool","tool.","tools,","top","topic,","total","tracking,","trade","traditionally,","train","trainabl","trained,","training,","training.","training:","transfer","transfer,","transfer.","transfer:","transform","transformations,","transformer)","transformers)","transit","translat","translation,","translation:","travers","tri","trial","trigger","try.","tune","turn","turn,","tutori","type","type,","type]","types,","types.","typic","u.s.","ubuntu","under","underscor","understand","understand,","understanding,","understanding.","unifi","uniqu","unit","units)","units).","unleash","unless","unlik","unlock","unseen","unsur","unveil","up","up),","up:","updat","upgrad","upgrades.","upgrades:","upon","upon.","upper","us","usabl","usag","usage,","usage.","usage:","usage—th","use,","use.","used,","used.","useeffect)","user","user.email","user.nam","usernam","users,","uses:","util","utility.","utilization:","v1.q4_k_s.gguf","v1.q4_k_s.gguf:","v1.q5_k_m.gguf","v1.q6_k_s.gguf","v1.q6_k_s.gguf:","v1.q8_0.gguf","v1.q8_0.gguf,","vagu","validation,","valu","valuabl","values,","values.","vari","variant","variant.","variants—base,","variat","variations,","varieti","variou","vary.","vast","vector","vehicles,","vengeanc","veri","verifi","versatil","versatile,","version","via","video,","videos,","view","vinc","virtual","visual","vital","volum","vram","vram,","vram.","vram]","vs","vs.","walk","want","warn","wasn't","wattag","way","way,","we'll","we'r","weak","web","website.","weight","welcom","well","we’ll","whether","which,","who'","wide","widen","wider","wil","window","windows,","with.","within","without","witt:","wizardcod","word","words)","words,","work","work,","work.","worked,","workflow","workflow,","workflow.","workflows.","workload","workloads,","workloads.","works.","works:","workspac","workspace.","world","worldwide.","worri","worth","writ","write","writing.","written,","wrong","x","xdr","year","yes,","you'll","you'r","you.","yourself","you’d","you’ll","you’r","you’v","{","|","}","}}","~$1,169.51","~$2,999","~$814.96","~1.17","~10","~14","~168.1","~18","~185.6","~203.7","~22","~23.84","~24","~27.3","~28","~35","~4","~4.57","~40","~5.73","~6","~7","~8","×","–","“fork”"],"pipeline":["stopWordFilter","stemmer"]},"store":{"./":{"url":"./","title":"Introduction","keywords":"","body":"AI For Dev\nWelcome to the AI For Dev guide! This constantly evolving guide is designed to help developers set up and optimize local Large Language Models (LLMs) for help with coding. Whether you're looking to leverage the power of AI to enhance your productivity, are still learning how to code, or want to gain a deeper understanding of how these models work, this guide is a collection of findings, experiments, and observations to help you in your journey.\nYou can explore the GitBook version of this guide at aifor.dev or use this Table of Contents to explore the guide's markdown on GitHub.\nHow to Contribute\nI invite anyone who's LLMs online or locally to help with coding to contribute and help grow and improve this guide. Whether you have new model test results, best practices, or other valuable insights, your contributions can help others as we figure out how to leverage AI in our development workflows. Please refer to the contribution guidelines for detailed instructions on how to contribute.\nJoin the Community\nStay updated with the latest additions, tips, and more by following me, staring this project, and joining our community:\n\nMailing List: Follow my mailing list for updates and insights at https://jessefreeman.substack.com/.\nGitHub Discussions: Join the conversation and share your thoughts on our GitHub Discussions page.\n\nThank you for being part of the AI For Dev community. Together, we're building a valuable resource for developers worldwide. Your interest, support, and contributions make this project thrive.\n"},"01_introduction/01_01_how_to_use_this_guide.html":{"url":"01_introduction/01_01_how_to_use_this_guide.html","title":"How To Use This Guide","keywords":"","body":"How To Use This Guide\nI’ve been using ChatGPT and other Large Language Models (LLMs) to help me code for well over a year now. For the most part, it’s amazing and has helped me turn a lot of my ideas into working projects. However, there are also times when something gets updated behind the scenes, and ChatGPT loses a few IQ points. As someone who loves to tinker, I figured I’d give running local LLMs a try. Some of the advantages are allowing me to use multiple models at once, leveraging my computer's own hardware, and have full control of my development environment.\nThis guide is designed to share my, and other's, understanding of how to set up and optimize local LLMs, solely for developers looking for help with coding. There’s a lot to cover, so I’m going to focus on breaking down everything you need to know in order to pick the right model, understanding your computer's capabilities, and benchmarking the models I use to ensure you can get the best performance possible. This guide was created from my personal notes and learnings over the past few years of coding with an LLM at my side. I’ll show you how to not only leverage LLM’s ability to generate code but also learn when and how to use it so you can scale your productivity and some tips on how to avoid relying on it like a crutch.\nI hope you enjoy this guide and please feel free to contribute if you have something to add or see something that needs that needs to be changed.\n"},"01_introduction/01_02_why_run_models_locally.html":{"url":"01_introduction/01_02_why_run_models_locally.html","title":"Why Run Large Language Models Locally?","keywords":"","body":"Why Run Large Language Models Locally?\nRunning large language models (LLMs) locally on your computer offers several compelling advantages for developers, such as control, cost-efficiency, and a deeper understanding of how these models work. Unlike relying on online services like OpenAI’s ChatGPT, local deployment allows you to leverage your existing hardware, maintain complete control over your data, and demystify the inner workings of how these things work.\nOne of the primary benefits of running LLMs locally is the complete control it provides over your data, which, in turn, ensures your privacy and security. By keeping all your information on your local machine, you can rest assured that your sensitive projects are protected and are not being used by companies to train their own models. This control extends to the ability to work offline, making it ideal for scenarios where a constant internet connection isn’t possible, like a long flight or staying somewhere that has limited bandwidth. This offline capability ensures continuous accessibility without the chain that comes with always being online.\nUtilizing your existing hardware to run LLMs also offers significant cost-efficiency. By leveraging your computing power, you can avoid the recurring subscription fees associated with cloud-based services. This is especially beneficial for long-term projects or continuous development, making it a more economical choice in the long run. The ability to use and optimize your hardware ensures that you get the most out of your investment. As you’ll see later one, I’ve been cobbling together computers from parts or older computers to build dedicated machines to run my LLMs on my own local network and to access remotely when I’m not at home.\nFurthermore, running LLMs locally provides a valuable learning opportunity. It offers an opportunity to demystify how these models work, offering a deeper understanding of their functionality and capabilities. This hands-on experience enables fine-tuning and configuration adjustments to better suit your specific needs and be customized around your own hardware’s capabilities, leading to improved performance and more tailored results. For developers, this can significantly speed up your development, as locally running LLMs can generate code, debug, and offer real-time suggestions without the dependency on external servers. You can also use them to help document code and create tutorials to teach others. \nWhen leveraged correctly, these models can help you scale your output and improve the quality of your code and documentation, opening up exciting opportunities for growth and improvement. There isn’t a day I don’t code without having an LLM by my side in one form or another. There is no better coding partner!\n"},"02_understanding_large_language_models/02_01_overview_of_llms.html":{"url":"02_understanding_large_language_models/02_01_overview_of_llms.html","title":"Overview of Large Language Models (LLMs)","keywords":"","body":"Overview of Large Language Models\nLarge Language Models (LLMs) represent a significant advancement in artificial intelligence, transforming how we interact with technology and manage complex tasks. When I joined Samsung in 2019 to do AI, LLMs like today’s version of ChatGPT were still science fiction. Fast forward to now and I can't imagine writing or coding without one. \nAt a high level, LLMs are sophisticated AI systems trained on vast amounts of text data to understand, generate, and manipulate human language. Their ability to comprehend and produce text with a high degree of fluency and accuracy makes them powerful tools for a wide range of applications but can be especially beneficial as you’ll see later on in this guide.\nWhat Are LLMs?\nLLMs are built using deep learning techniques, specifically leveraging transformer architecture (TODO: NEED TO EXPLAIN WHAT THIS IS). \nThese models are pre-trained on diverse and extensive datasets, which enables them to understand context, infer meaning, and generate coherent and contextually appropriate text based on given prompts. The level of interaction you can get out of these LLMs when using them for coding never ceases to impress me. \nHere are some common types of LLMs:\n\n\n\nModel Type\nDescription\nUse Cases\n\n\n\n\nGPT (Generative Pre-trained Transformer)\nDeveloped by OpenAI, GPT models are designed to generate human-like text based on input prompts. They are known for their versatility and ability to handle a wide range of natural language processing tasks.\nContent creation, conversational agents, text completion, and more.\n\n\nBERT (Bidirectional Encoder Representations from Transformers)\nDeveloped by Google, BERT models are designed to understand the context of words in a sentence by looking at both directions (left and right). This bidirectional approach helps in tasks that require understanding context, such as question answering and sentiment analysis.\nSearch engines, sentiment analysis, question answering systems, and more.\n\n\nLLaMA (Large Language Model Meta AI)\nDeveloped by Meta, LLaMA models are designed to be more efficient and accessible, providing high performance on language tasks with relatively fewer computational resources. They are suitable for both research and practical applications.\nResearch in language modeling, practical applications in resource-constrained environments.\n\n\n\nHow Developers Can Use LLMs\nDevelopers can leverage LLMs in numerous ways to enhance their applications and workflows. One of the primary uses is in natural language processing (NLP) tasks, where LLMs can help with understanding and generating human language. This includes applications like chatbots, virtual assistants, and automated customer support, where LLMs can provide more natural and accurate interactions. All of this can be leveraged by developers for more than content creation. LLMs can generate new code, find errors in existing code, and even add detailed contextual comments. When used correctly, they can save you a lot of time and effort, so you can focus on the bigger picture while the LLM handles repetitive or time-consuming tasks.\nLLMs are also powerful tools for data analysis and manipulation. They can help extract insights from large datasets, summarize information, and automate the creation of reports. For instance, developers can use LLMs to help clean up data, generate dummy data for testing and create instructional guides. Moreover, LLMs can assist in coding by providing auto-completion, code generation, and debugging assistance. This enhances the productivity of developers by reducing the time spent on routine coding tasks and allowing them to focus on more complex problems. If you don’t remember what something in your code does or want to understand code that someone else has written, just ask the LLM.\nIn research and development (R&D), LLMs offer a valuable ability to help you experiment with new ideas and pushing the boundaries of what you are capable of which is an incredibly powerful tool when you are learning a new language or trying to follow development best practices. Developers can fine-tune these models for specific applications, improving their performance on niche tasks and expanding their utility. Hate writing unit tests? Let the LLM do it for you. You’d be surprised at how they can help you optimize and refactor your code based on your direction or its own knowledge.\nBy integrating LLMs into your workflow, You can harness the power of advanced LLMs to build smarter, more efficient, and more responsive code. This not only improves the end-user experience but also accelerates development cycles when leveraged correctly. But don’t take my word for it, you should give it a try. Most of this guide was generated by an LLM and I cleaned it up to add my own perspective and verify what I’m learning is accurate since this is my first time focusing on running these models locally.\n"},"02_understanding_large_language_models/02_02_model_variations.html":{"url":"02_understanding_large_language_models/02_02_model_variations.html","title":"Models Variations","keywords":"","body":"Models Variations\nGiven LLMs' diverse capabilities, it's crucial to understand how different variations of these models can be tailored to specific tasks. A downside of LLM’s ability to assist in content creation across different needs, it’s circular to find ones that are specialized for coding. For instance, CodeLlama, which is a specialized version of Meta’s Llama, is a good place to start. However, finding the right version isn’t as easy as doing a search in LM Studio, a tool we’ll use later on for running these models locally. You’d be surprised by how many results come back so you’ll need to learn how to pick the right model for your own needs.\n\nAs I alluded to, there are dozens of CodeLlama model variations that are optimized for different aspects of coding, ensuring that you can find one that fits your specific requirements. By leveraging the right models, you can significantly improve your development workflow, from speeding up code completion to handling complex coding tasks. Picking the wrong one wil in turn slow you down, give you poor results, and generally make your life harder.\nIt’s not practical to go into every variation of the CodeLlama model, or the other coding specific models out there, but I’ve put together the following chart to help you understand and review different options, their strengths, and their ideal use cases:\n\n\n\nModel\nStrengths\nWhen to Use\n\n\n\n\nPhind CodeLlama\nOptimized for general coding tasks, providing a balance of precision and performance. Suitable for advanced coding tasks with high resource availability.\nGood for generating code snippets, understanding coding contexts, and completing code.\n\n\nCodeLlama Instruct\nSpecifically designed for following instructions and generating code based on detailed problem statements or high-level descriptions.\nIdeal for tasks where the model needs to understand and generate code from instructions, making it a better fit if your primary interaction involves explaining coding tasks to the LLM.\n\n\nCodeLlama Completion\nOptimized for auto-completing code snippets, fast and efficient for real-time coding environments.\nBest suited for enhancing IDE auto-complete functionality rather than instruction-based coding.\n\n\nCodeLlama WizardCoder\nAdvanced capabilities for complex coding tasks, offering superior performance for sophisticated code generation.\nSuitable for complex coding challenges but may be more resource-intensive than needed for basic instruction-based coding.\n\n\nStarcoder\nVersatile, optimized for multiple programming languages and diverse coding scenarios.\nGood if you work with various languages and need a flexible model, but might not be as specialized for instruction-based coding as CodeLlama Instruct.\n\n\nCohere Command R\nGeneral-purpose model capable of handling a broad range of tasks, including coding.\nSuitable for multi-functional environments where coding is one of many tasks, but not as specialized for instruction-based coding.\n\n\n\nSelecting the right model involves understanding the strengths and specific use cases of each variant. For instance, if your primary need is to generate code from detailed instructions, CodeLlama Instruct is particularly suited for this task. We’ll actually be talking about it a lot in this guide since it's designed to follow instructions and create accurate, coherent code based on detailed prompts or high-level descriptions. This is makes it an ideal candidate for developers looking to leverage local LLM to help improve and speed up our coding.\nAfter you select the correct model, it’s essential to test it to ensure it meets your performance expectations. Running a standardized test will help you evaluate the model's response time, accuracy, and resource usage. This step ensures that the model is well-suited to your hardware’s capabilities and aligns with your specific coding needs. We’ll go into how to test these models later on. For now, it’s important to understand that you can’t just pick a model randomly and hope it works. The more time you spend understanding the model’s requirements and your computer’s limitations, the better its responses will be and the faster it will generate them. Lastly, being able to version control the models you use locally will ensure consistent responses and a way to revert back to a previous model if a newer version doesn’t return the results you are expecting.\n"},"02_understanding_large_language_models/02_03_model_naming_conventions.html":{"url":"02_understanding_large_language_models/02_03_model_naming_conventions.html","title":"Model Naming Conventions","keywords":"","body":"Model Naming Conventions\nDifferent variations of modes like CodeLlama attempt to optimize memory usage, resources requirements, and speed. Choosing the right model depends on your specific needs and the capabilities of your hardware. Since our goal is to have a dependable local LLM that can assist us in generating detailed and accurate code, we’ll be focusing on the CodeLlama Instruct models and over time I’ll add information about new models I test or use. But before you pick the right one, we should discuss how to read and understand each model’s name. \nModel names consist of several abbreviations to help identify all kinds of requirements it will need to run properly. Here's a simplified key to interpret the model names:\nExample: CodeLlama-13b-Instruct.Q3_K_M.gguf\n\n\n\nComponent\nMeaning\n\n\n\n\nCodeLlama\nIndicates the model family optimized for coding tasks.\n\n\n13b\nParameter size: 13 billion parameters. Larger sizes offer better performance and accuracy but require more resources.\n\n\nInstruct\nIn this case, the variant that’s been optimized for instruction-following tasks.\n\n\nQ3\nQuantization level: Q3 indicates moderate quantization, balancing model size and precision.\n\n\nK_M\nQuantization scheme: K_M (Medium) balances resource usage and performance.\n\n\ngguf\nFile format optimized for running on specific frameworks or hardware configurations.\n\n\n\nIf you are new to this, it can feel a bit overwhelming. Choosing the right model involves understanding the various properties that affect performance and resource requirements. Luckily there are only 3 things you really need to understand once you’ve picked a model:\n_ Parameter Size (B): The parameter size represents the number of parameters in billions, such as 34B. Larger parameter sizes generally offer better performance and accuracy because they can capture more intricate patterns and relationships in the data. However, they also require more memory and computational resources, which can be a constraint for systems with limited hardware capabilities. You can learn more about this here.\nQuantization Level (Q): Quantization refers to the precision of the model parameters. Higher quantization levels (e.g., Q4, Q5) indicate larger models sizes and higher precision, resulting in better accuracy but also increased memory and computational power requirements. Conversely, lower quantization levels (e.g., Q2) result in smaller model sizes and faster loading times, but they compromise on accuracy. Therefore, selecting the right quantization level involves balancing the need for precision against the available system resources. You can learn more about this here.\nQuantization Scheme (K): The quantization scheme indicates the specific approach used within a quantization level. These schemes often impact resource usage and performance. For instance, the K_S scheme is designed to be small, using minimal resources, making it suitable for environments with strict resource constraints. The K_M scheme offers a medium balance, providing a good mix of performance and resource efficiency. On the other hand, the K_L scheme is larger and offers better performance but requires more resources, suitable for environments where performance is the top priority and resources are not a constraint. You can learn more about this here \n"},"02_understanding_large_language_models/02_04_parameter_size.html":{"url":"02_understanding_large_language_models/02_04_parameter_size.html","title":"Parameter Size","keywords":"","body":"Parameter Size\nYes, the parameter size does matter when it comes to selecting the right model. The size of the model's parameters directly impacts its performance, memory requirements, and suitability for different hardware configurations. In this section, we'll delve into everything you need to know about parameter size, how it affects model performance, and what to consider when choosing a model to run on your local machine.\nAll jokes aside, the parameter size refers to the number of trainable parameters in a neural network. In the context of LLMs, parameters are the weights and biases of the model that get adjusted during training. Larger models, with more parameters, typically have greater capacity to learn and represent complex patterns but come with higher computational and memory demands. \nHere’s what you need to know about a model’s parameters and how it will impact your ability to use it:\n\nPerformance: Larger models can perform better on complex tasks because they can capture more intricate patterns in the data. However, this performance boost comes at the cost of increased computational requirements.\nMemory Usage: The number of parameters directly correlates with the amount of memory needed to store the model. Larger models require more RAM and, if using a GPU, more VRAM.\nInference Speed: Larger models take longer to process inputs, resulting in slower inference times. This can impact real-time applications like coding assistance where responsiveness is crucial.\nHardware Compatibility: Depending on your hardware, you may be limited in the size of the model you can run. Ensuring that your system can handle the model's memory and computational requirements is essential.\n\nWhen choosing an LLM for local deployment, it's important to consider the trade-offs between parameter size and performance. Here's a general comparison of different parameter sizes:\n\n\n\nModel Size\nParameters (approx.)\nMemory Usage (RAM)\nInference Speed\nUse Case\n\n\n\n\nSmall\n\\\nLow\nFast\nBasic code suggestions, lightweight tasks\n\n\nMedium\n100M - 1B\nModerate\nModerate\nStandard coding assistance, general use\n\n\nLarge\n1B - 10B\nHigh\nSlow\nComplex code generation, detailed analysis\n\n\nExtra Large\n> 10B\nVery High\nVery Slow\nResearch, highly detailed and nuanced tasks\n\n\n\nTODO: Need to flesh this section out more\nPractical Considerations\n\nMemory Availability: Ensure your system has enough RAM to load and run the model. For GPU-based inference, check the VRAM capacity.\nInference Time: Balance the need for responsiveness with the model's capabilities. Larger models may offer better suggestions but can slow down your workflow.\nTask Complexity: Match the model size to the complexity of your coding tasks. For simple code completions, smaller models might suffice, while more complex refactoring might benefit from larger models.\nSystem Resources: Monitor your system's CPU and GPU usage to ensure the model doesn't overwhelm your system, leading to crashes or significant slowdowns.\n\nChoosing the Right Parameter Size\nTo choose the right parameter size for your LLM:\n\nAssess Your Hardware: Know the specifications of your system, including RAM, VRAM, CPU, and GPU capabilities.\nDefine Your Needs: Identify the complexity of tasks you want the model to handle. Simpler tasks can work well with smaller models, while more complex tasks may require larger models.\nExperiment and Optimize: Start with a medium-sized model to gauge performance and then scale up or down based on your needs and system's capabilities.\nBenchmarking: Run benchmarks to see how different models perform on your specific hardware. Measure inference times, memory usage, and the quality of the output to make an informed decision.\n\nConclusion\nUnderstanding parameter size is essential for effectively running LLMs locally. By considering the trade-offs between model size, performance, and hardware requirements, developers can select the most suitable model for their specific needs. This enables efficient and effective use of LLMs to enhance coding productivity without overwhelming system resources.\n"},"02_understanding_large_language_models/02_05_quantization.html":{"url":"02_understanding_large_language_models/02_05_quantization.html","title":"Quantization","keywords":"","body":"Quantization\nQuantization is a crucial concept in optimizing large language models (LLMs) for local use. It involves reducing the precision of model parameters to decrease model size and improve inference speed. This process allows for faster processing and lower memory usage, making it possible to run sophisticated models on a wider range of hardware. Here’s a detailed breakdown of different quantization levels and schemes, along with their implications, using CodeLlama as an example.\nQuantization Levels\nQuantization levels determine the precision of the model parameters. Higher quantization levels offer better performance but require more computational resources. Lower levels reduce resource usage but may sacrifice some accuracy. Understanding these levels helps in choosing the right balance between performance and resource efficiency.\n\nQ2: This level offers lower precision, resulting in smaller model sizes that load quickly. However, the trade-off is in accuracy, making it less suitable for tasks that require high precision.\nQ3: Provides a balanced approach with moderate precision and model size. It balances performance and resource usage, making it a versatile choice for various applications.\nQ4: This level offers better performance with higher precision, but it requires more resources and results in a larger model size. It's suitable for tasks where performance is critical.\nQ5: The highest precision level, providing the best performance. However, it demands the most resources and has the largest model size, making it ideal for high-end systems with ample computational power.\n\nRefer to the table below for a quick comparison:\n\n\n\nQuantization Level\nPrecision\nModel Size\nPerformance\nResource Usage\n\n\n\n\nQ2\nLower\nSmaller\nFaster loading\nLess accurate\n\n\nQ3\nModerate\nModerate\nBalanced\nBalanced\n\n\nQ4\nModerate\nLarger\nBetter performance\nMore resources\n\n\nQ5\nHigher\nLargest\nBest performance\nMost resources\n\n\n\nQuantization Schemes\nQuantization schemes further refine the model's performance and resource usage. These schemes determine how the quantization is applied, optimizing for different aspects such as resource usage and performance.\n\nK_S: This scheme is optimized for minimal resource usage, making it suitable for environments with limited computational power. It prioritizes efficiency over performance.\nK_M: A balanced scheme that offers a compromise between resource usage and performance. It’s ideal for general-purpose applications where both factors are important.\nK_L: This scheme is optimized for better performance, requiring more resources. It's designed for high-performance environments where maximizing model capabilities is crucial.\n\nRefer to the table below for a quick comparison:\n\n\n\nSuffix\nDescription\n\n\n\n\nK_S\nSmall, optimized for minimal resource usage\n\n\nK_M\nMedium, balance between resource usage and performance\n\n\nK_L\nLarge, optimized for better performance but requires more resources\n\n\n\nUnderstanding these quantization levels and schemes allows developers to tailor the model to their specific needs and hardware capabilities. By choosing the appropriate combination, you can optimize the balance between performance, accuracy, and resource usage, ensuring that your LLM setup is both efficient and effective. This knowledge is crucial for developers looking to run LLMs locally to assist with coding tasks, enabling them to select the best model configuration for their specific workflow and hardware setup.\n"},"02_understanding_large_language_models/02_06_quantization_schemes.html":{"url":"02_understanding_large_language_models/02_06_quantization_schemes.html","title":"Quantization Schemes","keywords":"","body":"Quantization Schemes\nQuantization refers to the process of converting a model's parameters from higher precision (e.g., 32-bit floating point) to lower precision (e.g., 8-bit integer). This reduction in precision can significantly reduce the model's size and the computational resources required for inference, making it feasible to run larger models on less powerful hardware.\nQuantization is a crucial technique for optimizing Large Language Models (LLMs) for local use, and understanding quantization schemes is essential for developers looking to maximize performance while minimizing resource usage. Quantization involves reducing the precision of the model parameters, which decreases model size and improves inference speed. Here’s a detailed breakdown of different quantization schemes, their implications, and how they can be applied effectively in environments like LM Studio.\nBenefits of Quantization\n\nReduced Model Size: Quantized models are smaller, making them easier to store and load.\nFaster Inference: Lower precision calculations require fewer computational resources, resulting in faster inference times.\nLower Power Consumption: Reduced computational load translates to lower power consumption, which is beneficial for battery-powered devices and energy-efficient applications.\nResource Efficiency: Allows the deployment of complex models on hardware with limited resources, such as laptops or edge computing devices.\n\nTypes of Quantization\nQuantization can be broadly categorized into several types, each with its own advantages and trade-offs:\n\nPost-Training Quantization (PTQ): This method quantizes a pre-trained model without retraining. It's straightforward but might lead to a slight reduction in accuracy.\nQuantization-Aware Training (QAT): This approach incorporates quantization into the training process. Models trained with QAT typically retain higher accuracy compared to PTQ.\nDynamic Quantization: Only activations are quantized during inference, leaving weights at full precision. This method is less effective in reducing model size but can still speed up inference.\nStatic Quantization: Both weights and activations are quantized, providing the greatest benefits in terms of model size and inference speed reduction.\n\nQuantization Schemes\nQuantization schemes refer to the specific strategies used to apply quantization. They determine how the quantization process is implemented and optimized for different use cases.\n\nSymmetric Quantization:\n\nDescription: Uses the same scale factor for positive and negative values. Simpler and more efficient for hardware implementations.\nPros: Easier to implement, faster inference.\nCons: May lead to reduced accuracy for models with a wide range of parameter values.\n\n\nAsymmetric Quantization:\n\nDescription: Uses different scale factors for positive and negative values, allowing for a more accurate representation of the original model.\nPros: Higher accuracy compared to symmetric quantization.\nCons: More complex to implement and may have slightly higher computational overhead.\n\n\nPer-Tensor Quantization:\n\nDescription: Applies a single scale factor to all elements of a tensor.\nPros: Simpler and faster, good for general use.\nCons: Less precise for tensors with a wide range of values.\n\n\nPer-Channel Quantization:\n\nDescription: Applies different scale factors to each channel of a tensor.\nPros: Higher precision, better for models with varying value ranges across channels.\nCons: More complex and slower compared to per-tensor quantization.\n\n\n\nUnderstanding \"IQ\" Models\nIn addition to standard quantization schemes, you may encounter models labeled with \"IQ\" (Intelligent Quantization) prefixes, such as IQ1 or IQ2. These IQ models take the concept of quantization further by applying advanced optimization techniques during or after the quantization process.\n\nEnhanced Performance: IQ models are designed to retain more of the original model's accuracy and performance, despite the reduced precision. They are particularly beneficial when you need to maintain high performance on tasks that require nuanced understanding, without the full computational overhead of a non-quantized model.\nOptimized Resource Usage: While IQ models may have similar memory footprints to their standard quantized counterparts, they are typically more efficient in handling inferencing tasks. This efficiency can lead to faster processing times and better overall system performance, especially on hardware with limited resources.\nWhen to Use IQ Models: Opt for IQ models if you’re looking to strike the best balance between resource efficiency and model accuracy. They are especially useful in scenarios where precision is critical, such as in detailed coding tasks or complex language understanding.\n\nOptimizing Quantization Schemes\nTo optimize quantization schemes for your LLMs in an environment like LM Studio, consider the following steps:\n\nEvaluate Model Requirements: Determine the acceptable trade-offs between model size, inference speed, and accuracy.\nChoose the Appropriate Scheme: Select a quantization scheme that aligns with your performance and resource requirements.\nImplement and Test in LM Studio: Apply the chosen quantization scheme within LM Studio and rigorously test the model to ensure it meets your accuracy and performance benchmarks.\nIterate and Refine: Based on testing results, adjust the quantization parameters and re-evaluate to achieve the best balance between performance and resource usage.\n\nUnderstanding and implementing the right quantization scheme is crucial for optimizing LLMs for local use. By carefully selecting and applying quantization strategies, developers can significantly reduce model size, improve inference speed, and make sophisticated AI models accessible on a broader range of hardware. This enables efficient, high-performance coding assistance tailored to the specific needs and constraints of your development environment.\n"},"02_understanding_large_language_models/02_07_scaling_models.html":{"url":"02_understanding_large_language_models/02_07_scaling_models.html","title":"How Models Are Scaled","keywords":"","body":"How Models Are Scaled\nScaling up a model, such as going from an 8B (8 billion parameters) model to a 13B, 34B, or even larger, involves a significant increase in the model’s complexity and capacity. This process is crucial for developing models that can handle more intricate tasks, provide greater accuracy, and generate more sophisticated outputs. Here’s how this scaling process works:\nWhat Does It Mean to Scale a Model?\nScaling a model means increasing the number of parameters—the weights and biases within the neural network—that the model uses to make predictions. These parameters are the core components of any machine learning model, determining how well it can learn from data and make accurate predictions or generate outputs.\nIn simple terms, the more parameters a model has, the \"bigger\" it is. A larger model can capture more complex patterns in data, allowing it to perform better on more challenging tasks. However, this also comes with increased computational requirements, both for training and inferencing.\nSteps to Make a Model Bigger\n\nIncrease the Number of Layers:\n\nLayer Expansion: One of the primary methods to scale up a model is by adding more layers to the neural network. Each layer in a neural network consists of neurons that process inputs and pass on their outputs to the next layer. By increasing the number of layers, the model can learn more abstract features of the data at deeper levels.\nDeeper Networks: As you add more layers, the network becomes \"deeper.\" Deeper networks are capable of learning more intricate patterns and representations, which is why they are often more accurate for complex tasks.\n\n\nWiden the Layers:\n\nIncreasing Neurons per Layer: Another approach to scaling is to increase the number of neurons (units) within each layer. This means that each layer can process more information at once, allowing the model to capture a broader range of features from the input data.\nWider Networks: Widening layers results in a \"wider\" network, which can improve the model’s ability to generalize from the training data, especially when dealing with large and diverse datasets.\n\n\nExpanding the Embedding Size:\n\nLarger Embeddings: In language models, the embedding layer converts words or tokens into numerical vectors that represent their meanings in a multi-dimensional space. By increasing the embedding size, the model can capture more nuanced meanings and relationships between words, leading to better performance on tasks involving language understanding and generation.\nEnhanced Representations: Larger embeddings allow the model to represent more complex relationships within the data, which is particularly important for tasks requiring high levels of comprehension or creativity.\n\n\nTraining with More Data:\n\nData Requirements: As models grow in size, they require more data to train effectively. This ensures that the additional parameters do not lead to overfitting (where the model performs well on training data but poorly on unseen data). Training on larger and more diverse datasets helps the model generalize better, taking full advantage of the increased capacity.\nLonger Training Times: Scaling a model not only requires more data but also significantly increases training times. Larger models take longer to train because they need to process more data and adjust a greater number of parameters.\n\n\nFine-Tuning and Specialization:\n\nTargeted Training: Once a larger model is trained, it can be fine-tuned on specific tasks or datasets. Fine-tuning allows the model to adapt its vast capacity to particular domains, improving its performance in areas like natural language understanding, code generation, or image recognition.\nSpecialized Models: For instance, fine-tuning a 13B model on instruction-based datasets might produce an \"Instruct\" variant of the model that is optimized for following and generating detailed instructions.\n\n\nQuantization and Optimization:\n\nMemory Efficiency: As models become larger, they require more memory to run effectively. To address this, techniques like quantization are used, where the precision of the model’s parameters is reduced (e.g., from 32-bit to 16-bit or 8-bit). This reduces the model’s memory footprint, making it more feasible to deploy on consumer hardware without sacrificing too much performance.\nInference Speed: Quantization also speeds up inference times because the model uses less computational power to process the reduced-precision parameters, which is particularly beneficial for deploying large models on GPUs with limited VRAM.\n\n\n\nChallenges of Scaling Models\nWhile scaling up a model provides greater capacity and performance, it also comes with challenges:\n\nIncreased Computational Costs: Larger models require more powerful hardware, including more RAM and VRAM, and longer training times, which can be costly.\nResource Management: Managing the resources to train and deploy larger models effectively requires careful planning, especially when dealing with extremely large models like those with tens or hundreds of billions of parameters.\nRisk of Overfitting: Without sufficient training data, larger models may overfit, meaning they learn the training data too well but fail to generalize to new inputs. This risk necessitates the use of vast and diverse datasets.\n\nConclusion\nScaling models is a crucial process in the development of advanced LLMs, allowing them to handle more complex tasks and deliver more accurate outputs. By understanding the methods and challenges involved in making a model bigger, developers and researchers can better utilize these powerful tools to push the boundaries of what AI can achieve.\n\nThis section should provide a comprehensive overview of how models are scaled up and what that entails. Let me know if this fits well into your guide or if there’s anything else you’d like to adjust!\n"},"03_selecting_models/03_01_selecting_the right model.html":{"url":"03_selecting_models/03_01_selecting_the right model.html","title":"Selecting the Right Model","keywords":"","body":"Selecting the Right Model\nAfter understanding the benefits of running large language models (LLMs) locally, the next step is to effectively select and use the right model for your needs. These are the 4 steps you’ll need to do in inorder to add local LLM models into your workflow, ensuring you make the most of your hardware and achieve the best performance possible.\nStep 1: Understand Your Computer’s Capabilities\nThe first step is to assess your computer's capabilities. The feasibility and performance of running models locally depend heavily on your hardware specifications, such as RAM, GPU, CPU, and storage. A system with 32GB of RAM and a powerful GPU, for instance, will handle more demanding models and deliver faster, more efficient performance than a less equipped setup. Understanding your hardware's strengths and limitations will guide you in choosing a model that fits your system’s capabilities.\nStep 2: Select the Right Model\nOnce you understand your hardware, the next step is to select the appropriate model. These models come in various configurations that balance memory usage, computational resources, and speed. Your choice should be based on your specific needs—whether you need quick, iterative code completion, detailed instruction-based coding, or the ability to handle complex, resource-intensive tasks. Familiarizing yourself with the different model variations and their strengths will help you choose the most suitable option.\nStep 3: Decode the Naming Structure\nThe naming structure of each model provides valuable insights into their configurations. For example, in the model name CodeLlama-13b-Instruct.Q3_K_M.gguf, each part denotes key attributes such as the model family, parameter size, variant type, quantization level, and optimization format. Understanding this structured naming helps you quickly identify a model’s capabilities and compatibility with your system specifications, making the selection process more straightforward.\nStep 4: Test the Model\nBefore fully integrating a selected model into your workflow, it’s crucial to test it. Running standardized tests allows you to evaluate the model's response time, accuracy, and resource usage. This step ensures that the model meets your performance expectations and aligns well with your hardware capabilities. Testing helps you identify any potential issues and make necessary adjustments before deploying the model for regular use.\nBy following these four steps—understanding your computer's capabilities, selecting the right model, decoding the naming structure, and performing thorough testing—you can effectively integrate local LLMs into your development workflow. This structured approach enhances your coding efficiency and leverages the full potential of advanced AI models, ensuring you get the best possible performance from your setup.\n"},"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html":{"url":"03_selecting_models/03_02_pick_the_right_model_for_your_memory.html","title":"Pick the Right Model for Your Memory","keywords":"","body":"Pick the Right Model for Your Memory\nWhen running local language models (LLMs), it's crucial to choose a model that matches your system's capabilities. Models typically come in standard sizes like 8B and 70B, which refer to the number of parameters or the complexity of the model. The more parameters a model has, the more detailed and nuanced its outputs can be, but this also requires more memory and processing power.\nThe most common sizes you'll encounter are 8B and 70B, which are typically the officially supported versions in many open-source communities. These sizes are designed to work well on consumer-grade hardware, with 8B models being optimized for systems with more modest resources, and 70B models catering to more advanced setups with significant memory and processing capacity.\nWhile these official sizes are the most reliable, you may also come across custom variations like 13B or 34B. These versions are often created by community members or other developers who modify the original models to fit specific needs. While these custom versions can offer additional capabilities or optimizations, they may also come with challenges in terms of compatibility, validation, and resource requirements.\nThe following table provides a guide to help you select the appropriate model variation based on your system's available memory. It includes key details such as memory usage, the resources required, and the expected speed. This should serve as a reference point when deciding which model to run on your local setup.\n\n\n\nSystem Memory\nModel Variation\nMemory Usage\nResources Required\nSpeed\nNotes\n\n\n\n\n8 GB\nModel-8B-Q3_K_L\n~4 GB\nLow\nFast\nBest for systems with very limited memory, providing decent performance and speed for general tasks.\n\n\n\nModel-8B-Q5_K_M\n~6 GB\nModerate\nModerate\nBalances performance and precision, suitable for more detailed tasks within the 8 GB limit.\n\n\n\nModel-8B-Q6_K\n~7 GB\nHigh\nModerate\nHigher precision, ideal for more detailed tasks while nearing the 8 GB memory cap.\n\n\n16 GB\nModel-13B-Q4_K_M\n~8 GB\nLow\nModerate\nSuitable for systems with 16 GB, balancing precision and performance effectively.\n\n\n\nModel-13B-Q5_K_M\n~10 GB\nModerate\nModerate\nCloser to the upper end of 16 GB, offering more precision for complex tasks.\n\n\n\nModel-13B-Q6_K_S\n~14-16 GB\nHigh\nSlower\nNears the 16 GB limit, offering maximum precision and detail for demanding tasks.\n\n\n32 GB\nModel-34B-Q5_K_S\n~18 GB\nLow\nModerate\nGood for systems around 16-32 GB, providing high precision and detailed outputs without using all available memory.\n\n\n\nModel-34B-Q6_K_M\n~24 GB\nModerate\nModerate\nHigh precision, nearing the upper end of 32 GB, making it suitable for detailed and demanding tasks.\n\n\n\nModel-34B-Q8_0\n~28-32 GB\nHigh\nSlower\nPushes the 32 GB limit, offering top-tier performance and precision for complex tasks.\n\n\n64 GB+\nModel-70B-IQ2_M\n~22 GB\nLow\nModerate\nSuitable for systems with more memory, balancing GPU offload and high precision while not fully utilizing 64 GB.\n\n\n\nModel-70B-Q4_K_S\n~35 GB\nModerate\nModerate\nFits well within 64 GB systems, offering higher precision and detailed outputs while utilizing a significant portion of memory.\n\n\n\nModel-70B-Q6_K_S\n~40 GB\nHigh\nSlower\nRequires 64 GB or more, providing the highest precision and performance but with slower speeds.\n\n\n\nAs you can see, these models span a range of memory footprints, from more lightweight options to those requiring significant resources. Based on their memory usage and performance characteristics, they can be grouped into three distinct categories:\n\nLightweight Models (4 GB - 8 GB): These are ideal for systems with limited memory, offering good performance for general coding tasks and faster response times. They are particularly useful if you're working on a laptop or a desktop with less RAM.\n\nModerate Models (~16 GB - 32 GB): These models are suitable for mid-range systems and offer a good balance of precision and performance. They can handle more demanding tasks and are ideal for developers who need more detailed outputs without sacrificing too much speed.\n\nAdvanced Models (~32 GB - 40 GB): These models are designed for high-performance systems with significant memory. They are best suited for resource-intensive tasks that require the highest level of detail and precision.\n\n\nBy understanding how different model variations affect memory usage and computational demands, you can make informed decisions that align with your system's capabilities and your project's requirements. This table should serve as a quick reference guide to help you match the right model with your hardware, ensuring smooth and efficient performance during development.\n"},"03_selecting_models/03_03_model_size_and_memory.html":{"url":"03_selecting_models/03_03_model_size_and_memory.html","title":"Model Size and Memory","keywords":"","body":"Model Size and Memory\nLLMs, have specific storage requirements when they are saved on disk and distinct memory usage characteristics when loaded into a computer’s RAM. Understanding these differences is crucial for optimizing their performance on various systems.\nModel Storage Size\nWhen we talk about the size of a model, we are generally referring to the amount of disk space it occupies when stored on your computer. This size is a fixed quantity and is the same regardless of whether you are using macOS, Windows, or Linux. The model’s storage size is determined by the number of parameters (the weights and biases) it contains, as well as the precision of these parameters. For example, models that use 16-bit floating-point precision will be larger than those that use 8-bit quantization.\nWhile the model file size is consistent across different operating systems, it is important to note that the size on disk does not necessarily reflect the memory usage when the model is loaded. The storage size gives you an idea of how much space you need on your hard drive to store the model but doesn’t fully account for the runtime memory requirements.\nModels in Memory\nWhen an LLM is loaded into memory (RAM) for inferencing, the memory usage can exceed the model’s storage size. This is due to several factors, including how the operating system manages memory and the additional resources required to actually run the model. Memory usage can also be influenced by the runtime environment, such as the libraries and dependencies used to manage the model.\nFor example, when a model is loaded, the entire set of weights needs to be accessible in memory, and additional memory is required for managing the data structures that facilitate the model's operation. Depending on how efficiently the model and its runtime environment are implemented, memory usage can vary slightly between different operating systems.\nFurthermore, as the model processes inputs (during inferencing), temporary memory allocations are made for intermediate computations. This means that, particularly for large models, you need more available RAM than the model’s file size might suggest.\n"},"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html":{"url":"03_selecting_models/03_04_cpu_vs_gpu_inferencing.html","title":"CPU vs. GPU and Inferencing","keywords":"","body":"CPU vs. GPU and Inferencing\nInferencing is the process of using a pre-trained model to make predictions or generate outputs based on new inputs. When working with large language models (LLMs), the process of inferencing involves a series of complex mathematical operations that are computationally intensive. Understanding how these operations are handled by your computer's hardware—specifically the CPU and GPU—is crucial for optimizing performance.\nWhat is Inferencing?\nInferencing with an LLM involves feeding input data (such as a text prompt) into the model and then performing a series of calculations to generate a response. These calculations are based on the model’s parameters (weights and biases), which were learned during the training phase. During inferencing, the model uses these parameters to process the input data, compute intermediate representations, and finally produce an output.\nThis process is computationally demanding because it involves numerous matrix multiplications, non-linear transformations, and other operations across potentially billions of parameters. The efficiency and speed of inferencing depend significantly on the hardware used to perform these calculations.\nCPU vs. GPU for Inferencing\nCPU Inferencing:\nTraditionally, CPUs (Central Processing Units) have been used for general-purpose computing, including running smaller or less complex models. A CPU has fewer cores compared to a GPU, but these cores are optimized for a wide range of tasks, making the CPU more versatile but generally slower at performing the highly parallelizable operations required for LLM inferencing.\nWhen an LLM runs on a CPU, the model’s computations are spread across the available cores. This works well for smaller models or systems without powerful GPUs. However, because inferencing with LLMs requires handling large amounts of data and performing many operations simultaneously, CPUs can become a bottleneck, leading to slower performance and longer response times.\nGPU Inferencing:\nGPUs (Graphics Processing Units) are designed specifically for parallel processing, making them ideal for tasks like inferencing with large language models. A GPU contains thousands of smaller cores that can perform many operations concurrently, which is perfect for the kind of matrix multiplications and other repetitive tasks involved in inferencing.\nWhen a model is offloaded to a GPU, the inferencing process can be significantly accelerated. The GPU handles the bulk of the mathematical operations, allowing for faster processing of large models and more efficient handling of complex inputs. This is especially important for real-time applications or when working with models that have a large number of parameters.\nOffloading to the GPU\nOffloading refers to the process of transferring the computational workload from the CPU to the GPU. In the context of LLMs, this means using the GPU to handle the inferencing process, which can drastically improve performance, particularly for large models.\nTo offload a model to the GPU, the model’s parameters and data need to be loaded into the GPU’s memory (VRAM). This allows the GPU to perform the necessary computations directly, without needing to constantly communicate with the CPU, which would otherwise slow down the process. The more VRAM available, the larger the model you can run on the GPU without hitting performance limits.\nHowever, if the model is too large to fit entirely into the GPU’s VRAM, it may need to be split between the CPU and GPU or only partially offloaded. This can lead to performance degradation, as the system has to manage the data transfer between the CPU and GPU, which adds overhead and reduces the speed of inferencing.\nUnderstanding the differences between CPU and GPU inferencing and the implications of model size on system memory and GPU resources is critical for optimizing the performance of large language models. By carefully selecting the right model size for your system’s capabilities and effectively leveraging GPU offloading, you can achieve faster and more efficient inferencing. This not only enhances the overall user experience but also ensures that your hardware is used to its fullest potential. As you work with LLMs, keep these considerations in mind to strike the best balance between model complexity and system performance, allowing you to take full advantage of the powerful tools at your disposal.\n"},"03_selecting_models/03_05_offloading_to_the_gpu.html":{"url":"03_selecting_models/03_05_offloading_to_the_gpu.html","title":"Offloading to the GPU","keywords":"","body":"Offloading to the GPU\nAs large language models (LLMs) continue to grow in size and complexity, the demands they place on computing hardware have increased significantly. While CPUs are capable of handling smaller models, the real power of LLMs is unleashed when they are run on GPUs (Graphics Processing Units). Offloading LLM inferencing to the GPU is a crucial step in optimizing model performance, especially when working with large-scale models that require substantial computational resources.\nWhy Offloading to the GPU Matters\nGPUs are designed for parallel processing, making them particularly well-suited for the kind of operations that LLMs require during inferencing. Unlike CPUs, which have a limited number of cores optimized for a wide range of tasks, GPUs have thousands of smaller cores that are optimized for performing many calculations simultaneously. This parallelism allows GPUs to handle the large matrix multiplications and other operations required by LLMs far more efficiently than CPUs.\nWhen you offload an LLM to the GPU, you are effectively leveraging this parallel processing capability to accelerate the inferencing process. This not only speeds up the time it takes to generate responses but also allows you to work with larger and more complex models that would be impractical to run solely on a CPU.\nBenefits of GPU Offloading\n\nIncreased Speed: The most immediate benefit of GPU offloading is the significant increase in processing speed. By utilizing the GPU’s parallel processing capabilities, inferencing times are drastically reduced, making real-time applications and quick iterations possible, even with large models.\n\nBetter Resource Utilization: Offloading to the GPU allows the CPU to focus on other tasks, thereby improving overall system efficiency. This division of labor ensures that both the CPU and GPU are being used optimally, which is especially important when running multiple applications simultaneously.\n\nEnhanced Scalability: As models grow in size, the ability to offload their processing to a GPU becomes increasingly important. GPUs with large amounts of VRAM can handle more extensive models, making it easier to scale your use of LLMs as your needs evolve.\n\nImproved Performance with Large Models: For very large models, GPU offloading is often the only way to achieve acceptable performance. CPUs alone may struggle to handle the sheer volume of calculations required, leading to slow response times and potential system instability.\n\n\nChallenges and Considerations\nWhile GPU offloading offers significant advantages, it’s not without its challenges. The amount of VRAM available on your GPU will determine how much of the model can be loaded and processed directly on the GPU. If the model is too large to fit into the available VRAM, you may experience performance issues, such as slower processing times or the need to revert to CPU processing for parts of the model.\nMoreover, not all GPUs are created equal. The performance gains from offloading can vary depending on the GPU’s architecture, the amount of VRAM, and the specific model being used. Therefore, understanding your GPU’s capabilities and selecting models that match its strengths is crucial for maximizing performance.\n\nIf a model exceeds the available system memory (RAM) or GPU memory (VRAM), several issues can arise:\n\nSystem Slowdown: The system may attempt to use swap space (a portion of disk storage used as virtual memory) to compensate for the lack of RAM. However, swap is much slower than RAM, leading to significant performance degradation and potential system instability.\n\nApplication Crashes: If the system cannot allocate enough memory for the model, the application running the model might crash or fail to load the model entirely, often resulting in \"Out of Memory\" errors.\n\nReduced Performance: When the GPU cannot fully load the model, it may revert to using the CPU for some operations, drastically reducing the speed of inferencing. This mixed CPU/GPU workload is less efficient and can lead to inconsistent performance.\n\n\n\nBy offloading LLM inferencing to a GPU, you can unlock the full potential of these powerful models, making it possible to work with larger datasets, generate more complex outputs, and achieve faster processing times. While there are considerations to keep in mind, particularly regarding VRAM limits and GPU capabilities, the benefits of GPU offloading are clear. For anyone serious about working with large language models, understanding and utilizing GPU offloading is not just an option—it’s a necessity for maximizing performance and efficiency.\n"},"03_selecting_models/03_06_picking_code_llama_instruct_variations.html":{"url":"03_selecting_models/03_06_picking_code_llama_instruct_variations.html","title":"CodeLlama 3.1 Instruct Variations","keywords":"","body":"CodeLlama 3.1 Instruct Variations\nTo optimize performance based on your system's memory, here are Code Llama Instruct model variations from LM Studio, categorized by performance tier. These categories range from fast and efficient to resource-intensive with maximum precision.\nWhen using LM Studio, you'll notice that it provides helpful compatibility suggestions for each model, such as \"Full GPU Offload Possible,\" \"GPU Offload Maybe Possible,\" and \"Likely Too Large for This Machine.\" These suggestions are critical in determining whether a model will run efficiently on your hardware.\n\nFull GPU Offload Possible: This indicates that the model can entirely offload processing to the GPU, which is ideal for maximizing performance and minimizing memory usage. If you see this message, it's likely that the model will run smoothly on your system.\nGPU Offload Maybe Possible: This suggests that the model might offload some, but not all, of its processing to the GPU. While this can still improve performance, it may not fully relieve the memory burden on your system, so you should be cautious with larger models.\nLikely Too Large for This Machine: This warning indicates that the model may exceed your system's capabilities due to insufficient memory or GPU power. Attempting to run such a model might result in crashes or significantly degraded performance, so it's generally best to only use these models if you have a high-end system with ample resources.\n\nThese guidelines are meant to help you get started with choosing the right models, but finding the optimal configuration for your specific needs and hardware will involve some trial and error. Start with models that fall comfortably within your system's capabilities, and gradually test more demanding models as you gain confidence in your system's performance. Here are some of the ones I've been testing with.\nFast and Efficient (8 GB Memory)\nFor developers with 8 GB of memory, the TheBloke/CodeLlama-7B-Instruct-GGUF-Q5_K_M model provides a good balance between speed and precision, offering the highest practical performance within this memory range.\n\n\n\nParameter\nValue\n\n\n\n\nMemory Usage\n~5.73 GB\n\n\nResources Required\nModerate\n\n\nSpeed\nFast\n\n\nPrecision\nHigh\n\n\n\nThis model is well-suited for tasks requiring quick responses and moderate precision, making it ideal for rapid iterations and less resource-intensive coding tasks. The Q5_K_M quantization strikes a good balance between memory usage and computational efficiency. However, suppose you want to push the boundaries with higher precision. In that case, you might try the phind-codellama-7B-v1.Q8_0.gguf model, which offers maximum precision with a memory usage that’s likely higher than most 8 GB systems can comfortably handle, so it may be worth trying on systems with 16 GB of memory.\n\nBalanced Performance (16 GB Memory)\nFor systems with 16 GB of memory, the TheBloke/CodeLlama-13B-Instruct-GGUF-Q6_K model offers balanced performance, combining speed with high precision.\n\n\n\nParameter\nValue\n\n\n\n\nMemory Usage\n~14-16 GB\n\n\nResources Required\nHigh\n\n\nSpeed\nModerate\n\n\nPrecision\nVery High\n\n\n\nThis model is ideal for complex coding tasks that demand both performance and accuracy. The Q6_K quantization ensures efficient use of memory while delivering high-quality outputs, making it a versatile choice for a wide range of programming challenges.  If your system can handle more, you could experiment with models like the phind-codellama-13B-v1.Q8_0.gguf, pushing precision even further, though at the cost of increased memory usage and potentially reduced speed.\n\nHigh Precision (32 GB Memory)\nFor developers using systems with 32 GB of memory, the phind-codellama-34B-v1.Q5_K_M.gguf model is a strong choice. It offers high precision without exceeding the system's memoryding capacity, provi a good balance between precision and resource usage.\n\n\n\nParameter\nValue\n\n\n\n\nMemory Usage\n~23.84 GB\n\n\nResources Required\nHigh\n\n\nSpeed\nModerate\n\n\nPrecision\nVery High\n\n\n\nThis model is designed for demanding coding tasks that require detailed and accurate outputs, such as advanced problem-solving or algorithm development. The Q5_K_M quantization ensures that your system's memory is used effectively, delivering high precision without compromising performance.\nHowever, if your system can handle even more demanding models, consider the phind-codellama-34B-v1.Q8_0.gguf model, which offers maximum precision with a memory usage of approximately 35.86 GB. This model is ideal for tasks requiring the highest detail and accuracy level. Remember that this model might be on the edge of what a 32 GB system can handle, especially without full GPU offload capabilities. Therefore, while pushing your system's limits with these higher-end versions is possible, it's essential to monitor performance and stability closely.\nMaximum Performance (64 GB Memory or More)\nFor systems with 64 GB of memory or more, the TheBloke/codellama-70B-Instruct.Q2_K.gguf model represents a significant step up in performance. This model uses around 28-32 GB of memory, which places it at the upper limit of what my 34 GB system can handle, especially since LM Studio indicates that \"GPU offload may be possible.\" While your system might manage this model, monitoring performance closely is essential as it could be on the edge of your system’s capabilities.\n\n\n\nParameter\nValue\n\n\n\n\nMemory Usage\n~28-32 GB\n\n\nResources Required\nHigh\n\n\nSpeed\nModerate\n\n\nPrecision\nVery High\n\n\n\nThis model is well-suited for complex tasks that require high precision, such as advanced algorithm development or detailed code analysis. However, due to your system's memory constraints, this model may push your resources to their limits, and performance might vary depending on the workload and GPU offload effectiveness.\nIf you have more than 64 GB of RAM, there are larger models you can experiment with for even better results. For example, models like phind-codellama-70B-v1.Q4_K_S.gguf or phind-codellama-70B-v1.Q6_K_S.gguf provide enhanced precision and detail, with memory usage that can range from 35 GB to over 40 GB. These models are designed for the most demanding tasks and offer the highest level of precision available in the Code Llama series.\n\nphind-codellama-70B-v1.Q4_K_S.gguf: With memory usage around 35-40 GB, this model provides a strong balance between resource utilization and precision. It's ideal for systems with 64 GB of memory that need to handle highly detailed coding tasks.\n\nphind-codellama-70B-v1.Q6_K_S.gguf: This model pushes the limits further, with memory usage exceeding 40 GB, delivering maximum precision and detail. It’s best suited for the most advanced systems with ample resources to spare.\n\n\n"},"04_models_for_ coding/04_01_overview_of_code_llama_variations.html":{"url":"04_models_for_ coding/04_01_overview_of_code_llama_variations.html","title":"Overview of Code Llama Variations","keywords":"","body":"Overview of Code Llama Variations\nIn my exploration of the Code Llama models, I’ve found them to be a strong tool for coding tasks. Built on Meta’s Llama 2, these models are trained on a vast corpus of publicly available code, making them effective at generating accurate and contextually relevant code snippets. This makes them invaluable for a range of coding tasks, from basic code completion to more complex problem-solving.\nCodeLlama Base Models serve as the foundation, offering general-purpose coding capabilities across multiple programming languages. These models come in 7B, 13B, and 34B variations, with the model size directly influencing the detail and accuracy of the generated code. Larger models offer more nuanced outputs but require more computational resources. The base models are versatile enough to handle various coding scenarios, from simple scripts to more complex applications, making them a good choice for developers who need a flexible coding tool.\nAs I’ve delved deeper into these models, I discovered that specialized variations cater to more specific needs. CodeLlama-Instruct stands out for its ability to bridge the gap between natural language and code. It’s fine-tuned to excel at converting detailed problem statements or high-level descriptions into functional code, making it particularly valuable when you need to describe what you want in plain language and get accurate code outputs. This variation is optimized for understanding and following natural language instructions, which helps streamline the coding process by making interactions more intuitive and reducing the need to worry about technical syntax or structure.\nCodeLlama-Instruct is particularly responsive to varying levels of detail in prompts. Whether you’re outlining a simple function or specifying intricate logic, it can interpret a wide range of instructions and produce code that aligns with your intent. This versatility is especially useful in projects where you need to iterate quickly and refine code based on natural language input. The model’s optimization also means it can handle ambiguous or incomplete instructions better than other models, providing relevant code suggestions that you can refine or expand upon.\nMoreover, CodeLlama-Instruct’s design makes it more responsive to varying levels of detail in prompts, whether you’re outlining a simple function or specifying intricate logic. It can effectively interpret a wide range of instructions, from vague descriptions to detailed requirements, and produce code that aligns with the developer’s intent. This versatility is a key advantage when working on projects where the ability to iterate quickly and refine code based on natural language input is essential.\nIn addition to CodeLlama-Instruct, there’s also CodeLlama-Python, which is fine-tuned specifically for Python development. While CodeLlama-Python is an excellent choice for Python-centric tasks, I’ve found CodeLlama-Instruct to be more broadly applicable across different coding scenarios. Its ability to translate natural language into code makes it a powerful tool for both experienced developers looking to streamline their workflow and those still learning the ropes.\nTo give you a clear overview of how each variation of CodeLlama performs, here’s a table that summarizes their memory usage, resource requirements, speed, and specific strengths. This should help you quickly identify which model best suits your coding needs and system capabilities.\n\n\n\nModel Variation\nMemory Usage\nResources Required\nSpeed\nNotes\n\n\n\n\nCodeLlama-7B\nModerate\nModerate\nFast\nGeneral-purpose, suitable for a wide range of coding tasks.\n\n\nCodeLlama-13B\nModerate-High\nModerate-High\nModerate\nBetter for more detailed and complex code generation.\n\n\nCodeLlama-34B\nHigh\nHigh\nSlower\nBest for highly detailed tasks but requires significant resources.\n\n\nCodeLlama-Python-7B\nModerate\nModerate\nFast\nOptimized for Python, ideal for Python-centric projects.\n\n\nCodeLlama-Python-13B\nModerate-High\nModerate-High\nModerate\nProvides detailed Python code generation, suitable for larger projects.\n\n\nCodeLlama-Python-34B\nHigh\nHigh\nSlower\nMaximum precision in Python, with higher resource requirements.\n\n\nCodeLlama-Instruct-7B\nModerate\nModerate\nFast\nOptimized for following natural language instructions, versatile use.\n\n\nCodeLlama-Instruct-13B\nModerate-High\nModerate-High\nModerate\nIdeal for handling complex instructions and generating detailed code.\n\n\nCodeLlama-Instruct-34B\nHigh\nHigh\nSlower\nHighest precision in instruction-following, suitable for resource-intensive tasks.\n\n\n\nThis table focuses exclusively on the different CodeLlama variations, providing a clear overview of their memory usage, resource requirements, speed, and key strengths.\nCodeLlama-Instruct isn’t the only option available. There are other open-source models that offer unique strengths, depending on your specific needs. Whether you’re working on multi-language projects, need general-purpose coding assistance, or are exploring more specialized tools, the following section will introduce some of these alternatives and how they might complement your development environment.\n"},"04_models_for_ coding/04_02_other_open_source_coding_models.html":{"url":"04_models_for_ coding/04_02_other_open_source_coding_models.html","title":"Other Notable Open Source Models for Coding","keywords":"","body":"Other Notable Open Source Models for Coding\nWhile Code Llama models are a significant step forward in coding LLMs, there are other open-source models that also offer robust capabilities for developers, particularly when it comes to versatility and multi-functional tasks.\nStarCoder is an impressive model designed with multi-language support, making it ideal for projects that require coding in different programming languages. Developed as part of the BigCode initiative, StarCoder is trained on a diverse set of programming languages, allowing it to handle complex, cross-language tasks efficiently. It’s particularly useful in environments where you need to switch between languages or work on projects that involve multiple coding standards. StarCoder’s ability to generate and complete code snippets across various languages makes it a versatile tool for developers working on diverse projects. Whether you’re integrating systems, working on polyglot projects, or simply need a model that can adapt to various coding environments, StarCoder is a reliable choice.\nOne of the standout features of StarCoder is its capability to understand and generate code across a wide range of programming languages, making it well-suited for tasks that involve multi-language workflows. StarCoder provides the tools to seamlessly transition between different languages without losing context or accuracy for developers who work in environments where flexibility and precision are crucial. This model's strength lies in its adaptability and precision, making it an excellent choice for developers who need a robust, multi-language coding assistant.\nCohere Command R is another versatile model, designed as a general-purpose tool that excels at a wide variety of tasks, including coding. Unlike models that are specifically tailored to coding, Cohere Command R’s strength lies in its adaptability. It’s particularly effective in environments where coding is just one of many tasks, such as in data analysis, content generation, or even creative writing. This model can handle coding tasks efficiently while also offering broad utility in other domains, making it ideal for developers who need a flexible tool that can do more than just code.\nCohere Command R’s general-purpose design makes it a strong choice for developers who require a model that can switch between different types of tasks without losing performance. Whether you're working on a multi-functional project that involves coding, documentation, and data manipulation, or you need a model that can generate content and analyze data in addition to coding, Cohere Command R provides a balanced solution. Its ability to adapt to various tasks makes it particularly useful in dynamic environments where the scope of work frequently changes.\nComparison Table\nHere’s a table comparing the relevant CodeLlama models with StarCoder and Cohere Command R, focusing on their memory usage, resource requirements, speed, and key strengths:\n\n\n\nModel Variation\nMemory Usage\nResources Required\nSpeed\nNotes\n\n\n\n\nCodeLlama Base\nModerate\nModerate\nFast\nGeneral-purpose, good for a wide range of coding tasks.\n\n\nCodeLlama-Instruct\nModerate\nModerate\nFast\nOptimized for natural language instructions, ideal for instruction-based code generation.\n\n\nCodeLlama-Python\nModerate\nModerate\nFast\nBest for Python-centric tasks, with enhanced performance in Python environments.\n\n\nStarCoder\nModerate-High\nModerate-High\nModerate\nExcels in multi-language coding scenarios, strong for cross-language tasks.\n\n\nCohere Command R\nModerate\nModerate\nModerate\nGeneral-purpose model, highly adaptable to various coding and non-coding tasks.\n\n\n\nAs I continue to explore these models, I plan to experiment with each one to understand their strengths and weaknesses in various coding scenarios. I’ll be updating this guide with my findings, especially as new models are released or significant updates are made to existing ones. This ongoing process will help refine the recommendations and ensure that the guide remains a valuable resource for developers looking to leverage the best open-source LLMs for coding. Stay tuned for updates as I dig deeper into these tools and discover what works best in different development environments.\n"},"05_installation/05_01_installing_lm_studio.html":{"url":"05_installation/05_01_installing_lm_studio.html","title":"Installing LM Studio","keywords":"","body":"Installing LM Studio\nLM Studio is a free, more user-friendly tool designed for developers and data scientists to run LLMs locally on their machines. It provides a clean interface and the ability to facilitate the deployment and testing of LLMs without needing extensive cloud resources or messing with the command line, setting up services, and connecting to API endpoints. To get started with LM Studio, download it from the official LM Studio website.\nIMAGE\nSelect the version compatible with your computer and download the installer. After the installation is complete, you’ll be ready to launch the app. The first time you open LM Studio, you may need to grant permission for it to run. Follow any on-screen prompts to complete this step.\nIMAGE\nUpon launching LM Studio for the first time, you will be guided through an initial setup process. During this setup, you can set your preferences, such as default language model settings and workspace configurations. LM Studio may also prompt you to configure the environment, including paths to necessary dependencies or model directories. You can always go back and customize this later on in the app’s settings.\nIMAGE\nOnce the initial setup is complete, take some time to explore the LM Studio interface. The interface is organized into key sections, including the model management area, settings menu, and workspace. In the model management area, you will add, configure, and manage the LLMs you plan to run. \nIMAGE\nHere, you can easily import models and adjust their settings to optimize performance. The settings menu provides access to various options that allow you to customize your experience and optimize performance based on your hardware capabilities. The workspace is the main area where you will input commands, test models, and view outputs.\n"},"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html":{"url":"05_installation/05_02_configuring_lm_studio_on_apple_silicon.html","title":"Configuring LM Studio on Apple Silicon","keywords":"","body":"Configuring LM Studio on Apple Silicon\nIf you are using an Apple Silicon computer, such as a MacBook Pro with an M3 Max chip, it is important to optimize LM Studio to fully leverage your hardware’s capabilities. Here’s how you can do it:\nCallout: Optimizing LM Studio for Apple Silicon\nTo ensure optimal performance on your Apple Silicon Mac, follow these steps:\n\nLaunch LM Studio: Open LM Studio from your Applications folder.\nNavigate to Settings: Click on the settings menu to access various configuration options.\nSelect the Default Preset: Under the settings presets, choose \"Default LM Studio macOS.\" This preset is specifically designed to balance resource usage and processing power on macOS systems.\nOptimize Performance: The \"Default LM Studio macOS\" preset configures LM Studio to take full advantage of your Mac’s hardware, ensuring smooth operation and efficient model execution.\n\nBy selecting this preset, you ensure that LM Studio is configured to make the most of your MacBook Pro's specifications, allowing you to run large language models effectively and seamlessly. This configuration is essential for maximizing performance and ensuring a seamless experience when working with LLMs locally on Apple Silicon.\n"},"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html":{"url":"05_installation/05_03_optimizing_lm_studio_for_apple_silicon.html","title":"Optimizing LM Studio for Apple Silicon","keywords":"","body":"Optimizing LM Studio for Apple Silicon\nRunning large language models (LLMs) locally on your Apple Silicon Mac can significantly enhance performance, privacy, and cost efficiency. However, to get the best results, it is crucial to configure LM Studio appropriately based on your specific hardware capabilities. Here’s a detailed guide on the key settings and their importance, followed by tailored configurations for high-end, medium-end, and low-end Apple Silicon Macs.\nKey Settings and Their Importance\nContext Length (n_ctx): The context length determines how much text the model can consider at once. Setting an appropriate context length is essential for balancing memory usage and processing speed. A shorter context length reduces memory consumption and speeds up processing but may limit the model’s understanding of the context. For high-end systems, a longer context length can be afforded, providing more comprehensive context analysis.\nTokens to Generate (n_predict): This setting controls the number of tokens (words or parts of words) the model generates in response to a prompt. Limiting the number of tokens helps prevent excessively long responses, speeding up the generation process. It’s crucial to find a balance that allows the model to provide useful outputs without overloading the system.\nCPU Threads (n_threads): Configuring the number of CPU threads used by LM Studio is vital for efficient processing. Utilizing more CPU threads can speed up computation by leveraging the multi-core capabilities of modern CPUs. However, it’s important to match the number of threads to the physical cores available to avoid diminishing returns or potential system instability.\nGPU Offload (n_gpu_layers): The GPU offload setting determines how many layers of the model’s computation are handled by the GPU. Offloading computation to the GPU can significantly reduce the load on the CPU and speed up processing, especially for larger models. The number of layers to offload should be adjusted based on the GPU’s capabilities and the overall system memory.\nMemory Allocation: Ensuring that sufficient memory is allocated to LM Studio is essential for handling larger models and datasets efficiently. Optimizing memory allocation helps prevent system slowdowns and ensures that the model runs smoothly without consuming excessive resources.\nOptimized Configurations for Apple Silicon Macs\nHere are the recommended configurations for three types of Apple Silicon Macs: a high-end system (MacBook Pro M3 Max with 32GB RAM), a medium-end system (MacBook Pro M1 Pro with 16GB RAM), and a low-end system (MacBook Air M1 with 8GB RAM).\nApple Silicon Cheat Sheet\nHigh-End System: MacBook Pro M3 Max with 32GB RAM\nFor high-end systems like the MacBook Pro M3 Max with 32GB RAM, maximizing the context length and offloading significant computation to the GPU will leverage the full potential of the hardware. The settings should be adjusted as follows:\n\n\n\nConfiguration Option\nSetting\nImpact\n\n\n\n\nContext Length (n_ctx)\n2048\nReduces memory usage and speeds up processing without significantly impacting context understanding.\n\n\nTokens to Generate (n_predict)\n100\nLimits the response length, speeding up response time.\n\n\nCPU Threads (n_threads)\n8 (or number of physical cores)\nUtilizes more CPU cores, speeding up processing.\n\n\nGPU Offload (n_gpu_layers)\n24\nShifts more computation to the GPU, reducing CPU load.\n\n\nMemory Allocation\nMaximize available RAM\nEnsures sufficient memory is allocated to LM Studio, preventing slowdowns.\n\n\n\n\nMedium-End System: MacBook Pro M1 Pro with 16GB RAM\nFor medium-end systems like the MacBook Pro M1 Pro with 16GB RAM, a balanced approach is necessary. Adjusting the context length and GPU offload settings will help maintain performance without overloading the system:\n\n\n\nConfiguration Option\nSetting\nImpact\n\n\n\n\nContext Length (n_ctx)\n1024\nBalances memory usage and processing speed, providing adequate context analysis.\n\n\nTokens to Generate (n_predict)\n100\nLimits the response length, speeding up response time.\n\n\nCPU Threads (n_threads)\n8 (or number of physical cores)\nUtilizes more CPU cores, speeding up processing.\n\n\nGPU Offload (n_gpu_layers)\n16\nDistributes some computation to the GPU, reducing CPU load.\n\n\nMemory Allocation\nOptimize for 16GB\nEnsures efficient memory usage without overloading the system.\n\n\n\n\nLow-End System: MacBook Air M1 with 8GB RAM\nFor low-end systems like the MacBook Air M1 with 8GB RAM, it's essential to reduce the load on both the CPU and GPU. Shortening the context length and limiting the number of tokens generated will help maintain performance:\n\n\n\nConfiguration Option\nSetting\nImpact\n\n\n\n\nContext Length (n_ctx)\n512\nReduces memory usage and processing load, while still providing sufficient context.\n\n\nTokens to Generate (n_predict)\n50\nLimits the response length, speeding up response time and reducing resource usage.\n\n\nCPU Threads (n_threads)\n4 (or number of physical cores)\nUtilizes available CPU cores efficiently without overloading the system.\n\n\nGPU Offload (n_gpu_layers)\n8\nShifts some computation to the GPU, alleviating CPU load.\n\n\nMemory Allocation\nOptimize for 8GB\nEnsures efficient memory usage without overloading the system.\n\n\n\nBy following these configuration settings tailored to your specific Apple Silicon Mac, you can ensure optimal performance when running large language models in LM Studio. Each table provides a balance between resource allocation and processing efficiency to make the most out of your hardware capabilities, from high-end systems with abundant resources to lower-end systems with more constrained hardware.\n"},"05_installation/05_04_picking_models_in_lm_studio.html":{"url":"05_installation/05_04_picking_models_in_lm_studio.html","title":"Picking Models in LM Studio","keywords":"","body":"Picking Models in LM Studio\nSelecting the right model in LM Studio is crucial for optimizing performance and ensuring that your system can handle the demands of large language models (LLMs). This guide will help you understand the key factors involved in choosing a model, from understanding model size and memory requirements to knowing how to leverage your CPU and GPU effectively.\nUnderstanding Model Size and Memory\nWhen working with LLMs, it's important to distinguish between the model's storage size and its memory usage. The storage size refers to the amount of disk space the model occupies, which remains consistent across different operating systems (macOS, Windows, Linux). However, the memory usage—the amount of RAM required when the model is loaded into your system—can vary. This variation is influenced by how the operating system manages memory, the model’s architecture, and additional overhead required for runtime computations.\nWhen an LLM is loaded into memory, the RAM usage can exceed the model's storage size due to factors like temporary memory allocations and data structures needed for inferencing. This means that, particularly for large models, you'll need more available RAM than the model’s file size might suggest. Understanding these memory dynamics is crucial when selecting a model that fits your system's capabilities.\nCPU vs. GPU for Inferencing\nInferencing is the process of using a trained model to generate predictions or outputs from new inputs. The efficiency of inferencing depends significantly on whether it's handled by the CPU or GPU:\n\nCPU Inferencing: CPUs are versatile and can handle a wide range of tasks, including running smaller or less complex models. However, because CPUs have fewer cores optimized for general-purpose computing, they can become a bottleneck for large LLMs, leading to slower inferencing and longer response times.\n\nGPU Inferencing: GPUs, with their thousands of smaller cores, are designed for parallel processing, making them ideal for the large-scale computations required by LLMs. Offloading inferencing to the GPU can drastically accelerate the process, allowing for real-time applications and the use of larger models.\n\n\nWhen selecting a model in LM Studio, it's essential to consider whether your system has the GPU capabilities to handle the model you choose. Models that can be fully offloaded to the GPU will perform significantly better than those that rely on the CPU.\nGPU Offloading: What You Need to Know\nOffloading a model to the GPU involves loading the model's parameters and data into the GPU’s memory (VRAM). This allows the GPU to perform the necessary computations, which can greatly enhance performance. However, there are a few key points to consider:\n\nFull GPU Offload Possible: If LM Studio indicates that \"full GPU offload\" is possible, this means that the entire model can fit into your GPU's VRAM. This is the best-case scenario for performance, as the GPU will handle all inferencing operations, leading to faster processing times.\n\nGPU Offload May Be Possible, Likely Too Large for This Computer: This message suggests that the model might be too large for your GPU’s VRAM, or it’s close to the limit. In such cases, LM Studio may attempt to offload as much as possible to the GPU, but some parts of the model may need to be handled by the CPU. This can result in slower performance and increased reliance on system RAM or swap space.\n\n\nPractical Tips for Model Selection\n\nKnow Your Hardware Limits: Before selecting a model, check your system's available RAM and VRAM. This will help you understand whether you can fully offload a model to the GPU or if you should opt for a smaller model that fits comfortably within your system’s capabilities.\n\nStart Small, Scale Up: If you’re unsure of your system’s limits, start with a smaller model to gauge performance. Once you’ve established how your system handles it, you can gradually move to larger models.\n\nMonitor System Performance: Use tools like nvidia-smi for NVIDIA GPUs or system monitoring tools to keep an eye on VRAM and RAM usage while running models. This will help you identify any bottlenecks or potential issues with model offloading.\n\nUnderstand the Trade-offs: If a model is too large for full GPU offload, be prepared for slower performance and potential system strain. In such cases, you might need to balance the need for precision and performance with the limitations of your hardware.\n\n\nBy understanding how model size, memory usage, and GPU offloading impact performance, you can make informed decisions when selecting models in LM Studio. This will allow you to maximize the efficiency and effectiveness of your work with large language models.\n"},"06_evaluation/06_01_testing_each_model.html":{"url":"06_evaluation/06_01_testing_each_model.html","title":"Testing Each Model","keywords":"","body":"Testing Each Model\nTo determine which model works best for your needs, you should follow a systematic approach to testing each one. Start by evaluating the codellama-13b-instruct.Q3_K_S.gguf model for tasks that require quick responses. Note the speed and adequacy of the outputs for your typical coding tasks.\nNext, switch to the codellama-13b-instruct.Q4_K_M.gguf model and compare its performance and accuracy against the first model. Pay attention to how well it handles more detailed and complex tasks and whether the balance of speed and precision meets your expectations.\nFinally, test the codellama-34b-instruct.Q5_K_M.gguf model for tasks that require the highest precision. Evaluate the quality of the outputs and consider if the slower response times are acceptable given the increased accuracy and detail.\nBy experimenting with these models, you can identify which one best suits your workflow and specific coding needs. Each model offers a unique balance of speed, resource usage, and precision, allowing you to choose the optimal solution for your MacBook Pro M3 Max.\n"},"06_evaluation/06_02_evaluating_models.html":{"url":"06_evaluation/06_02_evaluating_models.html","title":"Evaluating Models with a Standardized Test","keywords":"","body":"Evaluating Models with a Standardized Test\nTo determine the best CodeLlama Instruct model for your MacBook Pro M3 Max, we need a comprehensive and consistent test that challenges the models and allows us to evaluate their performance in terms of speed, accuracy, and resource usage. The chosen prompt involves creating a simple React note-taking application, which will test the models' abilities to generate detailed, functional code while handling various aspects of modern JavaScript development.\nThe Test Prompt\nDevelop a simple note-taking application using React. The application should have the following features:\n\n1. A homepage that displays a list of notes.\n2. A form to add a new note, which includes fields for the note title and content.\n3. Each note should have an option to delete it.\n4. Use functional components and React hooks (useState and useEffect) for state management.\n5. Include error handling for empty note submissions.\n6. Ensure the code is well-commented and follows best practices.\nWhy This Prompt?\nThis prompt is chosen for several compelling reasons. Firstly, it requires the model to generate multiple components, manage state, and handle user inputs, which are critical tasks in React development. By involving these complexities, we can thoroughly assess the model's capability to generate accurate and comprehensive code.\nFurthermore, the prompt specifies the use of functional components and React hooks, ensuring that the generated code adheres to current React practices. This helps us evaluate the model's understanding of modern development paradigms, which is crucial for any developer aiming to stay up-to-date with industry standards.\nThe inclusion of error handling in the prompt tests the model's ability to write robust and reliable code. Error handling is a fundamental aspect of real-world applications, and the model's competence in this area is essential for producing high-quality outputs.\nLastly, by requiring well-commented code and adherence to best practices, the prompt allows us to evaluate the readability and maintainability of the generated code. These are important aspects of software development, as they impact the ease with which other developers can understand and work with the code.\nWhat We Are Testing For\nThe primary goal of this test is to determine which CodeLlama Instruct model best suits your MacBook Pro M3 Max by evaluating key performance metrics.\nResponse Time: We will measure the time taken by each model to generate a complete response to the prompt. This metric will help us understand how quickly each model can produce usable code, which is crucial for iterative development and real-time coding assistance.\nAccuracy: Accuracy will be assessed on multiple fronts:\n\nFunctionality: We will verify that the generated code meets all the specified requirements, such as displaying notes, adding new notes, and deleting notes. This ensures the model can follow detailed instructions and implement complex functionality.\nCorrectness: We will check for any syntax or logical errors in the code. Correctness is vital for ensuring the generated code runs without issues and performs as expected.\nClarity: We will evaluate the quality of the comments and the use of proper coding conventions. Clear and well-commented code is easier to understand and maintain, which is essential for collaborative development environments.\n\nResource Usage: We will monitor CPU, GPU, and RAM usage during the code generation process. This metric will help us determine how efficiently each model uses the available hardware resources, which is important for optimizing performance on your MacBook Pro M3 Max.\nBy systematically testing each model with this detailed React note-taking app prompt, we can identify which one provides the best balance of speed, accuracy, and resource efficiency for your specific needs. Each model offers a unique balance of these factors, allowing you to choose the optimal solution for your MacBook Pro M3 Max.\n"},"07_best_practices/07_01_best_practices_for_running_local_llms.html":{"url":"07_best_practices/07_01_best_practices_for_running_local_llms.html","title":"Best Practices for Running Local LLMs","keywords":"","body":"Best Practices for Running Local LLMs\nThis section offers best practices for running LLMs locally.\n"},"08_prompts/08_01_collection_of_prompts.html":{"url":"08_prompts/08_01_collection_of_prompts.html","title":"Collection of Prompts","keywords":"","body":"Collection of Prompts\nThis section provides a collection of prompts for different coding tasks.\n"},"08_prompts/08_02_example_create_shell_script.html":{"url":"08_prompts/08_02_example_create_shell_script.html","title":"Creating a Shell Script","keywords":"","body":"Creating a Shell Script\nOne of the things LLMs excel at is creating simple, self-contained scripts to help you automate repetitive tasks. For example, I needed a way to rebuild this guide's TOC every time I moved sections or chapters around. I started with ChatGPT but wasn't happy with the results, so I switched to Github Copilot since I was already editing and running the script through Visual Studio code. I use these kinds of shell scripts all the time and they are a pain to build from scratch. I'll walk you through my process so you can see how I break the problems up into smaller tasks the LLM can handle better. The end goal was a script that would do the following:\n\nScan all the markdown files in the project.\nPull out all the chapter names from the folder via Regex.\nAdd each of the folder's markdown files to the TOC with links.\nUse each markdown file's title for the TOC.\nClean up the chapter names by converting from snake case to title case.\nIgnore folders and files that are not part of the guide.\n\nNow let's look at my prompts and how I responded to the LLM to get it to create the final script I used.\nListing Directory Contents\nI started by asking for a basic script to list directory contents recursively:\n\n\"Can you create a script to list the directory contents recursively?\"\n\nIn response, the LLM provided a basic script that traversed directories and listed markdown files:\n#!/bin/bash\n\n# Function to recursively list directory contents\nlist_structure() {\n  local dir_path=\"$1\"\n  local indent=\"$2\"\n\n  for entry in \"$dir_path\"/*; do\n    if [ -d \"$entry\" ]; then\n      if [[ \"$(basename \"$entry\")\" =~ ^[0-9]{2}_ ]]; then\n        echo \"${indent}* $(basename \"$entry\")\"\n        list_structure \"$entry\" \"  $indent\"\n      fi\n    elif [ -f \"$entry\" ] && [[ \"$entry\" == *.md ]]; then\n      if [ \"$dir_path\" != \".\" ]; then\n        echo \"${indent}  * $(basename \"$entry\")\"\n      fi\n    fi\n  done\n}\n\nlist_structure \".\" \"\"\n\nExtracting Titles from Markdown Files\nNext, I wanted to extract H1 titles from the markdown files:\n\n\"Can you extract the H1 title from each markdown file?\"\n\nThe LLM added a function to extract the H1 title:\nextract_h1_title() {\n  local file_path=\"$1\"\n  local h1_title=$(grep -m 1 '^# ' \"$file_path\" | sed 's/^# //')\n  echo \"$h1_title\"\n}\n\nThe list_structure function was then updated to use these titles:\nif [ \"$dir_path\" != \".\" ]; then\n  local h1_title=$(extract_h1_title \"$entry\")\n  local file_name=$(basename \"$entry\")\n  echo \"${indent}  * [$h1_title]($dir_path/$file_name)\"\nfi\n\nClean Up Chapter Names\nI wanted to clean up the chapter names by removing numbers and converting them to title case:\n\n\"Can you clean up the title names? Remove the number and then title case the chapter name.\"\n\nThe LLM provided functions to handle this:\nto_title_case() {\n  local input=\"$1\"\n  echo \"$input\" | awk '{for(i=1;i\nThese functions were integrated into the list_structure function:\nif [[ \"$(basename \"$entry\")\" =~ ^[0-9]{2}_ ]]; then\n  local chapter_name=$(clean_chapter_name \"$(basename \"$entry\")\")\n  echo \"${indent}* $chapter_name\"\n  list_structure \"$entry\" \"  $indent\"\nfi\n\nConverting Snake Case to Title Case\nNext, I saw that it wasn't converting the chapter titles correctly. It was still including the underscores and not capitalizing the other words so I asked to convert the snake case of the chapter folder titles to title case:\n\n\"You need to convert the snake case of the chapter folder titles to title case so remove the _ and capitalize things correctly.\"\n\nThe LLM updated the clean_chapter_name function handled this:\nclean_chapter_name() {\n  local chapter_name=\"$1\"\n  chapter_name=$(echo \"$chapter_name\" | sed 's/^[0-9]\\{2\\}_//')\n  chapter_name=$(echo \"$chapter_name\" | sed 's/_/ /g')\n  chapter_name=$(to_title_case \"$chapter_name\")\n  echo \"$chapter_name\"\n}\n\nUsing Regex to Clean Up Names\nI realized there was a cleaner way to do this, so I asked it to optimize the regex to clean up the names more effectively:\n\n\"Can you use regex to clean up the names more effectively?\"\n\nThe LLM updated the clean_chapter_name function to use regex for more efficient cleaning:\nclean_chapter_name() {\n  local chapter_name=\"$1\"\n  chapter_name=$(echo \"$chapter_name\" | sed -E 's/^[0-9]{2}_//; s/_/ /g')\n  chapter_name=$(to_title_case \"$chapter_name\")\n  echo \"$chapter_name\"\n}\n\nRemoving Hard-Coded Directory Exclusions\nAfter seeing I could clean this up more, I asked to remove the hard-coded directories I was asking it to ignore, like .github, images, and any loose markdown files in the root of the directory like the README and SUMMARY files since I was already using regex for the chapter names:\n\n\"Remove the hard-coded directories since we're using regex for the chapter names now.\"\n\nThe LLM updated the list_structure function to dynamically handle directories:\nlist_structure() {\n  local dir_path=\"$1\"\n  local indent=\"$2\"\n\n  for entry in \"$dir_path\"/*; do\n    if [ -d \"$entry\" ]; then\n      local chapter_name=$(clean_chapter_name \"$(basename \"$entry\")\")\n      echo \"${indent}* $chapter_name\"\n      list_structure \"$entry\" \"  $indent\"\n    elif [ -f \"$entry\" ] && [[ \"$entry\" == *.md ]]; then\n      local h1_title=$(extract_h1_title \"$entry\")\n      local file_name=$(basename \"$entry\")\n      echo \"${indent}  * [$h1_title]($dir_path/$file_name)\"\n    fi\n  done\n}\n\nWriting Back To Summary Markdown File\nFinally, after testing this a few times and double checking the terminal output was correct I asked to write the TOC back to the summary.md file:\n\n\"Can you add writting these changes back to the SUMMARY.md file in the root of the directory.\"\n\nThe LLM updated the script with the following:\n# Generate the TOC and write it to SUMMARY.md\n{\n  echo \"# Table of contents\"\n  echo \"\"\n  list_structure \".\" \"\"\n} > SUMMARY.md\n\nThe Final Result\nThis approach can be adapted for various documentation projects, ensuring that the TOC remains up-to-date and correctly formatted as content evolves. Once everything worked, I asked for one final revision to the code:\n\n\"Can you heavily comment this to explain what is going on in each block of code.\" \n\nThe final script was then commented and looked like this:\n#!/bin/bash\n\n# Function to extract the H1 title from a markdown file\n# Arguments:\n#   $1 - The path to the markdown file\n# Returns:\n#   The first H1 title found in the file\nextract_h1_title() {\n  local file_path=\"$1\"\n  # Use grep to find the first line that starts with '# ' and remove the '# ' prefix\n  local h1_title=$(grep -m 1 '^# ' \"$file_path\" | sed 's/^# //')\n  echo \"$h1_title\"\n}\n\n# Function to convert a string to title case\n# Arguments:\n#   $1 - The input string\n# Returns:\n#   The input string converted to title case\nto_title_case() {\n  local input=\"$1\"\n  # Use awk to capitalize the first letter of each word and make the rest lowercase\n  echo \"$input\" | awk '{for(i=1;i SUMMARY.md\n\nThis is an example of how I would use an LLM to automate something I could normally write in a few hours in a few minutes. The most important part is always to have the LLM comment on the code. This way, you can understand what is going on under the hood, and to help you if you need to go in and modify it yourself manually. \nYou can even go one step further and have it integrate this into your CI system:\n\nNow, create a GitHub action that will call this shell script when I check my code, and make sure the SUMMARY is correct.md file is always up to date in case I forget to update it manually.\n\nThe LLM instructed me to create a file named .github/workflows/update_summary.yml in your repository with the following content:\nname: Update SUMMARY.md\n\non:\n  push:\n    branches:\n      - main  # Change this to your default branch if it's not 'main'\n\njobs:\n  update-summary:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v2\n\n    - name: Set up Git\n      run: |\n        git config --global user.name 'github-actions[bot]'\n        git config --global user.email 'github-actions[bot]@users.noreply.github.com'\n\n    - name: Run generate_summary.sh\n      run: |\n        chmod +x generate_summary.sh\n        ./generate_summary.sh\n\n    - name: Commit and push changes\n      run: |\n        git add SUMMARY.md\n        git commit -m \"Update SUMMARY.md\"\n        git push\n      env:\n\n\n G\n\nITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nThe response also included an explanation of what this does and how it works:\n\nTrigger on Push: The workflow is triggered on every push to the main branch. You can change this to match your default branch.\nCheckout Repository: Uses the actions/checkout action to check out the repository.\nSet Up Git: Configures Git with a username and email for the commit.\nRun Shell Script: Makes the generate_summary.sh script executable and runs it.\nCommit and Push Changes: Adds the updated SUMMARY.md file, commits the changes, and pushes them back to the repository. The GITHUB_TOKEN is used to authenticate the push.\n\nAt this point, I now have a fully automated workflow that ensures every time I push code to my repository, the generate_summary.sh script is executed, and any changes to the SUMMARY.md file are committed and pushed automatically.\n"},"09_advanced_usage/09_01_advanced_model_tuning.html":{"url":"09_advanced_usage/09_01_advanced_model_tuning.html","title":"Advanced Model Tuning","keywords":"","body":"Advanced Model Tuning\nThis section covers advanced techniques for tuning models.\n"},"09_advanced_usage/09_02_integrating_llms_into_workflows.html":{"url":"09_advanced_usage/09_02_integrating_llms_into_workflows.html","title":"Integrating LLMs into Workflows","keywords":"","body":"Integrating LLMs into Workflows\nThis section discusses integrating LLMs into development workflows.\n"},"09_advanced_usage/09_03_custom_built_pcs.html":{"url":"09_advanced_usage/09_03_custom_built_pcs.html","title":"Custom Built PCs","keywords":"","body":"Custom Built PCs\nWhen building a custom PC for running local LLMs, it's important to prioritize the components that will have the greatest impact on performance for your specific use case. Here are some key considerations to keep in mind:\nCPU Considerations:\n\nCore Count vs. Clock Speed: LLM workloads don’t rely heavily on high core counts unless you're doing extensive multitasking or parallel processing. In most cases, a modern CPU with solid single-core performance is sufficient, particularly since the bulk of the processing is often offloaded to the GPU.\nMultitasking and Data Loading: Look for a CPU that can handle the simultaneous tasks of loading models, managing datasets, and running the necessary interfaces with the GPU. Newer CPUs, even mid-range ones, typically handle these tasks efficiently.\n\nRAM Considerations:\n\nMemory Size: Aim for 32GB to 64GB of RAM depending on the size of the models and datasets you'll be working with. More RAM ensures that you can run multiple models concurrently, perform data preprocessing, and avoid bottlenecks related to memory limits, particularly when working with larger datasets.\nMemory Speed: While high-speed RAM can provide small performance gains, it’s less critical than the overall memory size. Prioritize capacity first, then speed.\n\nGPU Considerations:\n\nVRAM Size: This is the most important factor when running LLMs. Aim for a GPU with at least 10GB of VRAM for smaller models and light inference tasks. For larger models (13B and up), a GPU with 16GB or more VRAM will provide better performance. Running out of VRAM can cause major slowdowns or make certain tasks impossible to complete.\nGPU Power: Choose a GPU that balances your needs and budget. High-end GPUs, such as the RTX 4090 or A100, are great for demanding workloads and large models but are expensive. Mid-tier GPUs like the RTX 4070 or 4060 offer solid performance for small to medium LLMs at a more affordable price.\n\nStorage Considerations:\n\nFast SSD: A fast NVMe SSD with a minimum of 1TB of storage is recommended for quick loading of datasets and models. If you're working with large amounts of data, consider expanding storage to at least 2TB or more. Storage speed can reduce bottlenecks when loading or moving large datasets.\n\nCooling and Power Supply:\n\nCooling: Ensure your system has adequate cooling, especially if you plan on running sustained workloads. This is particularly important for GPUs that can run hot under load. A good CPU cooler and adequate case airflow are essential for maintaining optimal performance.\nPower Supply: Choose a reliable PSU that provides enough wattage to support your GPU, CPU, and other components. Make sure to leave some headroom to accommodate future upgrades.\n\nFlexibility for Future Upgrades:\n\nMotherboard and Expandability: Consider choosing a motherboard with enough PCIe lanes and RAM slots to accommodate future expansions. As your workloads evolve, you may want to add more memory, storage, or upgrade to a more powerful GPU.\n\nConclusion:\nWhen building a system for running local LLMs, prioritize GPU VRAM and RAM capacity, as these will have the greatest impact on your ability to handle large models and datasets. A modern mid-range CPU will generally suffice for most tasks, with higher-end CPUs only being necessary for heavy multitasking or data-intensive operations. Invest in a fast SSD for quick data access, and ensure your system has proper cooling and a reliable power supply to maintain performance under load. Finally, build with flexibility in mind, allowing for future upgrades as your needs grow.\n"},"10_benchmarks/10_01_geekbench_ai_benchmark.html":{"url":"10_benchmarks/10_01_geekbench_ai_benchmark.html","title":"Personal Computer Results","keywords":"","body":"Personal Computer Results\nThese are my personal computers that I've been benchmarking using Geekbench AI. This benchmarking tool provides detailed insights into how different systems perform AI-centric tasks, including image processing, object detection, and style transfer. It offers performance scores based on three types of precision: Single Precision, Half Precision, and Quantized. Each system's GPU and CPU capabilities are tested, and the results help me evaluate their strengths in handling various AI workloads.\nThe Benchmark Tests\nWindows Tests\n\nWindows CPU Benchmarks (ONNX Framework)\nIn these tests, I focused on the ONNX framework using the CPU backend on my Windows systems. The ONNX framework provides flexibility for deep learning models, and this test simulates the performance of CPU-bound AI tasks.\n\nWindows CPU Benchmarks (OpenVINO Framework)\nAnother test using the OpenVINO framework and the CPU backend on Windows systems. OpenVINO is optimized for Intel hardware, making it a strong choice for performance in inference tasks. This test helps demonstrate the effectiveness of Intel CPUs in AI workloads.\n\nWindows GPU Benchmarks (ONNX Framework + DirectML)\nThis test uses the ONNX framework with DirectML backend, leveraging the GPU for AI tasks. GPU tests are crucial for showing how well a system handles parallel processing in tasks like image recognition and machine learning model inference.\n\n\nmacOS Tests\n\nmacOS CPU Benchmarks (Core ML Framework)\nHere, I tested the Apple M3 Max using Core ML with the CPU backend. Core ML is Apple's machine learning framework optimized for macOS, and these tests show how well Apple silicon performs when executing AI workloads directly on the CPU.\n\nmacOS Neural Engine Benchmarks (Core ML Framework)\nI also benchmarked Apple's Neural Engine using Core ML. The Neural Engine is optimized for fast AI inference, and this test showcases its ability to perform AI tasks efficiently, particularly in tasks like object detection and style transfer.\n\nmacOS GPU Benchmarks (Core ML Framework)**\nLastly, I tested the Apple M3 Max's GPU using Core ML. This benchmark shows how Apple's GPU handles AI workloads that benefit from parallel processing, particularly tasks like image super-resolution and pose estimation.\n\n\nUnderstanding the Tests\nThe Geekbench AI benchmark tests systems on various real-world AI workloads. Here's a breakdown of the types of tests and why they're important:\n\nStyle Transfer: Transfers artistic styles between images, important for creative AI tasks like photo editing and art generation.\nObject Detection: Detects and identifies objects in images and videos, used in applications like security, autonomous vehicles, and AR.\nPose Estimation: Estimates the positions of people or objects in images, crucial for motion tracking, fitness apps, and animation.\nImage Super-Resolution: Enhances image resolution, used in fields like photography and medical imaging to improve visual clarity.\nImage Classification: Categorizes images by identifying their contents, vital for tasks like facial recognition and photo organization.\nFace Detection: Identifies faces within images or video, critical for security systems and user identification.\nDepth Estimation: Measures the distance of objects in a scene, essential for 3D modeling and AR/VR.\nText Classification: Classifies blocks of text by type or topic, important in language processing tasks like spam detection.\nMachine Translation: Translates text between languages, crucial for chatbots, document translation, and multilingual communication.\nImage Segmentation: Splits an image into meaningful segments for analysis, widely used in medical imaging and autonomous driving.\n\nPrice-Performance Ratio\nTo help compare systems, I calculated the Performance per $100 Spent (IPS). This method highlights how much performance (in terms of IPS or KIPS) you're getting for every $100 invested.\nFormula Used to Calculate Performance per $100 Spent\nTo calculate Performance per $100 Spent, I used the following formula:\nPerformance per $100 Spent = (Best Task Performance (IPS) / System Price) × 100\nThis gives a standardized way to compare the systems based on the amount of performance you're getting for every $100 you spend.\nGPU Performance per $100 Spent (IPS)\n\n\n\nRank\nSystem\nBest AI Task\nBest AI Task Performance (IPS)\nCurrent Price\nPerformance per $100 Spent (IPS)\n\n\n\n\n1\nRyzen 7 5800X + RTX 4070 Super\nImage Super-Resolution\n2,170 IPS\n~$1,169.51\n~185.6 IPS per $100\n\n\n2\nIntel NUC9V7QNX\nImage Super-Resolution\n1,370 IPS\n~$814.96\n~168.1 IPS per $100\n\n\n3\nMacBook Pro M3 Max\nImage Super-Resolution\n817.9 IPS\n~$2,999\n~27.3 IPS per $100\n\n\n\nCPU Performance per $100 Spent (IPS)\n\n\n\nRank\nSystem\nBest AI Task\nBest AI Task Performance (IPS)\nCurrent Price\nPerformance per $100 Spent (IPS)\n\n\n\n\n1\nMacBook Pro M3 Max\nText Classification\n6,110 IPS\n~$2,999\n~203.7 IPS per $100\n\n\n2\nRyzen 7 5800X + RTX 4070 Super\nStyle Transfer\n53.4 IPS\n~$1,169.51\n~4.57 IPS per $100\n\n\n3\nIntel NUC9V7QNX\nPose Estimation\n9.57 IPS\n~$814.96\n~1.17 IPS per $100\n\n\n\nThese tables provide a clear comparison of how each system performs in terms of GPU and CPU tasks, relative to its cost. This allows for an easier comparison of performance per dollar, making it more straightforward to decide which system offers the best value for AI tasks.\nHere are the instructions for adding your own benchmarks to the project. This section will guide contributors on how to clone the project, perform tests, and submit their benchmark results for inclusion.\n\nContributing Your Own Benchmarks\nIf you would like to contribute your own benchmarks to this project, please follow the instructions below. We encourage submissions of different system configurations, but please ensure that you provide complete results for all test types.\nSteps to Contribute\n\nFork the Project on GitHubFirst, fork this project on GitHub to create your own copy of the repository. You can do this by clicking the “Fork” button on the project’s GitHub page.\n\nClone the RepositoryClone your forked repository to your local machine by running the following command:\ngit clone https://github.com/jessefreeman/ai-for-dev.git\n\n\nRun the BenchmarksDownload and run the Geekbench AI tool on your system, testing both CPU and GPU performance across all relevant tasks. These benchmarks include Single Precision, Half Precision, and Quantized tests.  \n\nFormat Your Benchmark ResultsCollect the benchmark results and follow the template provided below to format your results. Ensure the format is consistent and includes the system specs, benchmark scores, top AI tasks, and a total system price.\n\n\nBenchmark Submission Template\nPlease use the following template when submitting your benchmarks. Ensure that all sections are filled out and consistent with the formatting used for previous submissions.\n\n[System Name] | [Framework] | [Backend] Benchmarks\n[Provide a brief summary of your system’s performance. Describe the strengths and performance highlights from the tests.]\nSystem Specs\n\nCPU: [Name]  \nMotherboard: [Name]  \nRAM: [Size and Type]  \nGPU: [Name and VRAM]  \n\nTotal Price: [Current Price]\nBenchmark Scores\n\nSingle Precision: [Score]  \nHalf Precision: [Score]  \nQuantized: [Score]  \n\nTop AI Tasks\n\n\n\nAI Task\nPerformance (IPS)\nAI Task\nPerformance (IPS)\n\n\n\n\n1. [Task Name]\n[Score]\n4. [Task Name]\n[Score]\n\n\n2. [Task Name]\n[Score]\n5. [Task Name]\n[Score]\n\n\n3. [Task Name]\n[Score]\n6. [Task Name]\n[Score]\n\n\n\n\nSubmit a Pull Request\nOnce you’ve formatted your results according to the template, submit a pull request to the main project repository. Be sure to include your benchmark file with the results and formatted markdown.\nYou can submit a pull request by navigating to your forked repository on GitHub and clicking the New Pull Request button. Make sure to provide a short description of your benchmarks and explain the key findings in your tests.\nWe look forward to your contributions!\n"},"10_benchmarks/10_02_ryzen_7_5800_32.html":{"url":"10_benchmarks/10_02_ryzen_7_5800_32.html","title":"Ryzen 7 5800X + RTX 4070 Super Performance Overview","keywords":"","body":"Ryzen 7 5800X + RTX 4070 Super Performance Overview\nThe Ryzen 7 5800X, paired with 32GB of Corsair Vengeance LPX RAM and the NVIDIA GeForce RTX 4070 Super 12GB, provides exceptional performance in AI tasks like Style Transfer, Pose Estimation, and Object Detection. This system excels particularly in GPU-accelerated workloads, making it a reliable option for complex AI tasks, with strong parallel processing power from the RTX 4070 Super.\nHardware Specs:\n\nMotherboard: Asus Rog Strix B550-I – $262.36 (was $217.99 in 2022)\nCPU: AMD Ryzen 7 5800X – $179.92 (was $349.99 in 2022)\nRAM: Corsair Vengeance LPX 32GB (2 x 16GB) DDR4 3600 – $73.99 (was $129.99 in 2022)\nGPU: NVIDIA GeForce RTX 4070 Super 12GB GDDR6X Founders Edition – $653.24 (was $699 in 2022)\n\nTotal Estimated Price:\n$1,169.51\nThis reflects the current prices of the components in 2024, with the original purchase prices of $1,396.96 in 2022.\n\nONNX DirectML Benchmarks\nThis test evaluates the performance of the NVIDIA GeForce RTX 4070 Super using the ONNX framework and DirectML backend, focusing on GPU-accelerated AI tasks.\nBenchmark Scores\n\nSingle Precision: 37,950  \nHalf Precision: 19,608  \nQuantized: 25,495  \n\nTop AI Tasks\n\n\n\nAI Task\nHalf Precision Score\nAI Task\nHalf Precision Score\n\n\n\n\n1. Pose Estimation\n476.0 IPS\n4. Depth Estimation\n863.3 IPS\n\n\n2. Style Transfer\n880.8 IPS\n5. Face Detection\n481.6 IPS\n\n\n3. Image Super-Resolution\n2.17 KIPS\n6. Object Detection\n1.38 KIPS\n\n\n\n\nOpenVINO CPU Benchmarks\nThis test measures the Ryzen 7 5800X's performance using the OpenVINO framework, highlighting its ability to handle AI workloads using quantized models on a CPU backend.\nBenchmark Scores\n\nSingle Precision: 5,019  \nHalf Precision: 7,593  \nQuantized: 15,148  \n\nTop AI Tasks\n\n\n\nAI Task\nQuantized Score\nAI Task\nQuantized Score\n\n\n\n\n1. Style Transfer\n53.4 IPS\n4. Depth Estimation\n125.1 IPS\n\n\n2. Face Detection\n180.6 IPS\n5. Object Detection\n432.3 IPS\n\n\n3. Image Super-Resolution\n299.5 IPS\n6. Pose Estimation\n17.1 IPS\n\n\n\n\nONNX CPU Benchmarks\nThis test highlights the Ryzen 7 5800X's performance on a CPU backend using the ONNX framework. It performs efficiently across AI tasks with quantized models, even without GPU acceleration.\nBenchmark Scores\n\nSingle Precision: 884  \nHalf Precision: 3,313  \nQuantized: 3,582  \n\nTop AI Tasks\n\n\n\nAI Task\nQuantized Score\nAI Task\nQuantized Score\n\n\n\n\n1. Pose Estimation\n15.2 IPS\n4. Depth Estimation\n44.5 IPS\n\n\n2. Image Classification\n728.1 IPS\n5. Face Detection\n42.2 IPS\n\n\n3. Image Super-Resolution\n116.6 IPS\n6. Object Detection\n175.9 IPS\n\n\n\n"},"10_benchmarks/10_03_m3_max.html":{"url":"10_benchmarks/10_03_m3_max.html","title":"MacBook Pro M3 Max Performance Overview","keywords":"","body":"MacBook Pro M3 Max +  30 Core GPU Performance Overview**\nMacBook Pro M3 Max Performance Overview**\nThe MacBook Pro M3 Max, equipped with 36GB of RAM and powered by Apple’s Neural Engine and 30-core GPU, provides impressive AI performance across various workloads. The system excels in Style Transfer, Pose Estimation, and Object Detection, with both the Neural Engine and GPU offering strong parallel processing capabilities. The MacBook Pro is a solid choice for AI tasks that balance speed and efficiency, particularly for developers working within the macOS ecosystem.\nHardware Specs:\n\nCPU: Apple M3 Max (14-core CPU)\nGPU: 30-core GPU\nRAM: 36GB Unified Memory\nDisplay: 16.2-inch Liquid Retina XDR\n\nTotal Estimated Price*:\n$2,999\nThis reflects the current price for the system in 2024.\n\nCore ML Neural Engine Benchmarks\nThis test evaluates the performance of the Apple M3 Max using the Core ML framework with the Neural Engine backend. The system shows strong results in Pose Estimation, Style Transfer, and Image Super-Resolution, particularly with half precision models.\nBenchmark Scores\n\nSingle Precision: 13,410  \nHalf Precision: 13,896  \nQuantized: 2,457  \n\nTop AI Tasks\n\n\n\nAI Task\nHalf Precision Score\nAI Task\nHalf Precision Score\n\n\n\n\n1. Pose Estimation\n120.1 IPS\n4. Depth Estimation\n460.6 IPS\n\n\n2. Style Transfer\n191.7 IPS\n5. Image Super-Resolution\n712.5 IPS\n\n\n3. Object Detection\n755.7 IPS\n6. Face Detection\n150.6 IPS\n\n\n\n\nCore ML GPU Benchmarks\nThis test highlights the Apple M3 Max GPU's strong performance using the Core ML framework, excelling in Style Transfer, Face Detection, and Image Super-Resolution under half precision workloads.\nBenchmark Scores\n\nSingle Precision: 13,358  \nHalf Precision: 12,570  \nQuantized: 13,411  \n\nTop AI Tasks\n\n\n\nAI Task\nHalf Precision Score\nAI Task\nHalf Precision Score\n\n\n\n\n1. Style Transfer\n203.2 IPS\n4. Depth Estimation\n261.7 IPS\n\n\n2. Face Detection\n313.7 IPS\n5. Image Super-Resolution\n817.9 IPS\n\n\n3. Object Detection\n557.2 IPS\n6. Pose Estimation\n81.0 IPS\n\n\n\n\nCore ML CPU Benchmarks\nThis test measures the Apple M3 Max's performance using the Core ML framework with a CPU backend. The system performs well across tasks like Style Transfer and Text Classification, showing consistent results in AI workloads under CPU-only processing.\nBenchmark Scores\n\nSingle Precision: 5,356  \nHalf Precision: 4,472  \nQuantized: 3,841  \n\nTop AI Tasks\n\n\n\nAI Task\nHalf Precision Score\nAI Task\nHalf Precision Score\n\n\n\n\n1. Style Transfer\n37.9 IPS\n4. Depth Estimation\n75.5 IPS\n\n\n2. Pose Estimation\n19.3 IPS\n5. Object Detection\n205.0 IPS\n\n\n3. Text Classification\n6.11 KIPS\n6. Image Super-Resolution\n181.3 IPS\n\n\n\n"},"10_benchmarks/10_04_nuc9v7qnx.html":{"url":"10_benchmarks/10_04_nuc9v7qnx.html","title":"Intel NUC9V7QNX Performance Overview","keywords":"","body":"Intel NUC9V7QNX + RTX 4060 Performance Overview\nThe Intel NUC9V7QNX, paired with 64GB of Crucial DDR4 RAM and an MSI GeForce RTX 4060 8GB, delivers strong performance across various AI tasks, making it a versatile system for AI workloads. The compact design of the NUC paired with powerful components makes this a great option for small to medium-scale AI applications.\nHardware Specs:\n\nCPU & Motherboard: 9th Generation Intel Core i7-9750H (NUC9i7QN Extreme Kit) – $369.98\nRAM: Crucial RAM 64GB Kit (2x32GB) DDR4 3200MHz CL22 – $119.99\nGPU: MSI GeForce RTX 4060 8GB GDDR6 – $324.99\n\nTotal Estimated Price*:\n$814.96\nThis reflects the price for the system when I purchased it in 2024. I already had the RTX 4060 from another computer.\n\nONNX DirectML Benchmarks\nThis test highlights the NVIDIA GeForce RTX 4060's strong performance in Style Transfer, Pose Estimation, and Image Super-Resolution, particularly with half and single precision models.\nBenchmark Scores\n\nSingle Precision: 21,260  \nHalf Precision: 10,609  \nQuantized: 13,112  \n\nTop AI Tasks\n\n\n\nAI Task\nHalf Precision Score\nAI Task\nHalf Precision Score\n\n\n\n\n1. Style Transfer\n380.2 IPS\n4. Depth Estimation\n491.9 IPS\n\n\n2. Pose Estimation\n236.0 IPS\n5. Object Detection\n874.2 IPS\n\n\n3. Image Super-Resolution\n1.37 KIPS\n6. Face Detection\n240.1 IPS\n\n\n\n\nONNX CPU Benchmarks\nThis test measures the Intel Core i7-9850H's performance using the ONNX framework on a CPU backend. Despite being CPU-only, it shows solid performance in Pose Estimation and Style Transfer with quantized models.\nBenchmark Scores\n\nSingle Precision: 592  \nHalf Precision: 2,353  \nQuantized: 2,283  \n\nTop AI Tasks\n\n\n\nAI Task\nQuantized Score\nAI Task\nQuantized Score\n\n\n\n\n1. Pose Estimation\n9.57 IPS\n4. Depth Estimation\n36.0 IPS\n\n\n2. Style Transfer\n13.4 IPS\n5. Image Super-Resolution\n83.6 IPS\n\n\n3. Face Detection\n34.4 IPS\n6. Object Detection\n114.8 IPS\n\n\n\n\nOpenVINO CPU Benchmarks\nThe Intel Core i7-9850H performs well in Style Transfer, Face Detection, and Pose Estimation using the OpenVINO framework, particularly in quantized models, demonstrating strong AI processing without GPU acceleration.\nBenchmark Scores\n\nSingle Precision: 3,490  \nHalf Precision: 4,812  \nQuantized: 3,434  \n\nTop AI Tasks\n\n\n\nAI Task\nQuantized Score\nAI Task\nQuantized Score\n\n\n\n\n1. Style Transfer\n32.6 IPS\n4. Depth Estimation\n75.0 IPS\n\n\n2. Face Detection\n109.1 IPS\n5. Image Super-Resolution\n196.6 IPS\n\n\n3. Pose Estimation\n10.6 IPS\n6. Object Detection\n277.4 IPS\n\n\n\n"},"11_additional_resources/11_01_tools.html":{"url":"11_additional_resources/11_01_tools.html","title":"Tools","keywords":"","body":"Tools\nThis section lists recommended reading, tools, and community links.\n\nGPT4All: GPT4All is a project by Nomic AI that provides open-source tools and models for developing and running GPT-4 models locally.\nLM Studio: LM Studio is a platform for building, training, and deploying language models, making it easier to manage and scale LLMs.\nContinue: Continue is a VS Code extension that enhances coding productivity by integrating AI-powered suggestions and code completion features.\nHugging Face VS Code Extension: A Visual Studio Code extension that integrates Hugging Face's AI tools, enabling easy access to models and APIs within the coding environment.\nGeekbench AI Download: Geekbench AI offers a comprehensive benchmarking tool for evaluating AI performance across devices, allowing users to assess both CPU and GPU performance for machine learning tasks.\n\n"},"11_additional_resources/11_02_articles.html":{"url":"11_additional_resources/11_02_articles.html","title":"Articles","keywords":"","body":"Articles\nThis section lists recommended reading and community links.\nModel Selection\n\n50 Open-Source Options for Running LLMs Locally by Vince Lam: An extensive list of 50 open-source tools and projects for running LLMs locally, offering various options and their use cases.\nCode Llama: Llama 2 Learns to Code by https://huggingface.co/: An introduction to Code Llama, highlighting its variants—Base, Python, and Instruct—tailored for coding tasks. The article explores how these models, optimized from Meta's Llama 2, enhance code generation by bridging natural language and programming, making them powerful tools for developers.\n\nInstallation and Configuration\n\nMac for Large Language Models by Allan Witt: A comprehensive guide on configuring Macs, especially with Apple Silicon, for running large language models efficiently.\nLocal LLMs on Apple Silicon by Aaditya Bhat: Insights and techniques for running local LLMs `on Apple Silicon, highlighting performance optimizations and practical applications.\nLocal LLM Fine-Tuning on Mac M1 16GB by Shaw Talebi: This article explores fine-tuning local LLMs on a Mac M1 with 16GB RAM, offering insights and steps for effective model training and optimization.\nA Simple, Practical Guide to Running Large-Language Models on Your Laptop by Ryan Stewart: This guide shows how to run large-language models locally on your laptop using llama-cpp-python and GGUF models. It highlights benefits like cost savings, privacy, and faster iteration, providing step-by-step instructions for setup on both CPU and GPU.\n\nAdvanced Usage\n\nUsing a Local LLM as a Free Coding Copilot in VS Code by Simon Fraser: A guide on setting up and using a local LLM as a coding assistant in VS Code, detailing the setup process and benefits.\nIntegrating Large Language Models with Apple's Core ML by Pedro Cuenca: This article discusses the integration of large language models with Apple's Core ML, providing a detailed guide on leveraging these models in Swift applications.\n\nLocal Servers\n\nHow to Set Up and Use a Windows NAS: This article guides you through setting up a Windows NAS, covering hardware selection, drive configuration, and network sharing. It also includes tips for optimizing performance and securing your NAS for a home or small office network.\n\n"},"11_additional_resources/11_03_in_the_news.html":{"url":"11_additional_resources/11_03_in_the_news.html","title":"In the News","keywords":"","body":"In the News\nThis section lists interesting articles and announcements from my RSS feeds:\n\nAugust 19, 2024: Meta's Self-Taught Evaluator enables LLMs to create their own training data by Ben Dickson: Meta FAIR researchers have introduced the Self-Taught Evaluator, a novel system that eliminates the need for human-annotated data by allowing LLMs to generate and evaluate their own training data, significantly improving model accuracy and scalability while reducing reliance on manual evaluation.\n\nAugust 19, 2024: Build your own AI-powered robot: Hugging Face’s LeRobot tutorial is a game-changer by Michael Nuñez: Hugging Face has released a detailed tutorial guiding developers through building and training AI-powered robots, making advanced robotics accessible to a wider audience by lowering barriers to entry and encouraging community collaboration.\n\nAugust 16, 2024: MidJourney releases new unified AI image editor on the web by Carl Franzen: MidJourney has unveiled a new web editor that integrates inpainting, outpainting, and other features into a unified interface, providing users with a more seamless and precise tool for AI-generated image editing, while also introducing message mirroring between its web and Discord platforms.\n\nAugust 15, 2024: Exists launches GenAI platform to create 3D games from text prompts by Dean Takahashi: Exists has unveiled a generative AI platform that allows users to create high-quality 3D games from simple text prompts without any coding skills, providing instant multiplayer games and advanced customization options within a user-friendly cloud-based interface.\n\nAugust 15, 2024: Google quietly opens Imagen 3 access to all U.S. users by Michael Nuñez: Google has made its latest text-to-image AI model, Imagen 3, available to all U.S. users, expanding access to the powerful tool amid mixed reactions to its content filters and ethical considerations in the broader AI industry.\n\n\n"}}}