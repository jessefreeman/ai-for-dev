---
title: "Installation"
has_children: true
nav_order: 4
---

# Installation

# Installation

Getting local LLMs up and running was the biggest hurdle when I started. There are so many different tools and platforms, each with their own quirks. I've tried most of them and made plenty of mistakes along the way. Here's what I learned about getting everything set up properly.

## My Installation Journey

I started with LM Studio because it seemed the most beginner-friendly, and honestly, it still is. But over time I've experimented with Ollama (great for terminal users), GPT4All (decent UI), and LocalAI (if you want more control). Each has its place, but if you're just starting out, stick with LM Studio.

The biggest mistakes I made early on:

- Not checking my hardware specs first
- Downloading models that were way too big for my system
- Not understanding that Apple Silicon needs special handling
- Trying to run everything at once and melting my laptop

## What I've Tested

Here's what I actually use and can recommend:

- **Windows 10/11** (x64, ARM64)
- **macOS** (Intel and Apple Silicon)
- **Linux** (Ubuntu, Debian, Fedora, and other distributions)

## Hardware Requirements

### Minimum Requirements

- **RAM**: 8GB (16GB recommended)
- **Storage**: 10GB free space (more for larger models)
- **CPU**: Modern x64 or ARM64 processor

### Recommended for Best Experience

- **RAM**: 32GB or more
- **GPU**: NVIDIA RTX series or Apple Silicon
- **Storage**: Fast SSD with 100GB+ free space

## Getting Started

New users should start with [Installing LM Studio](05_01_installing_lm_studio.md) for the easiest setup experience. Mac users with Apple Silicon should also review the [Apple Silicon optimization guide](05_02_configuring_lm_studio_on_apple_silicon.md) for best performance.
